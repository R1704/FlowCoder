\contentsline {figure}{\numberline {1}{\ignorespaces taken from google}}{12}{figure.0.1}%
\contentsline {figure}{\numberline {2}{\ignorespaces Image from \cite {dehaene_symbols_2022}.}}{13}{figure.0.2}%
\contentsline {figure}{\numberline {3}{\ignorespaces (A) 8 different domains of tasks. (B) Representation of the learned library of concepts. On the left we see the initial primitives from which the concepts in the middle region are composed. On the right we see a task as input-output relations and the found solution. On the bottom is the same solution expressed only with initial primitives. Image taken with permission from the original paper \cite {ellis_dreamcoder_2021}.}}{15}{figure.0.3}%
\contentsline {figure}{\numberline {4}{\ignorespaces An example of an abstract syntax tree (AST) using deBruijn indexing. This translates to \texttt {var0 + var1 * var0}.}}{18}{figure.0.4}%
\contentsline {figure}{\numberline {5}{\ignorespaces Bar plot of unique programs created per task. The sorted tasks the model has been trained are on the x-axis and the number of unique programs that have been created per task are on the y-axis. Bars of tasks that have been solved are coloured green and unsolved tasks are coloured red. The black dotted line demarcates the average number of uniquely created programs.}}{27}{figure.0.5}%
\contentsline {figure}{\numberline {6}{\ignorespaces The plot displays the cumulative number of tasks solved (y-axis) against the number of steps (x-axis). Each step represents an iteration in the E-M cycle. The initial 2.000 steps correspond to the first E-step, marked with a blue background, followed by the first M-step spanning the next 2.000 steps (up to step 4.000), distinguished by a purple background. This pattern constitutes one complete epoch. The graph includes a dotted line representing the average number of tasks solved over all epochs, offering a benchmark for comparison. Furthermore, the intensity of the color hue in the plot encodes the temporal sequence of the epochs: brighter bars on the left signify earlier epochs (the first epoch), with the hue gradually darkening towards the right, culminating in the fifth and final epoch.}}{28}{figure.0.6}%
\contentsline {figure}{\numberline {7}{\ignorespaces Analysis of the number of unique programs generated per task during inference, as shown in Figure \ref {fig:program_variations_binary_inference}. Tasks are listed on the x-axis, and the count of unique programs is on the y-axis. The figure highlights the variance in program generation across tasks, with \texttt {caesar} tasks often showing limited program diversity, in contrast to other tasks that exhibit a wide range of attempts. The average number of unique programs created per task is approximately 145, indicating varied program attempts.}}{29}{figure.0.7}%
\contentsline {figure}{\numberline {8}{\ignorespaces Distribution of unique solutions per task during inference, displayed in a log-scaled bar chart. The y-axis lists the task names, while the x-axis quantifies the number of unique solutions. The chart reveals that 13 tasks not solved in training were resolved during inference, with 11 belonging to groups with at least one task previously solved in training. Only solved tasks are shown.}}{30}{figure.0.8}%
\contentsline {figure}{\numberline {9}{\ignorespaces Analysis of the minimum number of steps required to solve tasks during inference. The x-axis represents the number of steps, and the y-axis lists the task names, sorted by the number of steps taken to find a solution. The dotted line indicates the average number of steps needed. Tasks solved both during training and inference are highlighted in green, whereas tasks exclusively solved during inference are in purple. Only solved tasks are shown.}}{31}{figure.0.9}%
\contentsline {figure}{\numberline {10}{\ignorespaces Creation of trees showing the construction process.}}{38}{figure.0.10}%
\contentsline {figure}{\numberline {11}{\ignorespaces Insert explanation + maybe change task to an actual task; same with state; maybe also show the forward and Z output better.}}{38}{figure.0.11}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces (a) A sensorimotor input, here a table, is compressed sequentially into a Semantic Pointer. (b) A Semantic Pointer is decompressed and returns a representation of a table. Due to the compression process the decompression results in noise. The figure is taken from Blouw et al. \cite {blouw2016concepts}.}}{51}{figure.1.1}%
