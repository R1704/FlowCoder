\contentsline {todo}{\color@fb@x {}{black}{}{yellow}{\leavevmode {\color {yellow}o}}\ Go through text and see which words should be operationalised into the glossary}{5}{section*.6}%
\contentsline {todo}{\color@fb@x {}{black}{}{yellow}{\leavevmode {\color {yellow}o}}\ Put all terms in gpt and ask for similar terms}{5}{section*.7}%
\contentsline {todo}{\color@fb@x {}{black}{}{yellow}{\leavevmode {\color {yellow}o}}\ Use plagiarism checker}{5}{section*.8}%
\contentsline {todo}{\color@fb@x {}{black}{}{yellow}{\leavevmode {\color {yellow}o}}\ make sure that the tone of the thesis is the same everywhere. Write it with wit. Make it fun to read.}{5}{section*.9}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ structure to keep in mind:}{5}{section*.10}%
\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color {Plum!25}o}}\ change size}{6}{section*.11}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ - All agents have in common the ability to pursue goals. - The size of the goal is a way to classify and compare diverse intelligences. For example, a single-celled organism may have the goal of finding food and avoiding predators, while a human may have the goal of building a family or writing a thesis about cognition. - ML uses the term **cognitive light cone** (a reference to physics). The cognitive light cone is a metaphor for the limits of our knowledge and ability to act. It is defined by the set of all events that we can perceive, measure, and model. - The self gradually emerges from being an oocyte, which is "just physics," into a thing that can reason about itself. - This process of self-emergence is closely linked to the scaling of cell and agent organization. - When you scratch a blastoderm, it self-organizes and you get twins or triplets and so forth. - This shows that the number of selves in an embryo is not set up by genetics, but is rather a physiological self-organizational process. - How many selves are in an embryo is not set up by genetics. its a physiological self organisational process - This problem of where do I end and where does the world begin needs to be solved “online” - This means that the self is constantly being updated and renegotiated in light of new information and experiences. - How bodies self-organize is fundamentally the same question as how minds self-organize. All bodies use a multi-scale competency architecture. This means that they are made up of many different levels of organization, from the individual cell to the entire organism. Each level of organization has its own goals and objectives, and these goals and objectives are coordinated to achieve the overall goals of the organism. - Embodiment is the state of having a physical body. It is the grounding of our cognitive processes in our bodily experiences. Our bodies are not just passive vessels for our minds; they are active participants in our cognitive lives. - Although embodiment (may be?) is a necessary component of cognition, perhaps embodiment needs to be refined/ re-defined. - Axolotl arms can be amputated and regenerated, and they will know when to stop. Moreover, it gets to the same outcome despite perturbations from diverse starting positions via different paths. This shows that the body has a plan, or an internal representation of what it should look like. This plan is encoded in bioelectrical patterns. Bioelectrical events are also involved in the development of the embryo. - They don't distinguish between neural and developmental tissue. both have ion channels and electrical synapses and they developed a technique to read and write these electrical patterns - Like bioelectrical events in the brain are controlling muscles to move you through three-dimensional space, this more ancient system is using bioelectrical events elsewhere in the body right from the moment of fertilisation to control all of the cells to move the configuration of the body through morphospace. - endogenous bioelectric pre-patterns: reading the mind of the body. These electrical patterns, guide the cells to arrange in morphospace - They injected ion channel RNA of potassium channels that sets a particular voltage state encoding an eye in a spot where it shouldn't be. These cells now form eyes, by this bioelectrical instruction. Moreover, when there aren't enough cells to do that, they recruit other cells to achieve that goal. - Rewriting anatomical pattern memory: The bioelectrical pattern is an internal representation of what it’s body looks like. They edit the bioelectrical pattern to give it two heads. When injured, the planarian will regenerate according to the new pattern. This means that they store a memory of of where in morphospace they’re supposed to go. This memory is rewritable and is a primitive precursor to being able to imagine things that haven’t happened yet, i.e. the same body can have different electrical pattern memories. }{6}{section*.12}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ From Maxwell Ramstead Tutorial on active inference}{7}{section*.13}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ LLMs also create a generative model, but lack the causal structure.}{9}{section*.14}%
\contentsline {todo}{\color@fb@x {}{black}{}{yellow}{\leavevmode {\color {yellow}o}}\ establish the argument for a generative model. Then we talk about the kind of generative model. Maybe introduce Transformers here already.}{9}{section*.15}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ From \cite {ciaunica_nested_2023}: “Self-organization is typically defined as the spontaneous emergence of spatiotemporal order or pattern-formation processes in physical and biological systems resulting from interactions of its components with the environment (Camazine et al. 2001; Seeley 2002, Rosas et al. 2018). Properties of a global higher level system emerge from—and are dependent upon—interactions of its components at the lower level. For example, when the wind blows over a uniform surface of sand, a pattern of regularly spaced ranges emerges (cf. FIG. 1) as a combination of gravity forces and wind speed act on the sand particles (Forrest and Haff 1992). In turn, the surface of the sand determines the flow of air, shaping the reports. Self-organisation therefore entails both bottom-up and top-down causation (Ellis et al., 2011) that is reflected in the circular” “causality implicit in the enslaving principle (Haken \& Portugali, 2016) in synergetics or the centre manifold theorem in dynamical systems theory (Carr, 1981).” “However, biological organization is more than emergent complexity. It is fundamentally composed of nested goal-seeking (homeostatic and allostatic) agents, ranging from molecular pathways to whole organ systems and beyond. These not only exhibit complex behaviour, but also specifically act to minimize and maximize various quantities and solve problems by navigating (with diverse degrees of competency) a variety of spaces including transcriptional, physiological, and anatomical spaces in addition to the familiar space of behaviour (Fields \& Levin, 2022).”}{9}{section*.16}%
\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color {Plum!25}o}}\ Shared competencies, or similar strategy between compositional hierarchical structures in organisms and in the construction of concepts/ thoughts}{11}{section*.19}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ From "Building machines that learn and think like people}{11}{section*.20}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ argument for having two systems 1, 2}{12}{section*.21}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ argument for hemispheric lateralization}{12}{section*.22}%
\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color {Plum!25}o}}\ difference between what is meaning and what is meaningful, maps of meaning. what is meaningful is what guides you. its a gradient, a heuristic. Show studies that deeper understanding improves memory and meaning.}{13}{section*.23}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ here he says that point estimates are not enough. so MLE over MAP}{14}{section*.24}%
\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color {Plum!25}o}}\ this relates to gflownets representing the posterior}{14}{section*.25}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ from JB: LLMs learn how to complete sequences, they do statistics over patterns patterns → syntax → style → semantics they deep-fake so well that the outcome becomes indistinguishable from what we do. thats why its said to brute force. Minds learn how to understand we do it the other way around semantics → patterns → syntax → style}{16}{section*.27}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ Our conclusion from the above discussion is that in neurosymbolic AI: • Knowledge should be grounded onto vector representations for efficient learning from data based on message passing in neural networks as an efficient computational model. • Symbols should become available as a result of querying and knowledge extraction from trained networks, and offer a rich description language at an adequate level of abstraction, enabling infinite uses of finite means, but also compositional discrete reasoning at the symbolic level allowing for extrapolation beyond the data distribution. • The combination of learning and reasoning should offer an important alternative to the problem of combinatorial reasoning by learning to reduce the number of effective combinations, thus producing simpler symbolic descriptions as part of the neurosymbolic cycle. \cite {garcez_neurosymbolic_2020}}{19}{section*.31}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ Formalizing the problem as constructing thoughts out of concepts in a vast state space. This is the general notion of building thoughts out of more primitive components. We don't define the nature of these components here. They could be programs or hyperdimensional vectors, or other stuff.}{20}{section*.33}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ Here we make the case for a probabilistic programming paradigm.}{21}{section*.34}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ Formalizing the problem as program synthesis.}{21}{section*.35}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ At a high-level the approaches we develop in this work follow a 2-stage pipeline: in the first stage a learned model predicts probabilistic weights, and in the second stage a symbolic search algorithm uses those weights to explore the space of source code. Our contributions target the second stage of this pipeline, and we focus on theoretical analysis of sampling-based search algorithms, new search algorithms based on neurally-informed enumeration, and empirical evaluations showing that recent neural program synthesizers can compose well with our methods.}{26}{section*.40}%
\contentsline {todo}{\color@fb@x {}{cyan}{}{cyan!25}{\leavevmode {\color {cyan!25}o}}\ the DSL is given by a set of primitives together with their (possibly polymorphic) types and semantics. We describe the machine learning pipeline for program synthesis, illustrated in Figure 1 on a toy DSL describing integer list manipulating programs. The compilation phase constructs a context-free grammar (CFG) from the DSL together with a set of syntactic constraints. The CFG may incorporate important information about the program being generated, such as the n last primitives (encompassing n-gram models) or semantic information (e.g. non- zero integer, sorted list). A prediction model (typically a neural network) takes as inputs a set of I/O and outputs a probabilistic labelling of the CFG, inducing a probabilistic context-free grammar (PCFG). The network is trained so that most likely pro- grams (with respect to the PCFG) are the most likely to be solutions, meaning map the inputs to corresponding outputs. We refer to Appendix A for an in-depth technical discussion on program representations and on the compilation phase. In this work we focus on the search phase and start with defining a theoretical framework for analysing search algorithms. The PCFG obtained through the predictions of the neural network defines a probabilistic distribution D over programs. We make the theoretical assumption that the program we are looking for is actually sampled from D, and construct algorithms searching through programs which find programs sampled from D as quickly as possible. Formally, the goal is to minimise the expected number of programs the algorithm outputs before finding the right program. }{26}{section*.41}%
\contentsline {todo}{\color@fb@x {}{OliveGreen}{}{OliveGreen!25}{\leavevmode {\color {OliveGreen!25}o}}\ the following is from GFlowNet-EM}{30}{section*.47}%
