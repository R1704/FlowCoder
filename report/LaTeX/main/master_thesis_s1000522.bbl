% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{alroumiMentalCompressionSpatial2021}{article}{}
      \name{author}{5}{}{%
        {{hash=1a6972301c9d189783c54f07c6ec2c54}{%
           family={Al\bibnamedelima Roumi},
           familyi={A\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Fosca},
           giveni={F\bibinitperiod}}}%
        {{hash=1218f133d4c542b148c44e1002b0ed62}{%
           family={Marti},
           familyi={M\bibinitperiod},
           given={S{é}bastien},
           giveni={S\bibinitperiod}}}%
        {{hash=619428b9bc5864c8559e0a3744fa5c0a}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Liping},
           giveni={L\bibinitperiod}}}%
        {{hash=dd9623734f04e3b410550448cadcb707}{%
           family={Amalric},
           familyi={A\bibinitperiod},
           given={Marie},
           giveni={M\bibinitperiod}}}%
        {{hash=14ff7c970ba747f58e07de2e42e7c5bb}{%
           family={Dehaene},
           familyi={D\bibinitperiod},
           given={Stanislas},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{7234248e482ad7ef02a4a02cabc04ebc}
      \strng{fullhash}{9b0035dd85920c6149df3dfdffbb4108}
      \strng{bibnamehash}{9b0035dd85920c6149df3dfdffbb4108}
      \strng{authorbibnamehash}{9b0035dd85920c6149df3dfdffbb4108}
      \strng{authornamehash}{7234248e482ad7ef02a4a02cabc04ebc}
      \strng{authorfullhash}{9b0035dd85920c6149df3dfdffbb4108}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{How does the human brain store sequences of spatial locations? We propose that each sequence is internally compressed using an abstract, language-like code that captures its numerical and geometrical regularities. We exposed participants to spatial sequences of fixed length but variable regularity while their brain activity was recorded using magneto-encephalography. Using multivariate decoders, each successive location could be decoded from brain signals, and upcoming locations were anticipated prior to their actual onset. Crucially, sequences with lower complexity, defined as the minimal description length provided by the formal language, led to lower error rates and to increased anticipations. Furthermore, neural codes specific to the numerical and geometrical primitives of the postulated language could be detected, both in isolation and within the sequences. These results suggest that the human brain detects sequence regularities at multiple nested levels and uses them to compress long sequences in working memory.}
      \field{issn}{0896-6273}
      \field{journaltitle}{Neuron}
      \field{langid}{english}
      \field{month}{8}
      \field{number}{16}
      \field{title}{Mental Compression of Spatial Sequences in Human Working Memory Using Numerical and Geometrical Primitives}
      \field{urlday}{29}
      \field{urlmonth}{10}
      \field{urlyear}{2022}
      \field{volume}{109}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{2627\bibrangedash 2639.e4}
      \range{pages}{-1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/QSVLITNP/Al Roumi et al. - 2021 - Mental compression of spatial sequences in human w.pdf;/Users/ron/Zotero/storage/LABPQJCM/S0896627321004244.html
      \endverb
      \keyw{Geometry,Language of Thought,Magnetoencephalography,Memory,Ordinal Knowledge,Primitive Operations,Sequence Processing,Sequence Structure,Syntax}
    \endentry
    \entry{allamanis_learning_2018}{article}{}
      \name{author}{3}{}{%
        {{hash=e1e5eb773f262866a97eef52c0456008}{%
           family={Allamanis},
           familyi={A\bibinitperiod},
           given={Miltiadis},
           giveni={M\bibinitperiod}}}%
        {{hash=b4588d6a8a507f2c11c56b3da953c323}{%
           family={Khademi},
           familyi={K\bibinitperiod},
           given={Mahmoud},
           giveni={M\bibinitperiod}}}%
        {{hash=c1ccbd5993a88fd3df53291c31d8ba35}{%
           family={Brockschmidt},
           familyi={B\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{7886d3262d152e8882e12f995783030f}
      \strng{fullhash}{5d86dd74618d31d39ecf718168199e8f}
      \strng{bibnamehash}{5d86dd74618d31d39ecf718168199e8f}
      \strng{authorbibnamehash}{5d86dd74618d31d39ecf718168199e8f}
      \strng{authornamehash}{7886d3262d152e8882e12f995783030f}
      \strng{authorfullhash}{5d86dd74618d31d39ecf718168199e8f}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.}
      \field{langid}{english}
      \field{title}{Learning to Represent Programs with Graphs}
      \field{year}{2018}
      \verb{file}
      \verb /Users/ron/Zotero/storage/WU7Q6I6D/Allamanis et al. - 2018 - LEARNING TO REPRESENT PROGRAMS WITH GRAPHS.pdf
      \endverb
    \endentry
    \entry{AndoniKO10}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9f17feb9ebe3b97546681c377fb97416}{%
           family={Andoni},
           familyi={A\bibinitperiod},
           given={Alexandr},
           giveni={A\bibinitperiod}}}%
        {{hash=d3b044af230e488b33f1adfa14260de7}{%
           family={Krauthgamer},
           familyi={K\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=1f65acb3611c87d8dc593d1c054eb716}{%
           family={Onak},
           familyi={O\bibinitperiod},
           given={Krzysztof},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE Computer Society}%
      }
      \strng{namehash}{e1de77ce78cb02015b82464d2d79cad5}
      \strng{fullhash}{f99e5eae286df2b8632696b5e6da822d}
      \strng{bibnamehash}{f99e5eae286df2b8632696b5e6da822d}
      \strng{authorbibnamehash}{f99e5eae286df2b8632696b5e6da822d}
      \strng{authornamehash}{e1de77ce78cb02015b82464d2d79cad5}
      \strng{authorfullhash}{f99e5eae286df2b8632696b5e6da822d}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{51th Annual {{IEEE}} Symposium on Foundations of Computer Science, {{FOCS}} 2010, October 23-26, 2010, Las Vegas, Nevada, {{USA}}}
      \field{title}{Polylogarithmic Approximation for Edit Distance and the Asymmetric Query Complexity}
      \field{year}{2010}
      \field{pages}{377\bibrangedash 386}
      \range{pages}{10}
    \endentry
    \entry{auyespekHyperbolicEmbeddingFinding}{article}{}
      \name{author}{3}{}{%
        {{hash=03675a627ab20254d79864663c8b3045}{%
           family={Auyespek},
           familyi={A\bibinitperiod},
           given={Temirlan},
           giveni={T\bibinitperiod}}}%
        {{hash=282d1bec63019659201f378c21cf9aa4}{%
           family={Mach},
           familyi={M\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=99f38c73ef247d2ac80e867f0f86a978}{%
           family={Assylbekov},
           familyi={A\bibinitperiod},
           given={Zhenisbek},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{473f7bab853a5bf309da6b96f23997a1}
      \strng{fullhash}{49d5f3853309270932c81ac53ce58260}
      \strng{bibnamehash}{49d5f3853309270932c81ac53ce58260}
      \strng{authorbibnamehash}{49d5f3853309270932c81ac53ce58260}
      \strng{authornamehash}{473f7bab853a5bf309da6b96f23997a1}
      \strng{authorfullhash}{49d5f3853309270932c81ac53ce58260}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent advances in natural language processing have improved our understanding of what kind of linguistic knowledge is encoded in modern word representations. For example, methods for testing the ability to extract syntax trees from a language model architecture were developed by Hewitt and Manning (2019)—they project word vectors into Euclidean subspace in such a way that the corresponding squared Euclidean distance approximates the tree distance between words in the syntax tree. This work proposes a method for assessing whether embedding word representations in hyperbolic space can better reflect the graph structure of syntax trees. We show that the tree distance between words in a syntax tree can be approximated well by the hyperbolic distance between corresponding word vectors.}
      \field{langid}{english}
      \field{title}{Hyperbolic {{Embedding}} for {{Finding Syntax}} in {{BERT}}}
      \verb{file}
      \verb /Users/ron/Zotero/storage/MGCIZMYJ/Auyespek et al. - Hyperbolic Embedding for Finding Syntax in BERT.pdf
      \endverb
    \endentry
    \entry{bengioFlowNetworkBased2021}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=a0d376ad8a651c5596443bd634847aae}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Emmanuel},
           giveni={E\bibinitperiod}}}%
        {{hash=93d11841cec92dc61eb3d903b3ac2002}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Moksh},
           giveni={M\bibinitperiod}}}%
        {{hash=3f598cb6a2e1ff3b305c4503ceea8e5b}{%
           family={Korablyov},
           familyi={K\bibinitperiod},
           given={Maksym},
           giveni={M\bibinitperiod}}}%
        {{hash=4428b76e1301b2db58587fb18bb59a38}{%
           family={Precup},
           familyi={P\bibinitperiod},
           given={Doina},
           giveni={D\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{573a9f3ffdf5a27ea67c8df5417223c1}
      \strng{fullhash}{4777ca58c837423e7f68216c4db79f36}
      \strng{bibnamehash}{4777ca58c837423e7f68216c4db79f36}
      \strng{authorbibnamehash}{4777ca58c837423e7f68216c4db79f36}
      \strng{authornamehash}{573a9f3ffdf5a27ea67c8df5417223c1}
      \strng{authorfullhash}{4777ca58c837423e7f68216c4db79f36}
      \field{extraname}{1}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Flow {{Network}} Based {{Generative Models}} for {{Non-Iterative Diverse Candidate Generation}}}
      \field{urlday}{13}
      \field{urlmonth}{4}
      \field{urlyear}{2022}
      \field{volume}{34}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{27381\bibrangedash 27394}
      \range{pages}{14}
      \verb{file}
      \verb /Users/ron/Zotero/storage/RXZEKNQV/Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf
      \endverb
    \endentry
    \entry{bengio2021deep}{article}{}
      \name{author}{3}{}{%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=d0ab8cbca75df7aa3e0b6024d60ac5f8}{%
           family={Lecun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {ACM New York, NY, USA}%
      }
      \strng{namehash}{058f6aa7282521dda725a309bd2d3fea}
      \strng{fullhash}{1202fccd2dc74410d95e578c872a5dcb}
      \strng{bibnamehash}{1202fccd2dc74410d95e578c872a5dcb}
      \strng{authorbibnamehash}{1202fccd2dc74410d95e578c872a5dcb}
      \strng{authornamehash}{058f6aa7282521dda725a309bd2d3fea}
      \strng{authorfullhash}{1202fccd2dc74410d95e578c872a5dcb}
      \field{extraname}{2}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Communications of the ACM}
      \field{number}{7}
      \field{title}{Deep Learning for {{AI}}}
      \field{volume}{64}
      \field{year}{2021}
      \field{pages}{58\bibrangedash 65}
      \range{pages}{8}
    \endentry
    \entry{bengioGFlowNetFoundations2023}{article}{}
      \name{author}{6}{}{%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=b8fd22cd253cc620d7973894bb38d016}{%
           family={Lahlou},
           familyi={L\bibinitperiod},
           given={Salem},
           giveni={S\bibinitperiod}}}%
        {{hash=8a006dd742c8f7a1e71c8d5b862abaec}{%
           family={Deleu},
           familyi={D\bibinitperiod},
           given={Tristan},
           giveni={T\bibinitperiod}}}%
        {{hash=e6a95d07b8ae7f14e4eb777c6980a059}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Edward\bibnamedelima J.},
           giveni={E\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=059033df809a2cb14afa9cf461886527}{%
           family={Tiwari},
           familyi={T\bibinitperiod},
           given={Mo},
           giveni={M\bibinitperiod}}}%
        {{hash=a0d376ad8a651c5596443bd634847aae}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Emmanuel},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{058f6aa7282521dda725a309bd2d3fea}
      \strng{fullhash}{f2f8f5cfcc3fc8b134320b4884e6f721}
      \strng{bibnamehash}{058f6aa7282521dda725a309bd2d3fea}
      \strng{authorbibnamehash}{058f6aa7282521dda725a309bd2d3fea}
      \strng{authornamehash}{058f6aa7282521dda725a309bd2d3fea}
      \strng{authorfullhash}{f2f8f5cfcc3fc8b134320b4884e6f721}
      \field{extraname}{3}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new local and efficient training objective called detailed balance for the analogy with MCMC. GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, continuous actions and modular energy functions.}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{210}
      \field{title}{{{GFlowNet Foundations}}}
      \field{urlday}{29}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{24}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 55}
      \range{pages}{55}
      \verb{file}
      \verb /Users/ron/Zotero/storage/TDS7ER3X/Bengio et al. - 2023 - GFlowNet Foundations.pdf
      \endverb
    \endentry
    \entry{berwickPovertyStimulusRevisited2011}{article}{}
      \name{author}{4}{}{%
        {{hash=0242dba3692ae8a4b640b51d5a3f0911}{%
           family={Berwick},
           familyi={B\bibinitperiod},
           given={Robert\bibnamedelima C.},
           giveni={R\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=d020f6aa3ed5b44ad35445b93bbff0c1}{%
           family={Pietroski},
           familyi={P\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=7847238ccda6970879f6900e16f9d8ce}{%
           family={Yankama},
           familyi={Y\bibinitperiod},
           given={Beracah},
           giveni={B\bibinitperiod}}}%
        {{hash=28167c0b3800bc5cfb686676277005b0}{%
           family={Chomsky},
           familyi={C\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{d5ed09ed31fec3172b59bd2e86504975}
      \strng{fullhash}{b4d385ce60fc670c7d0ec897873313aa}
      \strng{bibnamehash}{b4d385ce60fc670c7d0ec897873313aa}
      \strng{authorbibnamehash}{b4d385ce60fc670c7d0ec897873313aa}
      \strng{authornamehash}{d5ed09ed31fec3172b59bd2e86504975}
      \strng{authorfullhash}{b4d385ce60fc670c7d0ec897873313aa}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A central goal of modern generative grammar has been to discover invariant properties of human languages that reflect ``the innate schematism of mind that is applied to the data of experience'' and that ``might reasonably be attributed to the organism itself as its contribution to the task of the acquisition of knowledge'' (Chomsky, 1971). Candidates for such invariances include the structure dependence of grammatical rules, and in particular, certain constraints on question formation. Various ``poverty of stimulus'' (POS) arguments suggest that these invariances reflect an innate human endowment, as opposed to common experience: Such experience warrants selection of the grammars acquired only if humans assume, a priori, that selectable grammars respect substantive constraints. Recently, several researchers have tried to rebut these POS arguments. In response, we illustrate why POS arguments remain an important source of support for appeal to a priori structure-dependent constraints on the grammars that humans naturally acquire.}
      \field{issn}{1551-6709}
      \field{journaltitle}{Cognitive Science}
      \field{langid}{english}
      \field{number}{7}
      \field{title}{Poverty of the {{Stimulus Revisited}}}
      \field{urlday}{23}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{35}
      \field{year}{2011}
      \field{urldateera}{ce}
      \field{pages}{1207\bibrangedash 1242}
      \range{pages}{36}
      \verb{file}
      \verb /Users/ron/Zotero/storage/UDN4W8SJ/Berwick et al. - 2011 - Poverty of the Stimulus Revisited.pdf
      \endverb
      \keyw{Language acquisition,Linguistics,Statistics,Syntax}
    \endentry
    \entry{bieber_learning_2020}{misc}{}
      \name{author}{4}{}{%
        {{hash=3b5d66a47c339bdbcc3a2af47b5aaf96}{%
           family={Bieber},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=fd59f6a0549332a54ac82d8840ea6ff1}{%
           family={Sutton},
           familyi={S\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
        {{hash=42a970b0a0f1ed24b23064370cc9392f}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={Hugo},
           giveni={H\bibinitperiod}}}%
        {{hash=f322f16653f77dad93e3079aeb208b57}{%
           family={Tarlow},
           familyi={T\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{aa14ec4182156c09dcad32546cc1fd80}
      \strng{fullhash}{a546fed6e75460ea9784b4354f363a89}
      \strng{bibnamehash}{a546fed6e75460ea9784b4354f363a89}
      \strng{authorbibnamehash}{a546fed6e75460ea9784b4354f363a89}
      \strng{authornamehash}{aa14ec4182156c09dcad32546cc1fd80}
      \strng{authorfullhash}{a546fed6e75460ea9784b4354f363a89}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graph neural networks (GNNs) have emerged as a powerful tool for learning software engineering tasks including code completion, bug finding, and program repair. They benefit from leveraging program structure like control flow graphs, but they are not well-suited to tasks like program execution that require far more sequential reasoning steps than number of GNN propagation steps. Recurrent neural networks (RNNs), on the other hand, are well-suited to long sequential chains of reasoning, but they do not naturally incorporate program structure and generally perform worse on the above tasks. Our aim is to achieve the best of both worlds, and we do so by introducing a novel GNN architecture, the Instruction Pointer Attention Graph Neural Networks (IPA-GNN), which achieves improved systematic generalization on the task of learning to execute programs using control flow graphs. The model arises by considering RNNs operating on program traces with branch decisions as latent variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN model or as a GNN variant more tailored to execution. To test the models, we propose evaluating systematic generalization on learning to execute using control flow graphs, which tests sequential reasoning and use of program structure. More practically, we evaluate these models on the task of learning to execute partial programs, as might arise if using the model as a heuristic function in program synthesis. Results show that the IPA-GNN outperforms a variety of RNN and GNN baselines on both tasks.}
      \field{month}{10}
      \field{title}{Learning to {{Execute Programs}} with {{Instruction Pointer Attention Graph Neural Networks}}}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/ron/Zotero/storage/VWDKV8D5/Bieber et al. - 2020 - Learning to Execute Programs with Instruction Poin.pdf;/Users/ron/Zotero/storage/LBFFPPB3/2010.html
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{boicho2001analogical}{book}{}
      \name{author}{2}{}{%
        {{hash=ee2659b5e62105202cb587a91a67656c}{%
           family={Boicho},
           familyi={B\bibinitperiod},
           given={Dedre\bibnamedelimb Gentner\bibnamedelimb Keith\bibnamedelimb James\bibnamedelima Holyoak},
           giveni={D\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=4a23e779e5ae5636b88227407bf2a8cf}{%
           family={Kokinov},
           familyi={K\bibinitperiod},
           given={N},
           giveni={N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT press}%
      }
      \strng{namehash}{c36b84de355d29167b85f9cc3b9c4f61}
      \strng{fullhash}{c36b84de355d29167b85f9cc3b9c4f61}
      \strng{bibnamehash}{c36b84de355d29167b85f9cc3b9c4f61}
      \strng{authorbibnamehash}{c36b84de355d29167b85f9cc3b9c4f61}
      \strng{authornamehash}{c36b84de355d29167b85f9cc3b9c4f61}
      \strng{authorfullhash}{c36b84de355d29167b85f9cc3b9c4f61}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{The Analogical Mind: {{Perspectives}} from Cognitive Science}
      \field{year}{2001}
    \endentry
    \entry{bottemannePredictiveMindIntroduction2022}{article}{}
      \name{author}{3}{}{%
        {{hash=d66dc146e9a67ac4ff29fe1bc5b8cff5}{%
           family={Bottemanne},
           familyi={B\bibinitperiod},
           given={H.},
           giveni={H\bibinitperiod}}}%
        {{hash=eaf8c6ec93e536bc5db973a6ac90bebe}{%
           family={Longuet},
           familyi={L\bibinitperiod},
           given={Y.},
           giveni={Y\bibinitperiod}}}%
        {{hash=730bf59108276c96ba0b3c36d90fb682}{%
           family={Gauld},
           familyi={G\bibinitperiod},
           given={C.},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{dbde2ff082f7d14e60b027408772cb76}
      \strng{fullhash}{b0b05a8fdd601ed4084b921d0f557661}
      \strng{bibnamehash}{b0b05a8fdd601ed4084b921d0f557661}
      \strng{authorbibnamehash}{b0b05a8fdd601ed4084b921d0f557661}
      \strng{authornamehash}{dbde2ff082f7d14e60b027408772cb76}
      \strng{authorfullhash}{b0b05a8fdd601ed4084b921d0f557661}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The question of how the mind works is at the heart of cognitive science. It aims to understand and explain the complex processes underlying perception, decision-making and learning, three fundamental areas of cognition. Bayesian Brain Theory, a computational approach derived from the principles of Predictive Processing (PP), offers a mechanistic and mathematical formulation of these cognitive processes. This theory assumes that the brain encodes beliefs (probabilistic states) to generate predictions about sensory input, then uses prediction errors to update its beliefs. In this paper, we present an introduction to the fundamentals of Bayesian Brain Theory. We show how this innovative theory hybridizes concepts inherited from the philosophy of mind and experimental data from neuroscience, and how it translates complex cognitive processes such as perception, action, emotion, or belief, or even the psychiatric symptomatology.}
      \field{issn}{0013-7006}
      \field{journaltitle}{L'Encephale}
      \field{langid}{fre}
      \field{month}{8}
      \field{number}{4}
      \field{shorttitle}{{[The predictive mind}}
      \field{title}{{[The predictive mind: An introduction to Bayesian Brain Theory]}}
      \field{volume}{48}
      \field{year}{2022}
      \field{pages}{436\bibrangedash 444}
      \range{pages}{9}
      \keyw{Bayes Theorem,Bayesian brain,Bayesianism,Bay{é}sianisme,Belief,Belief updating,Brain,Cerveau bay{é}sien,Codage pr{é}dictif,Cognition,Computational neuroscience,Computational psychiatry,Croyance,Emotions,Humans,Interoception,Mise {à} jour des croyances,Neurosciences,Neurosciences computationnelles,Predictive coding,Predictive processing,Traitement pr{é}dictif}
    \endentry
    \entry{cartuyvelsDiscreteContinuousRepresentations2021}{article}{}
      \name{author}{3}{}{%
        {{hash=45912039da07530ad26cb296e428d611}{%
           family={Cartuyvels},
           familyi={C\bibinitperiod},
           given={Ruben},
           giveni={R\bibinitperiod}}}%
        {{hash=224acf9cebac9089dd8380ef20de0a56}{%
           family={Spinks},
           familyi={S\bibinitperiod},
           given={Graham},
           giveni={G\bibinitperiod}}}%
        {{hash=04fe502dceb461055032ff714831f9fd}{%
           family={Moens},
           familyi={M\bibinitperiod},
           given={Marie-Francine},
           giveni={M\bibinithyphendelim F\bibinitperiod}}}%
      }
      \strng{namehash}{fc756ee5f7e9b66790cacd1567ad0193}
      \strng{fullhash}{8fe7d1088ae3d8b3a1a713d54ac6ea91}
      \strng{bibnamehash}{8fe7d1088ae3d8b3a1a713d54ac6ea91}
      \strng{authorbibnamehash}{8fe7d1088ae3d8b3a1a713d54ac6ea91}
      \strng{authornamehash}{fc756ee5f7e9b66790cacd1567ad0193}
      \strng{authorfullhash}{8fe7d1088ae3d8b3a1a713d54ac6ea91}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Discrete and continuous representations of content (e.g., of language or images) have interesting properties to be explored for the understanding of or reasoning with this content by machines. This position paper puts forward our opinion on the role of discrete and continuous representations and their processing in the deep learning field. Current neural network models compute continuous-valued data. Information is compressed into dense, distributed embeddings. By stark contrast, humans use discrete symbols in their communication with language. Such symbols represent a compressed version of the world that derives its meaning from shared contextual information. Additionally, human reasoning involves symbol manipulation at a cognitive level, which facilitates abstract reasoning, the composition of knowledge and understanding, generalization and efficient learning. Motivated by these insights, in this paper we argue that combining discrete and continuous representations and their processing will be essential to build systems that exhibit a general form of intelligence. We suggest and discuss several avenues that could improve current neural networks with the inclusion of discrete elements to combine the advantages of both types of representations.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{issn}{26666510}
      \field{journaltitle}{AI Open}
      \field{shorttitle}{Discrete and Continuous Representations and Processing in Deep Learning}
      \field{title}{Discrete and Continuous Representations and Processing in Deep Learning: {{Looking}} Forward}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{2}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{143\bibrangedash 159}
      \range{pages}{17}
      \verb{eprint}
      \verb 2201.01233
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/V6SN9229/Cartuyvels et al. - 2021 - Discrete and continuous representations and proces.pdf;/Users/ron/Zotero/storage/7GXRL6ZA/2201.html
      \endverb
      \keyw{Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{caucheteuxEvidencePredictiveCoding2023}{article}{}
      \name{author}{3}{}{%
        {{hash=60b0dee36c03f3d79184e3fe4c948885}{%
           family={Caucheteux},
           familyi={C\bibinitperiod},
           given={Charlotte},
           giveni={C\bibinitperiod}}}%
        {{hash=12ebe424e1c87de156c57e8fa7683f46}{%
           family={Gramfort},
           familyi={G\bibinitperiod},
           given={Alexandre},
           giveni={A\bibinitperiod}}}%
        {{hash=d02875daf33f4342c5e20392f5cb4057}{%
           family={King},
           familyi={K\bibinitperiod},
           given={Jean-R{é}mi},
           giveni={J\bibinithyphendelim R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{7b4d20fc0084248b693173021dccbce4}
      \strng{fullhash}{5cf612a5220e44e44e7ce7d423d73d59}
      \strng{bibnamehash}{5cf612a5220e44e44e7ce7d423d73d59}
      \strng{authorbibnamehash}{5cf612a5220e44e44e7ce7d423d73d59}
      \strng{authornamehash}{7b4d20fc0084248b693173021dccbce4}
      \strng{authorfullhash}{5cf612a5220e44e44e7ce7d423d73d59}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.}
      \field{issn}{2397-3374}
      \field{journaltitle}{Nature Human Behaviour}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{3}
      \field{title}{Evidence of a Predictive Coding Hierarchy in the Human Brain Listening to Speech}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{7}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{430\bibrangedash 441}
      \range{pages}{12}
      \verb{file}
      \verb /Users/ron/Zotero/storage/QW6KJIF8/Caucheteux et al. - 2023 - Evidence of a predictive coding hierarchy in the h.pdf
      \endverb
      \keyw{Computational neuroscience,Computer science,Language}
    \endentry
    \entry{cernaAntiunificationGeneralizationSurvey2023}{misc}{}
      \name{author}{2}{}{%
        {{hash=d4ee9347ba34ee20d1a16d567b00df3c}{%
           family={Cerna},
           familyi={C\bibinitperiod},
           given={David\bibnamedelima M.},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=be4a90b8ba0a3cb3667d1a6cdcc4945f}{%
           family={Kutsia},
           familyi={K\bibinitperiod},
           given={Temur},
           giveni={T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{4a5811fd8fff5b25cd7c2170685cf488}
      \strng{fullhash}{4a5811fd8fff5b25cd7c2170685cf488}
      \strng{bibnamehash}{4a5811fd8fff5b25cd7c2170685cf488}
      \strng{authorbibnamehash}{4a5811fd8fff5b25cd7c2170685cf488}
      \strng{authornamehash}{4a5811fd8fff5b25cd7c2170685cf488}
      \strng{authorfullhash}{4a5811fd8fff5b25cd7c2170685cf488}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Anti-unification (AU), also known as generalization, is a fundamental operation used for inductive inference and is the dual operation to unification, an operation at the foundation of theorem proving. Interest in AU from the AI and related communities is growing, but without a systematic study of the concept, nor surveys of existing work, investigations7 often resort to developing application-specific methods that may be covered by existing approaches. We provide the first survey of AU research and its applications, together with a general framework for categorizing existing and future developments.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{2}
      \field{number}{arXiv:2302.00277}
      \field{shorttitle}{Anti-Unification and {{Generalization}}}
      \field{title}{Anti-Unification and {{Generalization}}: {{A Survey}}}
      \field{urlday}{7}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2302.00277
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/Y8T3U7GV/Cerna and Kutsia - 2023 - Anti-unification and Generalization A Survey.pdf;/Users/ron/Zotero/storage/EELRQ6SN/2302.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science}
    \endentry
    \entry{cetinHyperbolicDeepReinforcement2022}{misc}{}
      \name{author}{4}{}{%
        {{hash=aa1696d965e066653b6dcd2ca25b0943}{%
           family={Cetin},
           familyi={C\bibinitperiod},
           given={Edoardo},
           giveni={E\bibinitperiod}}}%
        {{hash=019e35e427c1825f1fa4df90a2bb7416}{%
           family={Chamberlain},
           familyi={C\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=8e2cf15e0e3f2b957c8a1532cd04184f}{%
           family={Bronstein},
           familyi={B\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=e0c5cec59a932511b2af6220473fad61}{%
           family={Hunt},
           familyi={H\bibinitperiod},
           given={Jonathan\bibnamedelima J.},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{84a7bdc9806bd26e3f90000bd597b78a}
      \strng{fullhash}{e463b24fd10a8c70cdc22af5dfcc17c3}
      \strng{bibnamehash}{e463b24fd10a8c70cdc22af5dfcc17c3}
      \strng{authorbibnamehash}{e463b24fd10a8c70cdc22af5dfcc17c3}
      \strng{authornamehash}{84a7bdc9806bd26e3f90000bd597b78a}
      \strng{authorfullhash}{e463b24fd10a8c70cdc22af5dfcc17c3}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a new class of deep reinforcement learning (RL) algorithms that model latent representations in hyperbolic space. Sequential decision-making requires reasoning about the possible future consequences of current behavior. Consequently, capturing the relationship between key evolving features for a given task is conducive to recovering effective policies. To this end, hyperbolic geometry provides deep RL models with a natural basis to precisely encode this inherently hierarchical information. However, applying existing methodologies from the hyperbolic deep learning literature leads to fatal optimization instabilities due to the non-stationarity and variance characterizing RL gradient estimators. Hence, we design a new general method that counteracts such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope future RL research will consider hyperbolic representations as a standard tool.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{10}
      \field{number}{arXiv:2210.01542}
      \field{title}{Hyperbolic {{Deep Reinforcement Learning}}}
      \field{urlday}{7}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2210.01542
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/FR2XBSAI/Cetin et al. - 2022 - Hyperbolic Deep Reinforcement Learning.pdf;/Users/ron/Zotero/storage/Z3WZ6DRB/2210.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{chaterProgramsCausalModels2013}{article}{}
      \name{author}{2}{}{%
        {{hash=68f539b14c5ba5fb2395b9309d555ad7}{%
           family={Chater},
           familyi={C\bibinitperiod},
           given={Nick},
           giveni={N\bibinitperiod}}}%
        {{hash=aafa17fb35a5123e14e0c21659088821}{%
           family={Oaksford},
           familyi={O\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{643f22cc24d3aa120e41351d3621e42a}
      \strng{fullhash}{643f22cc24d3aa120e41351d3621e42a}
      \strng{bibnamehash}{643f22cc24d3aa120e41351d3621e42a}
      \strng{authorbibnamehash}{643f22cc24d3aa120e41351d3621e42a}
      \strng{authornamehash}{643f22cc24d3aa120e41351d3621e42a}
      \strng{authorfullhash}{643f22cc24d3aa120e41351d3621e42a}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Judea Pearl has argued that counterfactuals and causality are central to intelligence, whether natural or artificial, and has helped create a rich mathematical and computational framework for formally analyzing causality. Here, we draw out connections between these notions and various current issues in cognitive science, including the nature of mental ``programs'' and mental representation. We argue that programs (consisting of algorithms and data structures) have a causal (counterfactual-supporting) structure; these counterfactuals can reveal the nature of mental representations. Programs can also provide a causal model of the external world. Such models are, we suggest, ubiquitous in perception, cognition, and language processing.}
      \field{issn}{1551-6709}
      \field{journaltitle}{Cognitive Science}
      \field{langid}{english}
      \field{number}{6}
      \field{shorttitle}{Programs as {{Causal Models}}}
      \field{title}{Programs as {{Causal Models}}: {{Speculations}} on {{Mental Programs}} and {{Mental Representation}}}
      \field{urlday}{29}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{1171\bibrangedash 1191}
      \range{pages}{21}
      \verb{file}
      \verb /Users/ron/Zotero/storage/FHUQEI57/Chater and Oaksford - 2013 - Programs as Causal Models Speculations on Mental .pdf;/Users/ron/Zotero/storage/EEQ3WE73/cogs.html
      \endverb
    \endentry
    \entry{cholletMeasureIntelligence2019}{misc}{}
      \name{author}{1}{}{%
        {{hash=14134a39113404b1f80969c9ccc1f71e}{%
           family={Chollet},
           familyi={C\bibinitperiod},
           given={Fran{ç}ois},
           giveni={F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{14134a39113404b1f80969c9ccc1f71e}
      \strng{fullhash}{14134a39113404b1f80969c9ccc1f71e}
      \strng{bibnamehash}{14134a39113404b1f80969c9ccc1f71e}
      \strng{authorbibnamehash}{14134a39113404b1f80969c9ccc1f71e}
      \strng{authornamehash}{14134a39113404b1f80969c9ccc1f71e}
      \strng{authorfullhash}{14134a39113404b1f80969c9ccc1f71e}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{11}
      \field{number}{arXiv:1911.01547}
      \field{title}{On the {{Measure}} of {{Intelligence}}}
      \field{urlday}{22}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 1911.01547
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/GGGC5PA6/Chollet - 2019 - On the Measure of Intelligence.pdf;/Users/ron/Zotero/storage/KI8UK6H2/1911.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence}
    \endentry
    \entry{colasAutotelicAgentsIntrinsically2022}{misc}{}
      \name{author}{4}{}{%
        {{hash=9ee43015e4e1834db778e5c87e64cfeb}{%
           family={Colas},
           familyi={C\bibinitperiod},
           given={C{é}dric},
           giveni={C\bibinitperiod}}}%
        {{hash=080254be0eed2e741470246ff2646385}{%
           family={Karch},
           familyi={K\bibinitperiod},
           given={Tristan},
           giveni={T\bibinitperiod}}}%
        {{hash=f028dbfa54ab10f0e51f45bc1a5a3d24}{%
           family={Sigaud},
           familyi={S\bibinitperiod},
           given={Olivier},
           giveni={O\bibinitperiod}}}%
        {{hash=81727e3ce281057d5dd20923ea323c95}{%
           family={Oudeyer},
           familyi={O\bibinitperiod},
           given={Pierre-Yves},
           giveni={P\bibinithyphendelim Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{f014dfc08ed0451b4dd3cec6a2d59d9a}
      \strng{fullhash}{b537dde0a3745b130cf781999281260f}
      \strng{bibnamehash}{b537dde0a3745b130cf781999281260f}
      \strng{authorbibnamehash}{b537dde0a3745b130cf781999281260f}
      \strng{authornamehash}{f014dfc08ed0451b4dd3cec6a2d59d9a}
      \strng{authorfullhash}{b537dde0a3745b130cf781999281260f}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by \$autotelic\$ \$agents\$: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: \$developmental\$ \$reinforcement\$ \$learning\$. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem -- the \$intrinsically\$ \$motivated\$ \$acquisition\$ \$of\$ \$open\$-\$ended\$ \$repertoires\$ \$of\$ \$skills\$. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{7}
      \field{number}{arXiv:2012.09830}
      \field{shorttitle}{Autotelic {{Agents}} with {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning}}}
      \field{title}{Autotelic {{Agents}} with {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning}}: A {{Short Survey}}}
      \field{urlday}{13}
      \field{urlmonth}{1}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2012.09830
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/Y5J5KQZC/Colas et al. - 2022 - Autotelic Agents with Intrinsically Motivated Goal.pdf;/Users/ron/Zotero/storage/IP6HTU2U/2012.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
    \endentry
    \entry{garcezNeurosymbolicAI3rd2020}{article}{}
      \name{author}{2}{}{%
        {{hash=70658466246873155f5b9b3a737cdd4e}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur},
           giveni={A\bibinitperiod},
           prefix={d'Avila},
           prefixi={d\bibinitperiod}}}%
        {{hash=8a6bd56db2a68b7d2f595538052b85e9}{%
           family={Lamb},
           familyi={L\bibinitperiod},
           given={Luis\bibnamedelima C.},
           giveni={L\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{1de8e89e99536ec00f7f5ba26015d770}
      \strng{fullhash}{1de8e89e99536ec00f7f5ba26015d770}
      \strng{bibnamehash}{1de8e89e99536ec00f7f5ba26015d770}
      \strng{authorbibnamehash}{1de8e89e99536ec00f7f5ba26015d770}
      \strng{authornamehash}{1de8e89e99536ec00f7f5ba26015d770}
      \strng{authorfullhash}{1de8e89e99536ec00f7f5ba26015d770}
      \field{sortinit}{d}
      \field{sortinithash}{c438b3d5d027251ba63f5ed538d98af5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2012.05876 [cs]}
      \field{month}{12}
      \field{shorttitle}{Neurosymbolic {{AI}}}
      \field{title}{Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}}
      \field{urlday}{3}
      \field{urlmonth}{5}
      \field{urlyear}{2022}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2012.05876
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/XZHCS3J2/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf;/Users/ron/Zotero/storage/WYKZKFLG/2012.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.4,I.2.6}
    \endentry
    \entry{debruijnLambdaCalculusNotation1972}{article}{}
      \name{author}{1}{}{%
        {{hash=c8c9db118200be613ddf80714b8c71d8}{%
           family={{de Bruijn}},
           familyi={d\bibinitperiod},
           given={N.\bibnamedelimi G},
           giveni={N\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{c8c9db118200be613ddf80714b8c71d8}
      \strng{fullhash}{c8c9db118200be613ddf80714b8c71d8}
      \strng{bibnamehash}{c8c9db118200be613ddf80714b8c71d8}
      \strng{authorbibnamehash}{c8c9db118200be613ddf80714b8c71d8}
      \strng{authornamehash}{c8c9db118200be613ddf80714b8c71d8}
      \strng{authorfullhash}{c8c9db118200be613ddf80714b8c71d8}
      \field{sortinit}{d}
      \field{sortinithash}{c438b3d5d027251ba63f5ed538d98af5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In ordinary lambda calculus the occurrences of a bound variable are made recognizable by the use of one and the same (otherwise irrelevant) name at all occurrences. This convention is known to cause considerable trouble in cases of substitution. In the present paper a different notational system is developed, where occurrences of variables are indicated by integers giving the ``distance'' to the binding {$\lambda$} instead of a name attached to that {$\lambda$}. The system is claimed to be efficient for automatic formula manipulation as well as for metalingual discussion. As an example the most essential part of a proof of the Church-Rosser theorem is presented in this namefree calculus.}
      \field{issn}{1385-7258}
      \field{journaltitle}{Indagationes Mathematicae (Proceedings)}
      \field{month}{1}
      \field{number}{5}
      \field{title}{Lambda Calculus Notation with Nameless Dummies, a Tool for Automatic Formula Manipulation, with Application to the {{Church-Rosser}} Theorem}
      \field{urlday}{22}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{volume}{75}
      \field{year}{1972}
      \field{urldateera}{ce}
      \field{pages}{381\bibrangedash 392}
      \range{pages}{12}
      \verb{file}
      \verb /Users/ron/Zotero/storage/PS6QTBPX/de Bruijn - 1972 - Lambda calculus notation with nameless dummies, a .pdf
      \endverb
    \endentry
    \entry{dehaeneSymbolsMentalPrograms2022}{article}{}
      \name{author}{5}{}{%
        {{hash=14ff7c970ba747f58e07de2e42e7c5bb}{%
           family={Dehaene},
           familyi={D\bibinitperiod},
           given={Stanislas},
           giveni={S\bibinitperiod}}}%
        {{hash=1a6972301c9d189783c54f07c6ec2c54}{%
           family={Al\bibnamedelima Roumi},
           familyi={A\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Fosca},
           giveni={F\bibinitperiod}}}%
        {{hash=14dcf5daa1f8aef2b120fd7e5ede0e0e}{%
           family={Lakretz},
           familyi={L\bibinitperiod},
           given={Yair},
           giveni={Y\bibinitperiod}}}%
        {{hash=8d840bd1ec1a4ad86f183ea94f1c1df2}{%
           family={Planton},
           familyi={P\bibinitperiod},
           given={Samuel},
           giveni={S\bibinitperiod}}}%
        {{hash=ac45893539ec3765f93da94b267f6402}{%
           family={{Sabl{é}-Meyer}},
           familyi={S\bibinitperiod},
           given={Mathias},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{40db90f32a9abe431756be2e2abb43d3}
      \strng{fullhash}{41d265d701f6515cf51037a0292057f4}
      \strng{bibnamehash}{41d265d701f6515cf51037a0292057f4}
      \strng{authorbibnamehash}{41d265d701f6515cf51037a0292057f4}
      \strng{authornamehash}{40db90f32a9abe431756be2e2abb43d3}
      \strng{authorfullhash}{41d265d701f6515cf51037a0292057f4}
      \field{sortinit}{D}
      \field{sortinithash}{c438b3d5d027251ba63f5ed538d98af5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{issn}{13646613}
      \field{journaltitle}{Trends in Cognitive Sciences}
      \field{langid}{english}
      \field{month}{9}
      \field{number}{9}
      \field{shorttitle}{Symbols and Mental Programs}
      \field{title}{Symbols and Mental Programs: A Hypothesis about Human Singularity}
      \field{urlday}{22}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{volume}{26}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{751\bibrangedash 766}
      \range{pages}{16}
      \verb{file}
      \verb /Users/ron/Zotero/storage/3QSTG9EM/Dehaene et al. - 2022 - Symbols and mental programs a hypothesis about hu.pdf
      \endverb
    \endentry
    \entry{deleuBayesianStructureLearning2022}{misc}{}
      \name{author}{7}{}{%
        {{hash=8a006dd742c8f7a1e71c8d5b862abaec}{%
           family={Deleu},
           familyi={D\bibinitperiod},
           given={Tristan},
           giveni={T\bibinitperiod}}}%
        {{hash=6345912b14f6e7ae2dd5e8e3b3ba995d}{%
           family={G{ó}is},
           familyi={G\bibinitperiod},
           given={Ant{ó}nio},
           giveni={A\bibinitperiod}}}%
        {{hash=6fd57dc27bc0668e4235d9a33cba2cd0}{%
           family={Emezue},
           familyi={E\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=80e12d72b65f660a2065209175dde672}{%
           family={Rankawat},
           familyi={R\bibinitperiod},
           given={Mansi},
           giveni={M\bibinitperiod}}}%
        {{hash=19ce069c35db05c86f9cdb9d9e6e3ce7}{%
           family={{Lacoste-Julien}},
           familyi={L\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=6e0afe0dd7f80126ec663a279b8d6212}{%
           family={Bauer},
           familyi={B\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{6b33a2a43bfac8a3e20d67343acee0ef}
      \strng{fullhash}{bf7a665867183940ac0116c14f69f850}
      \strng{bibnamehash}{6b33a2a43bfac8a3e20d67343acee0ef}
      \strng{authorbibnamehash}{6b33a2a43bfac8a3e20d67343acee0ef}
      \strng{authornamehash}{6b33a2a43bfac8a3e20d67343acee0ef}
      \strng{authorfullhash}{bf7a665867183940ac0116c14f69f850}
      \field{sortinit}{D}
      \field{sortinithash}{c438b3d5d027251ba63f5ed538d98af5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{6}
      \field{number}{arXiv:2202.13903}
      \field{title}{Bayesian {{Structure Learning}} with {{Generative Flow Networks}}}
      \field{urlday}{25}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2202.13903
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/N9KGWTUX/Deleu et al. - 2022 - Bayesian Structure Learning with Generative Flow N.pdf;/Users/ron/Zotero/storage/UARUUKZQ/2202.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{doNeuralCircuitsSymbolic2021}{article}{}
      \name{author}{2}{}{%
        {{hash=1550ef4940a15c977aabd6221bb6f90b}{%
           family={Do},
           familyi={D\bibinitperiod},
           given={Quan},
           giveni={Q\bibinitperiod}}}%
        {{hash=7c84b4caeb0a2ee7da2e81c4d9825118}{%
           family={Hasselmo},
           familyi={H\bibinitperiod},
           given={Michael\bibnamedelima E.},
           giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{d5a1858c7de0e7a8f507fcf5a91ff1fd}
      \strng{fullhash}{d5a1858c7de0e7a8f507fcf5a91ff1fd}
      \strng{bibnamehash}{d5a1858c7de0e7a8f507fcf5a91ff1fd}
      \strng{authorbibnamehash}{d5a1858c7de0e7a8f507fcf5a91ff1fd}
      \strng{authornamehash}{d5a1858c7de0e7a8f507fcf5a91ff1fd}
      \strng{authorfullhash}{d5a1858c7de0e7a8f507fcf5a91ff1fd}
      \field{sortinit}{D}
      \field{sortinithash}{c438b3d5d027251ba63f5ed538d98af5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The ability to use symbols is a defining feature of human intelligence. However, neuroscience has yet to explain the fundamental neural circuit mechanisms for flexibly representing and manipulating abstract concepts. This article will review the research on neural models for symbolic processing. The review first focuses on the question of how symbols could possibly be represented in neural circuits. The review then addresses how neural symbolic representations could be flexibly combined to meet a wide range of reasoning demands. Finally, the review assesses the research on program synthesis and proposes that the most flexible neural representation of symbolic processing would involve the capacity to rapidly synthesize neural operations analogous to lambda calculus to solve complex cognitive tasks.}
      \field{issn}{1074-7427}
      \field{journaltitle}{Neurobiology of Learning and Memory}
      \field{langid}{english}
      \field{month}{12}
      \field{title}{Neural Circuits and Symbolic Processing}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{186}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{107552}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/CBEQBKQH/Do and Hasselmo - 2021 - Neural circuits and symbolic processing.pdf;/Users/ron/Zotero/storage/HLCDM7P8/S107474272100174X.html
      \endverb
      \keyw{Bayesian inference,Category theory,Conjunctive coding,Dynamic binding,Program synthesis,Reinforcement learning}
    \endentry
    \entry{ellisDreamCoderBootstrappingInductive2021}{inproceedings}{}
      \name{author}{9}{}{%
        {{hash=5404603ffd5dfdff4037647ecf8c172f}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=3ccda8dbf8aa43ce7053f7188d82b788}{%
           family={Wong},
           familyi={W\bibinitperiod},
           given={Catherine},
           giveni={C\bibinitperiod}}}%
        {{hash=49d657a6cda3ddc6f434500001ea6d42}{%
           family={Nye},
           familyi={N\bibinitperiod},
           given={Maxwell},
           giveni={M\bibinitperiod}}}%
        {{hash=ac45893539ec3765f93da94b267f6402}{%
           family={{Sabl{é}-Meyer}},
           familyi={S\bibinitperiod},
           given={Mathias},
           giveni={M\bibinitperiod}}}%
        {{hash=e6168da08ea78e9586145bb1ffb61902}{%
           family={Morales},
           familyi={M\bibinitperiod},
           given={Lucas},
           giveni={L\bibinitperiod}}}%
        {{hash=8707430b1cd45f60c731e52ea12046c5}{%
           family={Hewitt},
           familyi={H\bibinitperiod},
           given={Luke},
           giveni={L\bibinitperiod}}}%
        {{hash=908f57d4bc5c176f77d1062ec2504e8f}{%
           family={Cary},
           familyi={C\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod}}}%
        {{hash=6c3f3e278b534563f641aca05842bd0a}{%
           family={{Solar-Lezama}},
           familyi={S\bibinitperiod},
           given={Armando},
           giveni={A\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Virtual Canada}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{1b907c772c4ae07ba37a05178135bff9}
      \strng{fullhash}{d1873997cdf93b8d1de9297d491ede11}
      \strng{bibnamehash}{1b907c772c4ae07ba37a05178135bff9}
      \strng{authorbibnamehash}{1b907c772c4ae07ba37a05178135bff9}
      \strng{authornamehash}{1b907c772c4ae07ba37a05178135bff9}
      \strng{authorfullhash}{d1873997cdf93b8d1de9297d491ede11}
      \field{sortinit}{E}
      \field{sortinithash}{c554bd1a0b76ea92b9f105fe36d9c7b0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of {ł}wake-sleep{ž} approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.}
      \field{booktitle}{Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}}
      \field{isbn}{978-1-4503-8391-2}
      \field{langid}{english}
      \field{month}{6}
      \field{shorttitle}{{{DreamCoder}}}
      \field{title}{{{DreamCoder}}: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning}
      \field{urlday}{4}
      \field{urlmonth}{5}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{835\bibrangedash 850}
      \range{pages}{16}
      \verb{file}
      \verb /Users/ron/Zotero/storage/IADX3AGN/Ellis et al. - 2021 - DreamCoder bootstrapping inductive program synthe.pdf
      \endverb
    \endentry
    \entry{fijalkowScalingNeuralProgram2021}{misc}{}
      \name{author}{6}{}{%
        {{hash=797b350173c629853fe708683a9df179}{%
           family={Fijalkow},
           familyi={F\bibinitperiod},
           given={Nathana{ë}l},
           giveni={N\bibinitperiod}}}%
        {{hash=cca9a436f50a93973c3f2d6d2acfe921}{%
           family={Lagarde},
           familyi={L\bibinitperiod},
           given={Guillaume},
           giveni={G\bibinitperiod}}}%
        {{hash=c7526a409c9d145ae9ac6a7638bdf990}{%
           family={Matricon},
           familyi={M\bibinitperiod},
           given={Th{é}o},
           giveni={T\bibinitperiod}}}%
        {{hash=5404603ffd5dfdff4037647ecf8c172f}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=4446b1a5d9054d6a862ad45c2aceeaee}{%
           family={Ohlmann},
           familyi={O\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod}}}%
        {{hash=a55414946e3c8277900f0f8000cc7523}{%
           family={Potta},
           familyi={P\bibinitperiod},
           given={Akarsh},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{b3e89b376f204a5f51caf1f3c4af7269}
      \strng{fullhash}{ff83b22b5a2d27a8ea39c2d9c42ce215}
      \strng{bibnamehash}{b3e89b376f204a5f51caf1f3c4af7269}
      \strng{authorbibnamehash}{b3e89b376f204a5f51caf1f3c4af7269}
      \strng{authornamehash}{b3e89b376f204a5f51caf1f3c4af7269}
      \strng{authorfullhash}{ff83b22b5a2d27a8ea39c2d9c42ce215}
      \field{sortinit}{F}
      \field{sortinithash}{fb0c0faa89eb6abae8213bf60e6799ea}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the problem of automatically constructing computer programs from input-output examples. We investigate how to augment probabilistic and neural program synthesis methods with new search algorithms, proposing a framework called distribution-based search. Within this framework, we introduce two new search algorithms: Heap Search, an enumerative method, and SQRT Sampling, a probabilistic method. We prove certain optimality guarantees for both methods, show how they integrate with probabilistic and neural techniques, and demonstrate how they can operate at scale across parallel compute environments. Collectively these findings offer theoretical and applied studies of search algorithms for program synthesis that integrate with recent developments in machine-learned program synthesizers.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{10}
      \field{number}{arXiv:2110.12485}
      \field{title}{Scaling {{Neural Program Synthesis}} with {{Distribution-based Search}}}
      \field{urlday}{15}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2110.12485
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/RG9DAQXS/Fijalkow et al. - 2021 - Scaling Neural Program Synthesis with Distribution.pdf;/Users/ron/Zotero/storage/HZ7HYXZT/2110.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages}
    \endentry
    \entry{Fodor_Pylyshyn_1988}{article}{}
      \name{author}{2}{}{%
        {{hash=99a8aced15864fc92f53f1bb39897f06}{%
           family={Fodor},
           familyi={F\bibinitperiod},
           given={Jerry\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=6f610e8fce577b44007d2d4c9bdb3686}{%
           family={Pylyshyn},
           familyi={P\bibinitperiod},
           given={Zenon\bibnamedelima W.},
           giveni={Z\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{8ea1073583acb011b401d2b74a54e1ae}
      \strng{fullhash}{8ea1073583acb011b401d2b74a54e1ae}
      \strng{bibnamehash}{8ea1073583acb011b401d2b74a54e1ae}
      \strng{authorbibnamehash}{8ea1073583acb011b401d2b74a54e1ae}
      \strng{authornamehash}{8ea1073583acb011b401d2b74a54e1ae}
      \strng{authorfullhash}{8ea1073583acb011b401d2b74a54e1ae}
      \field{sortinit}{F}
      \field{sortinithash}{fb0c0faa89eb6abae8213bf60e6799ea}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{annotation}{Abstract Note: This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a `language of thought': i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the `systematicity' of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or `abstract neurological') structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation. R{é}sum{é} Cet article{é}tudie les diff{é}rences entre mod{è}les connectionistes et mod{è}les classiques de la structure cognitive. Nous pensons que, bien que les deux types de mod{è}les stipulent l'existence d'{é}tats mentaux repr{é}sentationnels, la diff{é}rence essentielle est que seuls les mod{è}les classiques requi{è}rent l'existence d'un niveau de repr{é}sentation symbolique—un ``langage de la pens{é}e''—, c'est-{à}-dire d'{é}tats repr{é}sentationnels poss{é}dant une structure syntaxique et s{é}mantique. Nous examinons ensuite diff{é}rents arguments qui militent en faveur de l'existence de repr{é}sentations mentales ayant ces propri{é}t{é}s. Certains de ces arguments reposent sur la ``syst{é}maticit{é}'' des repr{é}sentations mentales, c'est-{à}-dire sur le fait que les capacit{é}s cognitives exhibent toujours certaines sym{é}tries, de sorte que la capacit{é}d'entretenir certaines pens{é}es implique la capacit{é}d'entretenir d'autres pens{é}es apparent{é}es par leur contenu s{é}mantique. Nous pensons que ces arguments montrent de mani{è}re convainquante que l'architecture de l'esprit/du cerveau n'est pas connectioniste au niveau cognitif. Nous nous demandons ensuite s'il est possible d'interpr{é}ter le connectionisme comme une analyse des structures neuronales (ou des structures neurologiques ``abstraites'') dans lesquelles est r{é}alis{é}e l'architecture cognitive classique. Nous examinons plusieurs des arguments avanc{é}s habituellement en d{é}fense du connectionisme, et en concluons que ceux-ci n'ont de sens que dans cette interpr{é}tation.}
      \field{issn}{0010-0277}
      \field{journaltitle}{Cognition}
      \field{month}{3}
      \field{number}{1}
      \field{title}{Connectionism and Cognitive Architecture: {{A}} Critical Analysis}
      \field{volume}{28}
      \field{year}{1988}
      \field{pages}{3\bibrangedash 71}
      \range{pages}{69}
    \endentry
    \entry{fristonWorldModelLearning2021}{article}{}
      \name{author}{6}{}{%
        {{hash=9f452a84796a900973333b32a5f8bc61}{%
           family={Friston},
           familyi={F\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
        {{hash=556462bfe7bb42eba9a687ff4153f957}{%
           family={Moran},
           familyi={M\bibinitperiod},
           given={Rosalyn\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=2560c027c1fa184105e02c67f78711e7}{%
           family={Nagai},
           familyi={N\bibinitperiod},
           given={Yukie},
           giveni={Y\bibinitperiod}}}%
        {{hash=3b8729c76cc1a627c6b234fad87a5908}{%
           family={Taniguchi},
           familyi={T\bibinitperiod},
           given={Tadahiro},
           giveni={T\bibinitperiod}}}%
        {{hash=5216cb5ae5f2f9273b0c8cf82776f42c}{%
           family={Gomi},
           familyi={G\bibinitperiod},
           given={Hiroaki},
           giveni={H\bibinitperiod}}}%
        {{hash=48de56ec34034a2f537d9cc56f0774d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{fullhash}{c6aa572b62271607d17553ddb62d588c}
      \strng{bibnamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authorbibnamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authornamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authorfullhash}{c6aa572b62271607d17553ddb62d588c}
      \field{sortinit}{F}
      \field{sortinithash}{fb0c0faa89eb6abae8213bf60e6799ea}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Understanding information processing in the brain—and creating general-purpose artificial intelligence—are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{langid}{english}
      \field{month}{12}
      \field{title}{World Model Learning and Inference}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{144}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{573\bibrangedash 590}
      \range{pages}{18}
      \verb{file}
      \verb /Users/ron/Zotero/storage/BHMH7H34/Friston et al. - 2021 - World model learning and inference.pdf;/Users/ron/Zotero/storage/HH7D6IZB/S0893608021003610.html
      \endverb
      \keyw{Bayesian inference,Cognitive development,Free energy principle,Generative model,Predictive coding,Probabilistic inference}
    \endentry
    \entry{goyalInductiveBiasesDeep2022}{article}{}
      \name{author}{2}{}{%
        {{hash=eb6a43771a55bde951c0119ac5c7ad13}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Anirudh},
           giveni={A\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Royal Society}%
      }
      \strng{namehash}{eb8f315f8ada558445b113512c711116}
      \strng{fullhash}{eb8f315f8ada558445b113512c711116}
      \strng{bibnamehash}{eb8f315f8ada558445b113512c711116}
      \strng{authorbibnamehash}{eb8f315f8ada558445b113512c711116}
      \strng{authornamehash}{eb8f315f8ada558445b113512c711116}
      \strng{authorfullhash}{eb8f315f8ada558445b113512c711116}
      \field{sortinit}{G}
      \field{sortinithash}{62eb2aa29549e4fdbd3cb154ec5711cb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.}
      \field{journaltitle}{Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences}
      \field{month}{10}
      \field{number}{2266}
      \field{title}{Inductive Biases for Deep Learning of Higher-Level Cognition}
      \field{urlday}{9}
      \field{urlmonth}{2}
      \field{urlyear}{2023}
      \field{volume}{478}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{20210068}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/N28TYQQK/Goyal and Bengio - 2022 - Inductive biases for deep learning of higher-level.pdf
      \endverb
      \keyw{causality,deep learning,reasoning,system 2,systematic and out-of-distribution generalization}
    \endentry
    \entry{griffithsBayesianModelsCognition}{article}{}
      \name{author}{3}{}{%
        {{hash=15f19cfb916ba0b8afb494f250ea80a2}{%
           family={Griffiths},
           familyi={G\bibinitperiod},
           given={Thomas\bibnamedelima L},
           giveni={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=d3d5c11491b1f45f9d77946df28b87a9}{%
           family={Kemp},
           familyi={K\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{42109f484fab9100385a11b133426195}
      \strng{fullhash}{c5646d4094545fec5d0aad11ed6fd374}
      \strng{bibnamehash}{c5646d4094545fec5d0aad11ed6fd374}
      \strng{authorbibnamehash}{c5646d4094545fec5d0aad11ed6fd374}
      \strng{authornamehash}{42109f484fab9100385a11b133426195}
      \strng{authorfullhash}{c5646d4094545fec5d0aad11ed6fd374}
      \field{extraname}{1}
      \field{sortinit}{G}
      \field{sortinithash}{62eb2aa29549e4fdbd3cb154ec5711cb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{BAYESIAN MODELS}
      \field{langid}{english}
      \field{title}{Bayesian Models of Cognition}
      \verb{file}
      \verb /Users/ron/Zotero/storage/TRMLQ8WZ/Griﬃths et al. - Bayesian models of cognition.pdf
      \endverb
    \endentry
    \entry{Griffiths_Zhu_Grant_McCoy_2023}{article}{}
      \name{author}{4}{}{%
        {{hash=15f19cfb916ba0b8afb494f250ea80a2}{%
           family={Griffiths},
           familyi={G\bibinitperiod},
           given={Thomas\bibnamedelima L.},
           giveni={T\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=245e8a0e5ec9197531ea6432f477f0ad}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Jian-Qiao},
           giveni={J\bibinithyphendelim Q\bibinitperiod}}}%
        {{hash=1ff7c28ba0cef32a7a3c9ead42460199}{%
           family={Grant},
           familyi={G\bibinitperiod},
           given={Erin},
           giveni={E\bibinitperiod}}}%
        {{hash=e12de261223703598382d4e3c6de3d6d}{%
           family={McCoy},
           familyi={M\bibinitperiod},
           given={R.\bibnamedelimi Thomas},
           giveni={R\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{42109f484fab9100385a11b133426195}
      \strng{fullhash}{0817da4531947d490f04720da96f55e0}
      \strng{bibnamehash}{0817da4531947d490f04720da96f55e0}
      \strng{authorbibnamehash}{0817da4531947d490f04720da96f55e0}
      \strng{authornamehash}{42109f484fab9100385a11b133426195}
      \strng{authorfullhash}{0817da4531947d490f04720da96f55e0}
      \field{extraname}{2}
      \field{sortinit}{G}
      \field{sortinithash}{62eb2aa29549e4fdbd3cb154ec5711cb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{11}
      \field{note}{arXiv:2311.10206 [cs]}
      \field{number}{arXiv:2311.10206}
      \field{title}{Bayes in the age of intelligent machines}
      \field{year}{2023}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2311.10206
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2311.10206
      \endverb
    \endentry
    \entry{gulwaniProgramSynthesis2017}{book}{}
      \name{author}{3}{}{%
        {{hash=0cfa8554eaf674605b00a55cb88d5e1a}{%
           family={Gulwani},
           familyi={G\bibinitperiod},
           given={Sumit},
           giveni={S\bibinitperiod}}}%
        {{hash=21029155fee7b53a534e92b5dca700ba}{%
           family={Polozov},
           familyi={P\bibinitperiod},
           given={Oleksandr},
           giveni={O\bibinitperiod}}}%
        {{hash=975baf0ae00cfc98fb6c09483005986b}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Rishabh},
           giveni={R\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Hanover, MA Delft}%
      }
      \list{publisher}{1}{%
        {Now Publishers}%
      }
      \strng{namehash}{255df3ed8bfac3bbbc303d9862104cd9}
      \strng{fullhash}{3b8045501107545082fed1c41b28bf76}
      \strng{bibnamehash}{3b8045501107545082fed1c41b28bf76}
      \strng{authorbibnamehash}{3b8045501107545082fed1c41b28bf76}
      \strng{authornamehash}{255df3ed8bfac3bbbc303d9862104cd9}
      \strng{authorfullhash}{3b8045501107545082fed1c41b28bf76}
      \field{sortinit}{G}
      \field{sortinithash}{62eb2aa29549e4fdbd3cb154ec5711cb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-1-68083-292-1}
      \field{langid}{english}
      \field{number}{4.2017, 1-2}
      \field{series}{Foundations and Trends in Programming Languages}
      \field{title}{Program Synthesis}
      \field{year}{2017}
      \verb{file}
      \verb /Users/ron/Zotero/storage/9UZ68Y9G/Gulwani et al. - 2017 - Program synthesis.pdf
      \endverb
    \endentry
    \entry{han2022data}{book}{}
      \name{author}{3}{}{%
        {{hash=7cacfe272c4d395c979d6aecd2f5ec9c}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Jiawei},
           giveni={J\bibinitperiod}}}%
        {{hash=56af930e1f5e2c9a791b71e7b347c0ba}{%
           family={Pei},
           familyi={P\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
        {{hash=accfc7f5c581f746112f5fd641490dda}{%
           family={Tong},
           familyi={T\bibinitperiod},
           given={Hanghang},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Morgan kaufmann}%
      }
      \strng{namehash}{98470ec376276ab821f0f5c1661aaa94}
      \strng{fullhash}{04b214a9d6966be1d9d97c2da2bbc9e5}
      \strng{bibnamehash}{04b214a9d6966be1d9d97c2da2bbc9e5}
      \strng{authorbibnamehash}{04b214a9d6966be1d9d97c2da2bbc9e5}
      \strng{authornamehash}{98470ec376276ab821f0f5c1661aaa94}
      \strng{authorfullhash}{04b214a9d6966be1d9d97c2da2bbc9e5}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Data Mining: Concepts and Techniques}
      \field{year}{2022}
    \endentry
    \entry{heTreesTransformersTheoretical2021}{misc}{}
      \name{author}{3}{}{%
        {{hash=954a8b117e47bd74e7e7fc588d404cc1}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Qi},
           giveni={Q\bibinitperiod}}}%
        {{hash=55f408d4f03c90fb75393e721053df3d}{%
           family={Sedoc},
           familyi={S\bibinitperiod},
           given={Jo{ã}o},
           giveni={J\bibinitperiod}}}%
        {{hash=4ddc0f611481e97af21b1fd603df9a21}{%
           family={Rodu},
           familyi={R\bibinitperiod},
           given={Jordan},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{812d5e56fd8d1feeb26f5c9935811908}
      \strng{fullhash}{5b9909d002612c516746bf2773eb7d3a}
      \strng{bibnamehash}{5b9909d002612c516746bf2773eb7d3a}
      \strng{authorbibnamehash}{5b9909d002612c516746bf2773eb7d3a}
      \strng{authornamehash}{812d5e56fd8d1feeb26f5c9935811908}
      \strng{authorfullhash}{5b9909d002612c516746bf2773eb7d3a}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Transformer networks are the de facto standard architecture in natural language processing. To date, there are no theoretical analyses of the Transformer's ability to capture tree structures. We focus on the ability of Transformer networks to learn tree structures that are important for tree transduction problems. We first analyze the theoretical capability of the standard Transformer architecture to learn tree structures given enumeration of all possible tree backbones, which we define as trees without labels. We then prove that two linear layers with ReLU activation function can recover any tree backbone from any two nonzero, linearly independent starting backbones. This implies that a Transformer can learn tree structures well in theory. We conduct experiments with synthetic data and find that the standard Transformer achieves similar accuracy compared to a Transformer where tree position information is explicitly encoded, albeit with slower convergence. This confirms empirically that Transformers can learn tree structures.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{12}
      \field{number}{arXiv:2112.11913}
      \field{shorttitle}{Trees in Transformers}
      \field{title}{Trees in Transformers: A Theoretical Analysis of the {{Transformer}}'s Ability to Represent Trees}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2112.11913
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/M8QIKVIP/He et al. - 2021 - Trees in transformers a theoretical analysis of t.pdf;/Users/ron/Zotero/storage/PLW2GDFZ/2112.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{hintonHowRepresentPartwhole2021}{misc}{}
      \name{author}{1}{}{%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{fullhash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{bibnamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authorbibnamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authornamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authorfullhash}{9a8750ccdb2a4cf14d2655face1ce016}
      \field{extraname}{1}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{2}
      \field{number}{arXiv:2102.12627}
      \field{title}{How to Represent Part-Whole Hierarchies in a Neural Network}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2102.12627
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/PXJDMPZG/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf;/Users/ron/Zotero/storage/KS4BB9LM/2102.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,I.2.6,I.4.8}
    \endentry
    \entry{hinton2002training}{article}{}
      \name{author}{1}{}{%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{813bd95fe553e6079cd53a567b238287}
      \strng{fullhash}{813bd95fe553e6079cd53a567b238287}
      \strng{bibnamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authorbibnamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authornamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authorfullhash}{813bd95fe553e6079cd53a567b238287}
      \field{extraname}{2}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Neural computation}
      \field{number}{8}
      \field{title}{Training Products of Experts by Minimizing Contrastive Divergence}
      \field{volume}{14}
      \field{year}{2002}
      \field{pages}{1771\bibrangedash 1800}
      \range{pages}{30}
    \endentry
    \entry{hinton1995wake}{article}{}
      \name{author}{4}{}{%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=ef0afbe0b4e15059c963e026fe213c34}{%
           family={Dayan},
           familyi={D\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=11002922cf690abf8522c8fbb1eb5b92}{%
           family={Frey},
           familyi={F\bibinitperiod},
           given={Brendan\bibnamedelima J},
           giveni={B\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=f366a77ba01bf711161e56bcc244e7f2}{%
           family={Neal},
           familyi={N\bibinitperiod},
           given={Radford\bibnamedelima M},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Association for the Advancement of Science}%
      }
      \strng{namehash}{7c3a69040c0122d64c38f463aa1410df}
      \strng{fullhash}{7648431919298390cb1ca5f41ca080bc}
      \strng{bibnamehash}{7648431919298390cb1ca5f41ca080bc}
      \strng{authorbibnamehash}{7648431919298390cb1ca5f41ca080bc}
      \strng{authornamehash}{7c3a69040c0122d64c38f463aa1410df}
      \strng{authorfullhash}{7648431919298390cb1ca5f41ca080bc}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Science (New York, N.Y.)}
      \field{number}{5214}
      \field{title}{The" Wake-Sleep" Algorithm for Unsupervised Neural Networks}
      \field{volume}{268}
      \field{year}{1995}
      \field{pages}{1158\bibrangedash 1161}
      \range{pages}{4}
    \endentry
    \entry{hofstadter2013surfaces}{book}{}
      \name{author}{2}{}{%
        {{hash=18508284242debcc15c15f04d32c3467}{%
           family={Hofstadter},
           familyi={H\bibinitperiod},
           given={Douglas},
           giveni={D\bibinitperiod}}}%
        {{hash=03ef3234e5492b57e8b1a03032d29fe2}{%
           family={Sander},
           familyi={S\bibinitperiod},
           given={Emmanuel},
           giveni={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Basic Books}%
      }
      \strng{namehash}{1939da1600f9adde2265dd8c9f34c58b}
      \strng{fullhash}{1939da1600f9adde2265dd8c9f34c58b}
      \strng{bibnamehash}{1939da1600f9adde2265dd8c9f34c58b}
      \strng{authorbibnamehash}{1939da1600f9adde2265dd8c9f34c58b}
      \strng{authornamehash}{1939da1600f9adde2265dd8c9f34c58b}
      \strng{authorfullhash}{1939da1600f9adde2265dd8c9f34c58b}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Surfaces and Essences: {{Analogy}} as the Fuel and Fire of Thinking}
      \field{year}{2013}
    \endentry
    \entry{Hu_Malkin_Jain_Everett_Graikos_Bengio_2023}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=e6a95d07b8ae7f14e4eb777c6980a059}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Edward\bibnamedelima J.},
           giveni={E\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=c6bdc75ef5c2c84edff84fdad7d7a93b}{%
           family={Malkin},
           familyi={M\bibinitperiod},
           given={Nikolay},
           giveni={N\bibinitperiod}}}%
        {{hash=93d11841cec92dc61eb3d903b3ac2002}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Moksh},
           giveni={M\bibinitperiod}}}%
        {{hash=61a982e32365a8e00ff79d71b4a83dc9}{%
           family={Everett},
           familyi={E\bibinitperiod},
           given={Katie\bibnamedelima E.},
           giveni={K\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=1be47bc141e55d07981b2bee63bf681d}{%
           family={Graikos},
           familyi={G\bibinitperiod},
           given={Alexandros},
           giveni={A\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{d775178c34083951ab1e07df578582c3}
      \strng{fullhash}{67c55156fe0f3d138982606d4b18fa6f}
      \strng{bibnamehash}{d775178c34083951ab1e07df578582c3}
      \strng{authorbibnamehash}{d775178c34083951ab1e07df578582c3}
      \strng{authornamehash}{d775178c34083951ab1e07df578582c3}
      \strng{authorfullhash}{67c55156fe0f3d138982606d4b18fa6f}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 40th International Conference on Machine Learning}
      \field{issn}{2640-3498}
      \field{month}{7}
      \field{title}{GFlowNet-EM for Learning Compositional Latent Variable Models}
      \field{year}{2023}
      \field{pages}{13528\bibrangedash 13549}
      \range{pages}{22}
    \endentry
    \entry{ibarz_generalist_2022}{misc}{}
      \name{author}{15}{}{%
        {{hash=9aec8d0168b98a6e60f4cc97834d0d76}{%
           family={Ibarz},
           familyi={I\bibinitperiod},
           given={Borja},
           giveni={B\bibinitperiod}}}%
        {{hash=41120cd3ecef9fc9fa0bf14a7f0db310}{%
           family={Kurin},
           familyi={K\bibinitperiod},
           given={Vitaly},
           giveni={V\bibinitperiod}}}%
        {{hash=eab39f18cad46e4abd05bb4d62b340cd}{%
           family={Papamakarios},
           familyi={P\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
        {{hash=e429d9b3d068dba82fcab6d30dedda92}{%
           family={Nikiforou},
           familyi={N\bibinitperiod},
           given={Kyriacos},
           giveni={K\bibinitperiod}}}%
        {{hash=e20d3878c18c24bdfc7feeac9b6073a6}{%
           family={Bennani},
           familyi={B\bibinitperiod},
           given={Mehdi},
           giveni={M\bibinitperiod}}}%
        {{hash=612fda6a225fc99663b98d48443de287}{%
           family={Csord{á}s},
           familyi={C\bibinitperiod},
           given={R{ó}bert},
           giveni={R\bibinitperiod}}}%
        {{hash=76a7579d94d000dca5e0071fb3b69382}{%
           family={Dudzik},
           familyi={D\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=243e17666a6bd2b98c5db4b0971d0146}{%
           family={Bo{š}njak},
           familyi={B\bibinitperiod},
           given={Matko},
           giveni={M\bibinitperiod}}}%
        {{hash=bdc16300b676174e42fb52f6bd46e0ba}{%
           family={Vitvitskyi},
           familyi={V\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=d7891653274cb8d85e3a83b41a0c2067}{%
           family={Rubanova},
           familyi={R\bibinitperiod},
           given={Yulia},
           giveni={Y\bibinitperiod}}}%
        {{hash=1f4d8c376133008af929850877ff1d58}{%
           family={Deac},
           familyi={D\bibinitperiod},
           given={Andreea},
           giveni={A\bibinitperiod}}}%
        {{hash=bcdca3ada17ba985d10e15f2f4c6a65a}{%
           family={Bevilacqua},
           familyi={B\bibinitperiod},
           given={Beatrice},
           giveni={B\bibinitperiod}}}%
        {{hash=08ddf7ec3c94815436149213b48c1dc1}{%
           family={Ganin},
           familyi={G\bibinitperiod},
           given={Yaroslav},
           giveni={Y\bibinitperiod}}}%
        {{hash=24fe1af011227491e54e299ba4cb24b5}{%
           family={Blundell},
           familyi={B\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
        {{hash=b1208a71c9b067bcef8fb29c15a09b5d}{%
           family={Veli{č}kovi{ć}},
           familyi={V\bibinitperiod},
           given={Petar},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{60d2517c74c4c55a5676c8a58a9686fd}
      \strng{fullhash}{c54317804b24e9afb9ff35ae16d7bf16}
      \strng{bibnamehash}{60d2517c74c4c55a5676c8a58a9686fd}
      \strng{authorbibnamehash}{60d2517c74c4c55a5676c8a58a9686fd}
      \strng{authornamehash}{60d2517c74c4c55a5676c8a58a9686fd}
      \strng{authorfullhash}{c54317804b24e9afb9ff35ae16d7bf16}
      \field{sortinit}{I}
      \field{sortinithash}{9417e9a1288a9371e2691d999083ed39}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner – a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20\% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.}
      \field{month}{12}
      \field{title}{A {{Generalist Neural Algorithmic Learner}}}
      \field{urlday}{12}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/ron/Zotero/storage/6LEFUTWY/Ibarz et al. - 2022 - A Generalist Neural Algorithmic Learner.pdf;/Users/ron/Zotero/storage/IASZ9HIB/2209.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{joshi2020transformers}{article}{}
      \name{author}{1}{}{%
        {{hash=d18ed5539dd780de8fcb275079bd68d6}{%
           family={Joshi},
           familyi={J\bibinitperiod},
           given={Chaitanya},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{d18ed5539dd780de8fcb275079bd68d6}
      \strng{fullhash}{d18ed5539dd780de8fcb275079bd68d6}
      \strng{bibnamehash}{d18ed5539dd780de8fcb275079bd68d6}
      \strng{authorbibnamehash}{d18ed5539dd780de8fcb275079bd68d6}
      \strng{authornamehash}{d18ed5539dd780de8fcb275079bd68d6}
      \strng{authorfullhash}{d18ed5539dd780de8fcb275079bd68d6}
      \field{sortinit}{J}
      \field{sortinithash}{c45040a764d616897e7f5b30174d7b92}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The Gradient}
      \field{title}{Transformers Are Graph Neural Networks}
      \field{volume}{7}
      \field{year}{2020}
    \endentry
    \entry{Juechems_Summerfield_2019}{article}{}
      \name{author}{2}{}{%
        {{hash=139effce6490bd683c3d04a484d78bc9}{%
           family={Juechems},
           familyi={J\bibinitperiod},
           given={Keno},
           giveni={K\bibinitperiod}}}%
        {{hash=0e19fe61e6ad22ca347bffe3e453c372}{%
           family={Summerfield},
           familyi={S\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{18134d7df032c4681be8a96c248712fe}
      \strng{fullhash}{18134d7df032c4681be8a96c248712fe}
      \strng{bibnamehash}{18134d7df032c4681be8a96c248712fe}
      \strng{authorbibnamehash}{18134d7df032c4681be8a96c248712fe}
      \strng{authornamehash}{18134d7df032c4681be8a96c248712fe}
      \strng{authorfullhash}{18134d7df032c4681be8a96c248712fe}
      \field{sortinit}{J}
      \field{sortinithash}{c45040a764d616897e7f5b30174d7b92}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{13646613}
      \field{journaltitle}{Trends in Cognitive Sciences}
      \field{month}{10}
      \field{number}{10}
      \field{title}{Where Does Value Come From?}
      \field{volume}{23}
      \field{year}{2019}
      \field{pages}{836\bibrangedash 850}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1016/j.tics.2019.07.012
      \endverb
    \endentry
    \entry{Kahneman11}{book}{}
      \name{author}{1}{}{%
        {{hash=d0dcea81f8587e7d56bb5dbe34a39d6c}{%
           family={Kahneman},
           familyi={K\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York}%
      }
      \list{publisher}{1}{%
        {Farrar, Straus and Giroux}%
      }
      \strng{namehash}{d0dcea81f8587e7d56bb5dbe34a39d6c}
      \strng{fullhash}{d0dcea81f8587e7d56bb5dbe34a39d6c}
      \strng{bibnamehash}{d0dcea81f8587e7d56bb5dbe34a39d6c}
      \strng{authorbibnamehash}{d0dcea81f8587e7d56bb5dbe34a39d6c}
      \strng{authornamehash}{d0dcea81f8587e7d56bb5dbe34a39d6c}
      \strng{authorfullhash}{d0dcea81f8587e7d56bb5dbe34a39d6c}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Daniel Kahneman, the renowned psychologist and winner of the Nobel Prize in Economics, takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. The impact of overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the profound effect of cognitive biases on everything from playing the stock market to planning our next vacation - each of these can be understood only by knowing how the two systems shape our judgments and decisions. Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives - and how we can use different techniques to guard against the mental glitches that often get us into trouble.}
      \field{gender}{sm}
      \field{isbn}{978-0-374-27563-1}
      \field{relatedtype}{translationas}
      \field{title}{Thinking, Fast and Slow}
      \field{year}{2011}
      \keyw{-shelf 02061 105 8book cognitive decision science}
    \endentry
    \entry{keles2022computational}{misc}{}
      \name{author}{3}{}{%
        {{hash=84ff6a443ebfbfac4e4a815df75af9d3}{%
           family={Keles},
           familyi={K\bibinitperiod},
           given={Feyza\bibnamedelima Duman},
           giveni={F\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=ee380af123e54cabf98342641ae23018}{%
           family={Wijewardena},
           familyi={W\bibinitperiod},
           given={Pruthuvi\bibnamedelima Mahesakya},
           giveni={P\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=2edc4108b4aef67fe73f9c0369572451}{%
           family={Hegde},
           familyi={H\bibinitperiod},
           given={Chinmay},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{bf550df17d27b0694c7dbc580a0278d4}
      \strng{fullhash}{499f4c4a9efacc40f8cf4d0d5e548c4a}
      \strng{bibnamehash}{499f4c4a9efacc40f8cf4d0d5e548c4a}
      \strng{authorbibnamehash}{499f4c4a9efacc40f8cf4d0d5e548c4a}
      \strng{authornamehash}{bf550df17d27b0694c7dbc580a0278d4}
      \strng{authorfullhash}{499f4c4a9efacc40f8cf4d0d5e548c4a}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arxiv}
      \field{title}{On the Computational Complexity of Self-Attention}
      \field{year}{2022}
      \verb{eprint}
      \verb 2209.04881
      \endverb
    \endentry
    \entry{khanHyperbolicRepresentationsSource}{article}{}
      \name{author}{3}{}{%
        {{hash=864a8003bfff7229c3260dad99324f23}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Raiyan},
           giveni={R\bibinitperiod}}}%
        {{hash=6fc40687fd5bb836cc7c45ddf7e67984}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Thanh\bibnamedelima V},
           giveni={T\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=8e48d9e7520b1c5a75280cfcd2aeec7b}{%
           family={Srinivasan},
           familyi={S\bibinitperiod},
           given={Sengamedu\bibnamedelima H},
           giveni={S\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{7be29ccbdf5ee97542f844aab5e7617f}
      \strng{fullhash}{769518bbf2114c879d4c26453630e55c}
      \strng{bibnamehash}{769518bbf2114c879d4c26453630e55c}
      \strng{authorbibnamehash}{769518bbf2114c879d4c26453630e55c}
      \strng{authornamehash}{7be29ccbdf5ee97542f844aab5e7617f}
      \strng{authorfullhash}{769518bbf2114c879d4c26453630e55c}
      \field{extraname}{1}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learning effective representations of data is an important task in machine learning. Existing methods typically compute representations or embeddings in Euclidean space, which has shortcomings in representing hierarchical structures of the underlying data. Alternatively, hyperbolic geometry offers a representation scheme that is suited for robust, high-fidelity representations of tree-structured data. In this paper, we explore hyperbolic graph convolutional models for learning hyperbolic representations of source code, which exhibit natural hierarchies. We leverage the abstract syntax tree (AST) of source code and learn its graph-based representation to predict the function name from its body. We compare Lorentz and Poincar{é} Disk models of hyperbolic geometry with Euclidean geometry. We also propose several readout schemes to compute the graph-level representations and apply them to the method name prediction task. Using a Lorentz hyperbolic model, we establish a new state-of-the-art result on the ogbg-code2 benchmark for the task.}
      \field{langid}{english}
      \field{title}{Hyperbolic {{Representations}} of {{Source Code}}}
      \verb{file}
      \verb /Users/ron/Zotero/storage/UILJKTIW/Khan et al. - Hyperbolic Representations of Source Code.pdf
      \endverb
    \endentry
    \entry{khanTransformersVisionSurvey2022}{article}{}
      \name{author}{6}{}{%
        {{hash=2893f9d103585f3f0297c0c0bc10c36e}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Salman},
           giveni={S\bibinitperiod}}}%
        {{hash=d2067f49891ea04bc40d0c162f88d741}{%
           family={Naseer},
           familyi={N\bibinitperiod},
           given={Muzammal},
           giveni={M\bibinitperiod}}}%
        {{hash=5c498e9d05eb27dc35d7ac701641b5a8}{%
           family={Hayat},
           familyi={H\bibinitperiod},
           given={Munawar},
           giveni={M\bibinitperiod}}}%
        {{hash=59919e3cc59a0d0f166430d75affebce}{%
           family={Zamir},
           familyi={Z\bibinitperiod},
           given={Syed\bibnamedelima Waqas},
           giveni={S\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=a40e1228783564f44861095bd68df4f7}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Fahad\bibnamedelima Shahbaz},
           giveni={F\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=c75751f493070716733608d20f74e3af}{%
           family={Shah},
           familyi={S\bibinitperiod},
           given={Mubarak},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{e1ea979d97d189695a7bdf0a92d4ae16}
      \strng{fullhash}{b581a4b75164003f1de9112a6ece1b91}
      \strng{bibnamehash}{e1ea979d97d189695a7bdf0a92d4ae16}
      \strng{authorbibnamehash}{e1ea979d97d189695a7bdf0a92d4ae16}
      \strng{authornamehash}{e1ea979d97d189695a7bdf0a92d4ae16}
      \strng{authorfullhash}{b581a4b75164003f1de9112a6ece1b91}
      \field{extraname}{2}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.}
      \field{issn}{0360-0300}
      \field{journaltitle}{ACM Computing Surveys}
      \field{month}{9}
      \field{number}{10s}
      \field{shorttitle}{Transformers in {{Vision}}}
      \field{title}{Transformers in {{Vision}}: {{A Survey}}}
      \field{urlday}{23}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{54}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{200:1\bibrangedash 200:41}
      \range{pages}{-1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/ANBP2VKR/Khan et al. - 2022 - Transformers in Vision A Survey.pdf
      \endverb
      \keyw{bidirectional encoders,convolutional networks,deep neural networks,literature survey,Self-attention,self-supervision,transformers}
    \endentry
    \entry{kimCompoundProbabilisticContextFree2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=1fecefbf07117382be6f83e7910ea8a0}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Yoon},
           giveni={Y\bibinitperiod}}}%
        {{hash=dc0d6b065a6566cde5d6b93214be1b55}{%
           family={Dyer},
           familyi={D\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=1dd218ae435ec5c0143b3fbbc5da192e}{%
           family={Rush},
           familyi={R\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Florence, Italy}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{5eee5f2bd4e556439437103f838da06b}
      \strng{fullhash}{1cfc82e683b0351e9eafcbdc623fab22}
      \strng{bibnamehash}{1cfc82e683b0351e9eafcbdc623fab22}
      \strng{authorbibnamehash}{1cfc82e683b0351e9eafcbdc623fab22}
      \strng{authornamehash}{5eee5f2bd4e556439437103f838da06b}
      \strng{authorfullhash}{1cfc82e683b0351e9eafcbdc623fab22}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.}
      \field{booktitle}{Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}}
      \field{month}{7}
      \field{title}{Compound {{Probabilistic Context-Free Grammars}} for {{Grammar Induction}}}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{2369\bibrangedash 2385}
      \range{pages}{17}
      \verb{file}
      \verb /Users/ron/Zotero/storage/6TLEYY57/Kim et al. - 2019 - Compound Probabilistic Context-Free Grammars for G.pdf
      \endverb
    \endentry
    \entry{kicimanCausalReasoningLarge2023}{misc}{}
      \name{author}{4}{}{%
        {{hash=1e4bc52180cca5a898b8c85f95865bdf}{%
           family={K{ı}c{ı}man},
           familyi={K\bibinitperiod},
           given={Emre},
           giveni={E\bibinitperiod}}}%
        {{hash=f7f806b4b495f5141cda6a1499d5fb4f}{%
           family={Ness},
           familyi={N\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=af1491a2bbe671539e635699d0e0b9c5}{%
           family={Sharma},
           familyi={S\bibinitperiod},
           given={Amit},
           giveni={A\bibinitperiod}}}%
        {{hash=f28958af092cc402e768f5989866a3e7}{%
           family={Tan},
           familyi={T\bibinitperiod},
           given={Chenhao},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{9c924c3285e82d2a4c40396647925c1d}
      \strng{fullhash}{1958aebe11f453d3fdba92d7b7e1b47b}
      \strng{bibnamehash}{1958aebe11f453d3fdba92d7b7e1b47b}
      \strng{authorbibnamehash}{1958aebe11f453d3fdba92d7b7e1b47b}
      \strng{authornamehash}{9c924c3285e82d2a4c40396647925c1d}
      \strng{authorfullhash}{1958aebe11f453d3fdba92d7b7e1b47b}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain), and actual causality (86\% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{5}
      \field{number}{arXiv:2305.00050}
      \field{shorttitle}{Causal {{Reasoning}} and {{Large Language Models}}}
      \field{title}{Causal {{Reasoning}} and {{Large Language Models}}: {{Opening}} a {{New Frontier}} for {{Causality}}}
      \field{urlday}{2}
      \field{urlmonth}{10}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2305.00050
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/5HBU4IZ3/Kıcıman et al. - 2023 - Causal Reasoning and Large Language Models Openin.pdf;/Users/ron/Zotero/storage/QGKRNKU4/2305.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Methodology}
    \endentry
    \entry{kolb2001introduction}{book}{}
      \name{author}{5}{}{%
        {{hash=b6d2912ead14248093a50af895cfdc8b}{%
           family={Kolb},
           familyi={K\bibinitperiod},
           given={Bryan},
           giveni={B\bibinitperiod}}}%
        {{hash=30ae769e55126367139d9a3bdf5c333c}{%
           family={Whishaw},
           familyi={W\bibinitperiod},
           given={Ian\bibnamedelima Q},
           giveni={I\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
        {{hash=60937dc4dec914c6ff723e6c34851551}{%
           family={Teskey},
           familyi={T\bibinitperiod},
           given={G\bibnamedelima Campbell},
           giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=30ae769e55126367139d9a3bdf5c333c}{%
           family={Whishaw},
           familyi={W\bibinitperiod},
           given={Ian\bibnamedelima Q},
           giveni={I\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
        {{hash=60937dc4dec914c6ff723e6c34851551}{%
           family={Teskey},
           familyi={T\bibinitperiod},
           given={G\bibnamedelima Campbell},
           giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Worth New York}%
      }
      \strng{namehash}{3a0ffafcda8a24fccf998486574b62d4}
      \strng{fullhash}{838b6b50493ab3d53eac1ed2834a72b4}
      \strng{bibnamehash}{838b6b50493ab3d53eac1ed2834a72b4}
      \strng{authorbibnamehash}{838b6b50493ab3d53eac1ed2834a72b4}
      \strng{authornamehash}{3a0ffafcda8a24fccf998486574b62d4}
      \strng{authorfullhash}{838b6b50493ab3d53eac1ed2834a72b4}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{An Introduction to Brain and Behavior}
      \field{year}{2001}
    \endentry
    \entry{lakeBuildingMachinesThat2017}{article}{}
      \name{author}{4}{}{%
        {{hash=8449df2d22be789ac9cc32444c31921e}{%
           family={Lake},
           familyi={L\bibinitperiod},
           given={Brenden\bibnamedelima M.},
           giveni={B\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e226fb1da66be94e73828778e6eb8af8}{%
           family={Ullman},
           familyi={U\bibinitperiod},
           given={Tomer\bibnamedelima D.},
           giveni={T\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=947b5ff2e5969eafe5136b3e017c5c01}{%
           family={Gershman},
           familyi={G\bibinitperiod},
           given={Samuel\bibnamedelima J.},
           giveni={S\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{eff7bbe99dbb8df47713e2e6698163af}
      \strng{fullhash}{f3f65c1f5ec76205ed74a281c8ea6f9f}
      \strng{bibnamehash}{f3f65c1f5ec76205ed74a281c8ea6f9f}
      \strng{authorbibnamehash}{f3f65c1f5ec76205ed74a281c8ea6f9f}
      \strng{authornamehash}{eff7bbe99dbb8df47713e2e6698163af}
      \strng{authorfullhash}{f3f65c1f5ec76205ed74a281c8ea6f9f}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.}
      \field{issn}{0140-525X, 1469-1825}
      \field{journaltitle}{Behavioral and Brain Sciences}
      \field{langid}{english}
      \field{title}{Building Machines That Learn and Think like People}
      \field{urlday}{3}
      \field{urlmonth}{10}
      \field{urlyear}{2023}
      \field{volume}{40}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{e253}
      \range{pages}{-1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/CU7YFC4U/Lake et al. - 2017 - Building machines that learn and think like people.pdf
      \endverb
    \endentry
    \entry{lecun2022path}{article}{}
      \name{author}{1}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{6a1aa6b7eab12b931ca7c7e3f927231d}
      \strng{fullhash}{6a1aa6b7eab12b931ca7c7e3f927231d}
      \strng{bibnamehash}{6a1aa6b7eab12b931ca7c7e3f927231d}
      \strng{authorbibnamehash}{6a1aa6b7eab12b931ca7c7e3f927231d}
      \strng{authornamehash}{6a1aa6b7eab12b931ca7c7e3f927231d}
      \strng{authorfullhash}{6a1aa6b7eab12b931ca7c7e3f927231d}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Open Review}
      \field{number}{1}
      \field{title}{A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27}
      \field{volume}{62}
      \field{year}{2022}
    \endentry
    \entry{lecunTutorialEnergyBasedLearning}{article}{}
      \name{author}{5}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=83e9081b3be58a20d597b22b70648e30}{%
           family={Chopra},
           familyi={C\bibinitperiod},
           given={Sumit},
           giveni={S\bibinitperiod}}}%
        {{hash=dc7c14ebfc2b431292d1ff4ded63d5e7}{%
           family={Hadsell},
           familyi={H\bibinitperiod},
           given={Raia},
           giveni={R\bibinitperiod}}}%
        {{hash=6d93a2b5e4e1f368f5ffe4344bfec8ef}{%
           family={Ranzato},
           familyi={R\bibinitperiod},
           given={Marc'Aurelio},
           giveni={M\bibinitperiod}}}%
        {{hash=a128f901f27089486fa73ef3c8ef4528}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Fu\bibnamedelima Jie},
           giveni={F\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{fullhash}{868943b556a58c87eee5627658c1765f}
      \strng{bibnamehash}{868943b556a58c87eee5627658c1765f}
      \strng{authorbibnamehash}{868943b556a58c87eee5627658c1765f}
      \strng{authornamehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{authorfullhash}{868943b556a58c87eee5627658c1765f}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.}
      \field{langid}{english}
      \field{title}{A {{Tutorial}} on {{Energy-Based Learning}}}
      \verb{file}
      \verb /Users/ron/Zotero/storage/NVP8F4JI/LeCun et al. - A Tutorial on Energy-Based Learning.pdf
      \endverb
    \endentry
    \entry{Levenshtein}{article}{}
      \name{author}{1}{}{%
        {{hash=a5e59262cac745d7fb0662cef4a94991}{%
           family={Levenshtein},
           familyi={L\bibinitperiod},
           given={V.\bibnamedelimi I.},
           giveni={V\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \strng{namehash}{a5e59262cac745d7fb0662cef4a94991}
      \strng{fullhash}{a5e59262cac745d7fb0662cef4a94991}
      \strng{bibnamehash}{a5e59262cac745d7fb0662cef4a94991}
      \strng{authorbibnamehash}{a5e59262cac745d7fb0662cef4a94991}
      \strng{authornamehash}{a5e59262cac745d7fb0662cef4a94991}
      \strng{authorfullhash}{a5e59262cac745d7fb0662cef4a94991}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Soviet Physics Doklady}
      \field{month}{2}
      \field{title}{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}
      \field{volume}{10}
      \field{year}{1966}
      \field{pages}{707}
      \range{pages}{1}
    \endentry
    \entry{liuDiscreteValuedNeuralCommunication2021}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=95d773c681dd33c18a678e20556d7472}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Dianbo},
           giveni={D\bibinitperiod}}}%
        {{hash=d54eb15dced6ed5d4d82bed3927424ad}{%
           family={Lamb},
           familyi={L\bibinitperiod},
           given={Alex\bibnamedelima M},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e6710bfa0fd4c3d0c8e305c8035df221}{%
           family={Kawaguchi},
           familyi={K\bibinitperiod},
           given={Kenji},
           giveni={K\bibinitperiod}}}%
        {{hash=c751efac7dd828a8b1a796c2850ca771}{%
           family={ALIAS\bibnamedelimb PARTH\bibnamedelima GOYAL},
           familyi={A\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim G\bibinitperiod},
           given={Anirudh\bibnamedelima Goyal},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=7437556e576c6f4e2ade58a9aa980ec5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Chen},
           giveni={C\bibinitperiod}}}%
        {{hash=091907acecffa990a510ca1189a4d722}{%
           family={Mozer},
           familyi={M\bibinitperiod},
           given={Michael\bibnamedelima C},
           giveni={M\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{5b839d9bcb1714afe8f6507774e86195}
      \strng{fullhash}{39f59070779345589b28e6e6b85c22a9}
      \strng{bibnamehash}{5b839d9bcb1714afe8f6507774e86195}
      \strng{authorbibnamehash}{5b839d9bcb1714afe8f6507774e86195}
      \strng{authornamehash}{5b839d9bcb1714afe8f6507774e86195}
      \strng{authorfullhash}{39f59070779345589b28e6e6b85c22a9}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that discrete-valued neural communication (DVNC) substantially improves systematic generalization in a variety of architectures—transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method useful in practice.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Discrete-{{Valued Neural Communication}}}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2022}
      \field{volume}{34}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{2109\bibrangedash 2121}
      \range{pages}{13}
      \verb{file}
      \verb /Users/ron/Zotero/storage/5CTB3K88/Liu et al. - 2021 - Discrete-Valued Neural Communication.pdf
      \endverb
    \endentry
    \entry{luHyperbolicFunctionEmbedding2019}{article}{}
      \name{author}{7}{}{%
        {{hash=e6bd72a5b8acfa99c266a7cd42f56566}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Mingming},
           giveni={M\bibinitperiod}}}%
        {{hash=92f7474e36f4f2b59a14e85c0e738e40}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Yan},
           giveni={Y\bibinitperiod}}}%
        {{hash=71247cd94b58ed8406cb818281ce0cc8}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Haifeng},
           giveni={H\bibinitperiod}}}%
        {{hash=143dfb78bc4ff969aa5335ea94ae15d8}{%
           family={Tan},
           familyi={T\bibinitperiod},
           given={Dingwu},
           giveni={D\bibinitperiod}}}%
        {{hash=c0bc76c761b18375e19793915853b7da}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Xiaoxian},
           giveni={X\bibinitperiod}}}%
        {{hash=bee72bb9abdbd2e60b987a29da85fb7d}{%
           family={Bi},
           familyi={B\bibinitperiod},
           given={Wenjie},
           giveni={W\bibinitperiod}}}%
        {{hash=3c126445a4b807f6567f1f8672a1de19}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Wendbo},
           giveni={W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Multidisciplinary Digital Publishing Institute}%
      }
      \strng{namehash}{0efc6b8ca8f84ca8502e886fe1e2c51b}
      \strng{fullhash}{3f09a77a2325f03aacfcac67f6c40271}
      \strng{bibnamehash}{0efc6b8ca8f84ca8502e886fe1e2c51b}
      \strng{authorbibnamehash}{0efc6b8ca8f84ca8502e886fe1e2c51b}
      \strng{authornamehash}{0efc6b8ca8f84ca8502e886fe1e2c51b}
      \strng{authorfullhash}{3f09a77a2325f03aacfcac67f6c40271}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recently, source code mining has received increasing attention due to the rapid increase of open-sourced code repositories and the tremendous values implied in this large dataset, which can help us understand the organization of functions or classes in different software and analyze the impact of these organized patterns on the software behaviors. Hence, learning an effective representation model for the functions of source code, from a modern view, is a crucial problem. Considering the inherent hierarchy of functions, we propose a novel hyperbolic function embedding (HFE) method, which can learn a distributed and hierarchical representation for each function via the Poincar{é} ball model. To achieve this, a function call graph (FCG) is first constructed to model the call relationship among functions. To verify the underlying geometry of FCG, the Ricci curvature model is used. Finally, an HFE model is built to learn the representations that can capture the latent hierarchy of functions in the hyperbolic space, instead of the Euclidean space, which are usually used in those state-of-the-art methods. Moreover, HFE is more compact in terms of lower dimensionality than the existing graph embedding methods. Thus, HFE is more effective in terms of computation and storage. To experimentally evaluate the performance of HFE, two application scenarios, namely, function classification and link prediction, have been applied. HFE achieves up to 7.6\% performance improvement compared to the chosen state-of-the-art methods, namely, Node2vec and Struc2vec.}
      \field{issn}{2073-8994}
      \field{journaltitle}{Symmetry}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{2}
      \field{shorttitle}{Hyperbolic {{Function Embedding}}}
      \field{title}{Hyperbolic {{Function Embedding}}: {{Learning Hierarchical Representation}} for {{Functions}} of {{Source Code}} in {{Hyperbolic Space}}}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{11}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{254}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/TJ86UALI/Lu et al. - 2019 - Hyperbolic Function Embedding Learning Hierarchic.pdf
      \endverb
      \keyw{function embedding representation,function-call graph,hyperbolic space,source code mining}
    \endentry
    \entry{mairsonLinearLambdaCalculus}{article}{}
      \name{author}{1}{}{%
        {{hash=8d5b3c9f2b857ac1c640c5a612f3a490}{%
           family={Mairson},
           familyi={M\bibinitperiod},
           given={Harry\bibnamedelima G},
           giveni={H\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{8d5b3c9f2b857ac1c640c5a612f3a490}
      \strng{fullhash}{8d5b3c9f2b857ac1c640c5a612f3a490}
      \strng{bibnamehash}{8d5b3c9f2b857ac1c640c5a612f3a490}
      \strng{authorbibnamehash}{8d5b3c9f2b857ac1c640c5a612f3a490}
      \strng{authornamehash}{8d5b3c9f2b857ac1c640c5a612f3a490}
      \strng{authorfullhash}{8d5b3c9f2b857ac1c640c5a612f3a490}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We give transparent proofs of the PTIME-completeness of two decision problems for terms in the {$\lambda$}-calculus. The first is a reproof of the theorem that type inference for the simplytyped {$\lambda$}-calculus is PTIME-complete. Our proof is interesting because it uses no more than the standard combinators Church knew of some 70 years ago, in which the terms are linear affine—each bound variable occurs at most once.}
      \field{journaltitle}{Journal of Functional Programming}
      \field{langid}{english}
      \field{title}{Linear Lambda Calculus and {{PTIME-completeness}}}
      \verb{file}
      \verb /Users/ron/Zotero/storage/SELGQARQ/Mairson - Linear lambda calculus and PTIME-completeness.pdf
      \endverb
    \endentry
    \entry{malkinTrajectoryBalanceImproved2022}{misc}{}
      \name{author}{5}{}{%
        {{hash=c6bdc75ef5c2c84edff84fdad7d7a93b}{%
           family={Malkin},
           familyi={M\bibinitperiod},
           given={Nikolay},
           giveni={N\bibinitperiod}}}%
        {{hash=93d11841cec92dc61eb3d903b3ac2002}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Moksh},
           giveni={M\bibinitperiod}}}%
        {{hash=a0d376ad8a651c5596443bd634847aae}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Emmanuel},
           giveni={E\bibinitperiod}}}%
        {{hash=7437556e576c6f4e2ade58a9aa980ec5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Chen},
           giveni={C\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{29a8479e1028858b037281a672f7bb61}
      \strng{fullhash}{92c6632f6238fd9f0b5b77203ef0cb95}
      \strng{bibnamehash}{92c6632f6238fd9f0b5b77203ef0cb95}
      \strng{authorbibnamehash}{92c6632f6238fd9f0b5b77203ef0cb95}
      \strng{authornamehash}{29a8479e1028858b037281a672f7bb61}
      \strng{authorfullhash}{92c6632f6238fd9f0b5b77203ef0cb95}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets, flow matching and detailed balance, which are analogous to temporal difference learning, to be prone to inefficient credit propagation across long action sequences. We thus propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{10}
      \field{number}{arXiv:2201.13259}
      \field{shorttitle}{Trajectory Balance}
      \field{title}{Trajectory Balance: {{Improved}} Credit Assignment in {{GFlowNets}}}
      \field{urlday}{7}
      \field{urlmonth}{11}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2201.13259
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/7MSINDRW/Malkin et al. - 2022 - Trajectory balance Improved credit assignment in .pdf;/Users/ron/Zotero/storage/IY683DI3/2201.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{marr1976understanding}{article}{}
      \name{author}{2}{}{%
        {{hash=2cf5062b61963ac0956179a605c38d66}{%
           family={Marr},
           familyi={M\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=f65eec80d26b2ab02398d1e78643c9a6}{%
           family={Poggio},
           familyi={P\bibinitperiod},
           given={Tomaso},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{7caf5ee667167b23ff66ea856455985f}
      \strng{fullhash}{7caf5ee667167b23ff66ea856455985f}
      \strng{bibnamehash}{7caf5ee667167b23ff66ea856455985f}
      \strng{authorbibnamehash}{7caf5ee667167b23ff66ea856455985f}
      \strng{authornamehash}{7caf5ee667167b23ff66ea856455985f}
      \strng{authorfullhash}{7caf5ee667167b23ff66ea856455985f}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{From understanding computation to understanding neural circuitry}
      \field{year}{1976}
    \endentry
    \entry{martinsHowChildrenPerceive2014}{article}{}
      \name{author}{5}{}{%
        {{hash=be180a9d3c94cf430283483a8a2a855d}{%
           family={Martins},
           familyi={M\bibinitperiod},
           given={Maur{í}cio\bibnamedelima Dias},
           giveni={M\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=a733f8f532ecac3e6bc5582c1e9fe443}{%
           family={Laaha},
           familyi={L\bibinitperiod},
           given={Sabine},
           giveni={S\bibinitperiod}}}%
        {{hash=1a25067d1a45056cf024e87a55c694dc}{%
           family={Freiberger},
           familyi={F\bibinitperiod},
           given={Eva\bibnamedelima Maria},
           giveni={E\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=c5a08a77ee84fe37290b5d08460caaf4}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={Soonja},
           giveni={S\bibinitperiod}}}%
        {{hash=fe08ae2b8f0f88662b02f66207f0a6ff}{%
           family={Fitch},
           familyi={F\bibinitperiod},
           given={W.\bibnamedelimi Tecumseh},
           giveni={W\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{7bf6aa7b8da67487fb072625b5b3e6f2}
      \strng{fullhash}{03949537186c8e37bfc4d27004bc3bdf}
      \strng{bibnamehash}{03949537186c8e37bfc4d27004bc3bdf}
      \strng{authorbibnamehash}{03949537186c8e37bfc4d27004bc3bdf}
      \strng{authornamehash}{7bf6aa7b8da67487fb072625b5b3e6f2}
      \strng{authorfullhash}{03949537186c8e37bfc4d27004bc3bdf}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The ability to understand and generate hierarchical structures is a crucial component of human cognition, available in language, music, mathematics and problem solving. Recursion is a particularly useful mechanism for generating complex hierarchies by means of self-embedding rules. In the visual domain, fractals are recursive structures in which simple transformation rules generate hierarchies of infinite depth. Research on how children acquire these rules can provide valuable insight into the cognitive requirements and learning constraints of recursion. Here, we used fractals to investigate the acquisition of recursion in the visual domain, and probed for correlations with grammar comprehension and general intelligence. We compared second (n=26) and fourth graders (n=26) in their ability to represent two types of rules for generating hierarchical structures: Recursive rules, on the one hand, which generate new hierarchical levels; and iterative rules, on the other hand, which merely insert items within hierarchies without generating new levels. We found that the majority of fourth graders, but not second graders, were able to represent both recursive and iterative rules. This difference was partially accounted by second graders' impairment in detecting hierarchical mistakes, and correlated with between-grade differences in grammar comprehension tasks. Empirically, recursion and iteration also differed in at least one crucial aspect: While the ability to learn recursive rules seemed to depend on the previous acquisition of simple iterative representations, the opposite was not true, i.e., children were able to acquire iterative rules before they acquired recursive representations. These results suggest that the acquisition of recursion in vision follows learning constraints similar to the acquisition of recursion in language, and that both domains share cognitive resources involved in hierarchical processing.}
      \field{issn}{0010-0277}
      \field{journaltitle}{Cognition}
      \field{month}{10}
      \field{number}{1}
      \field{shorttitle}{How Children Perceive Fractals}
      \field{title}{How Children Perceive Fractals: {{Hierarchical}} Self-Similarity and Cognitive Development}
      \field{urlday}{23}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{133}
      \field{year}{2014}
      \field{urldateera}{ce}
      \field{pages}{10\bibrangedash 24}
      \range{pages}{15}
      \verb{file}
      \verb /Users/ron/Zotero/storage/W7WWWDGF/S0010027714000997.html
      \endverb
      \keyw{Development,Hierarchy,Iteration,Language evolution,Recursion,Visuo-spatial}
    \endentry
    \entry{matsuoDeepLearningReinforcement2022}{article}{}
      \name{author}{8}{}{%
        {{hash=db1de55b1de9e30bc3b3b38b224040bf}{%
           family={Matsuo},
           familyi={M\bibinitperiod},
           given={Yutaka},
           giveni={Y\bibinitperiod}}}%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=277241c95489c0dc41e83a400c407cc8}{%
           family={Sahani},
           familyi={S\bibinitperiod},
           given={Maneesh},
           giveni={M\bibinitperiod}}}%
        {{hash=4428b76e1301b2db58587fb18bb59a38}{%
           family={Precup},
           familyi={P\bibinitperiod},
           given={Doina},
           giveni={D\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=39334f77cbd67c8922133952b8095d89}{%
           family={Sugiyama},
           familyi={S\bibinitperiod},
           given={Masashi},
           giveni={M\bibinitperiod}}}%
        {{hash=853fbd55798e43443fcee049fe60a0e9}{%
           family={Uchibe},
           familyi={U\bibinitperiod},
           given={Eiji},
           giveni={E\bibinitperiod}}}%
        {{hash=27c6cae8e17b3c38ffcd03efd6861ffd}{%
           family={Morimoto},
           familyi={M\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ded8353b7ec7d6a1eec4d8e2cf372960}
      \strng{fullhash}{852968fbcc7cf30918a2d8fe1692897c}
      \strng{bibnamehash}{ded8353b7ec7d6a1eec4d8e2cf372960}
      \strng{authorbibnamehash}{ded8353b7ec7d6a1eec4d8e2cf372960}
      \strng{authornamehash}{ded8353b7ec7d6a1eec4d8e2cf372960}
      \strng{authorfullhash}{852968fbcc7cf30918a2d8fe1692897c}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the ``Deep Learning and Reinforcement Learning'' session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{8}
      \field{title}{Deep Learning, Reinforcement Learning, and World Models}
      \field{urlday}{27}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{152}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{267\bibrangedash 275}
      \range{pages}{9}
      \verb{file}
      \verb /Users/ron/Zotero/storage/85WS7SAZ/Matsuo et al. - 2022 - Deep learning, reinforcement learning, and world m.pdf;/Users/ron/Zotero/storage/DIK2B2MW/S0893608022001150.html
      \endverb
      \keyw{Artificial intelligence,Deep learning,Machine learning,Reinforcement learning,World models}
    \endentry
    \entry{mazzagliaFreeEnergyPrinciple2022}{article}{}
      \name{author}{4}{}{%
        {{hash=f5f328ed70c8b2dfc4b2eb2d82372eec}{%
           family={Mazzaglia},
           familyi={M\bibinitperiod},
           given={Pietro},
           giveni={P\bibinitperiod}}}%
        {{hash=7ae0249027282ce63a8b130db60b88f6}{%
           family={Verbelen},
           familyi={V\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=a9469e9b7223203fccc44acf37205929}{%
           family={{Ç}atal},
           familyi={Ç\bibinitperiod},
           given={Ozan},
           giveni={O\bibinitperiod}}}%
        {{hash=f3a7566cafe3d87f70b255c25b92611a}{%
           family={Dhoedt},
           familyi={D\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{4f813d1eb3cd3b30e234dde7dfe87c57}
      \strng{fullhash}{dcc8937c6ad1f296ff375de637693722}
      \strng{bibnamehash}{dcc8937c6ad1f296ff375de637693722}
      \strng{authorbibnamehash}{dcc8937c6ad1f296ff375de637693722}
      \strng{authornamehash}{4f813d1eb3cd3b30e234dde7dfe87c57}
      \strng{authorfullhash}{dcc8937c6ad1f296ff375de637693722}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The free energy principle, and its corollary active inference, constitute a bio-inspired theory that assumes biological agents act to remain in a restricted set of preferred states of the world, i.e., they minimize their free energy. Under this principle, biological agents learn a generative model of the world and plan actions in the future that will maintain the agent in an homeostatic state that satisfies its preferences. This framework lends itself to being realized in silico, as it comprehends important aspects that make it computationally affordable, such as variational inference and amortized planning. In this work, we investigate the tool of deep learning to design and realize artificial agents based on active inference, presenting a deep-learning oriented presentation of the free energy principle, surveying works that are relevant in both machine learning and active inference areas, and discussing the design choices that are involved in the implementation process. This manuscript probes newer perspectives for the active inference framework, grounding its theoretical aspects into more pragmatic affairs, offering a practical guide to active inference newcomers and a starting point for deep learning practitioners that would like to investigate implementations of the free energy principle.}
      \field{eprintclass}{cs, q-bio}
      \field{eprinttype}{arxiv}
      \field{issn}{1099-4300}
      \field{journaltitle}{Entropy}
      \field{month}{2}
      \field{number}{2}
      \field{shorttitle}{The {{Free Energy Principle}} for {{Perception}} and {{Action}}}
      \field{title}{The {{Free Energy Principle}} for {{Perception}} and {{Action}}: {{A Deep Learning Perspective}}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{24}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{301}
      \range{pages}{1}
      \verb{eprint}
      \verb 2207.06415
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/UPXE9BT5/Mazzaglia et al. - 2022 - The Free Energy Principle for Perception and Actio.pdf;/Users/ron/Zotero/storage/HH5BHAE8/2207.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition}
    \endentry
    \entry{nickelPoincareEmbeddingsLearning2017}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=b9c4a0982e0955e1a05c82922626e2a6}{%
           family={Nickel},
           familyi={N\bibinitperiod},
           given={Maximillian},
           giveni={M\bibinitperiod}}}%
        {{hash=ca01c762f4a52dd8a8548508f8d727c9}{%
           family={Kiela},
           familyi={K\bibinitperiod},
           given={Douwe},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{51636ce85701f8c8a73031ee9bebbfbc}
      \strng{fullhash}{51636ce85701f8c8a73031ee9bebbfbc}
      \strng{bibnamehash}{51636ce85701f8c8a73031ee9bebbfbc}
      \strng{authorbibnamehash}{51636ce85701f8c8a73031ee9bebbfbc}
      \strng{authornamehash}{51636ce85701f8c8a73031ee9bebbfbc}
      \strng{authorfullhash}{51636ce85701f8c8a73031ee9bebbfbc}
      \field{sortinit}{N}
      \field{sortinithash}{98cf339a479c0454fe09153a08675a15}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar{é} ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar{é} embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Poincar{é} {{Embeddings}} for {{Learning Hierarchical Representations}}}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{30}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/ron/Zotero/storage/V7QLFFDV/Nickel and Kiela - 2017 - Poincaré Embeddings for Learning Hierarchical Repr.pdf
      \endverb
    \endentry
    \entry{nyeImprovingCoherenceConsistency2021}{misc}{}
      \name{author}{4}{}{%
        {{hash=49d657a6cda3ddc6f434500001ea6d42}{%
           family={Nye},
           familyi={N\bibinitperiod},
           given={Maxwell},
           giveni={M\bibinitperiod}}}%
        {{hash=d38fb2182140bfd8126c669e5cada5b8}{%
           family={Tessler},
           familyi={T\bibinitperiod},
           given={Michael\bibnamedelima Henry},
           giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=8449df2d22be789ac9cc32444c31921e}{%
           family={Lake},
           familyi={L\bibinitperiod},
           given={Brenden\bibnamedelima M.},
           giveni={B\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{2d680eaffddc5a1a3b82c633830a2730}
      \strng{fullhash}{1980b33344e7da9d1c3e31374537ddf3}
      \strng{bibnamehash}{1980b33344e7da9d1c3e31374537ddf3}
      \strng{authorbibnamehash}{1980b33344e7da9d1c3e31374537ddf3}
      \strng{authornamehash}{2d680eaffddc5a1a3b82c633830a2730}
      \strng{authorfullhash}{1980b33344e7da9d1c3e31374537ddf3}
      \field{sortinit}{N}
      \field{sortinithash}{98cf339a479c0454fe09153a08675a15}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Human reasoning can often be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models -- which have been increasingly successful at performing complex, structured tasks -- exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{12}
      \field{number}{arXiv:2107.02794}
      \field{title}{Improving {{Coherence}} and {{Consistency}} in {{Neural Sequence Models}} with {{Dual-System}}, {{Neuro-Symbolic Reasoning}}}
      \field{urlday}{10}
      \field{urlmonth}{2}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2107.02794
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/8L35HG43/Nye et al. - 2021 - Improving Coherence and Consistency in Neural Sequ.pdf;/Users/ron/Zotero/storage/E86K25FW/2107.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{oliveiraAbstractSyntaxGraphs2013}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=6371db8240613b58ac2d89cda9fe8bff}{%
           family={Oliveira},
           familyi={O\bibinitperiod},
           given={Bruno\bibnamedelimb C.\bibnamedelimi D.\bibnamedelimi S.},
           giveni={B\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim D\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=dea227d45d0919684686f3671a2987f6}{%
           family={L{ö}h},
           familyi={L\bibinitperiod},
           given={Andres},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Rome Italy}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{5dab3f8cd7db91e1aa2a61e64f4baca7}
      \strng{fullhash}{5dab3f8cd7db91e1aa2a61e64f4baca7}
      \strng{bibnamehash}{5dab3f8cd7db91e1aa2a61e64f4baca7}
      \strng{authorbibnamehash}{5dab3f8cd7db91e1aa2a61e64f4baca7}
      \strng{authornamehash}{5dab3f8cd7db91e1aa2a61e64f4baca7}
      \strng{authorfullhash}{5dab3f8cd7db91e1aa2a61e64f4baca7}
      \field{sortinit}{O}
      \field{sortinithash}{ff8d4eeb5101e3cf3809959b3592d942}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An important problem in the context of embedded domain specific languages (EDSLs) is how to provide easy to use, yet expressive representations of abstract syntax. So far providing user-friendly encodings of abstract syntax that enable operations that observe or preserve sharing and recursion has proved to be quite elusive.}
      \field{booktitle}{Proceedings of the {{ACM SIGPLAN}} 2013 Workshop on {{Partial}} Evaluation and Program Manipulation}
      \field{isbn}{978-1-4503-1842-6}
      \field{langid}{english}
      \field{month}{1}
      \field{title}{Abstract Syntax Graphs for Domain Specific Languages}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{87\bibrangedash 96}
      \range{pages}{10}
      \verb{file}
      \verb /Users/ron/Zotero/storage/CE6IREWV/Oliveira and Löh - 2013 - Abstract syntax graphs for domain specific languag.pdf
      \endverb
    \endentry
    \entry{pearlTheoreticalImpedimentsMachine2018}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Marina Del Rey CA USA}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.}
      \field{booktitle}{Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}}
      \field{isbn}{978-1-4503-5581-0}
      \field{langid}{english}
      \field{month}{2}
      \field{title}{Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}}
      \field{urlday}{28}
      \field{urlmonth}{10}
      \field{urlyear}{2023}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{3\bibrangedash 3}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/JSCDKVT8/Pearl - 2018 - Theoretical Impediments to Machine Learning With S.pdf
      \endverb
    \endentry
    \entry{pearsonHumanImaginationCognitive2019}{article}{}
      \name{author}{1}{}{%
        {{hash=d194e548fb1a0b037ef9838d503979fb}{%
           family={Pearson},
           familyi={P\bibinitperiod},
           given={Joel},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{d194e548fb1a0b037ef9838d503979fb}
      \strng{fullhash}{d194e548fb1a0b037ef9838d503979fb}
      \strng{bibnamehash}{d194e548fb1a0b037ef9838d503979fb}
      \strng{authorbibnamehash}{d194e548fb1a0b037ef9838d503979fb}
      \strng{authornamehash}{d194e548fb1a0b037ef9838d503979fb}
      \strng{authorfullhash}{d194e548fb1a0b037ef9838d503979fb}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Mental imagery can be advantageous, unnecessary and even clinically disruptive. With methodological constraints now overcome, research has shown that visual imagery involves a network of brain areas from the frontal cortex to sensory areas, overlapping with the~default mode network, and can function much like a weak version of afferent perception. Imagery vividness and strength range from completely absent (aphantasia) to photo-like (hyperphantasia). Both the anatomy and function of the primary visual cortex are related to visual imagery. The use of imagery as a tool has been linked to many compound cognitive processes and imagery plays both symptomatic and mechanistic roles in neurological and mental disorders and treatments.}
      \field{issn}{1471-0048}
      \field{journaltitle}{Nature Reviews Neuroscience}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{10}
      \field{shorttitle}{The Human Imagination}
      \field{title}{The Human Imagination: The Cognitive Neuroscience of Visual Mental Imagery}
      \field{urlday}{5}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{20}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{624\bibrangedash 634}
      \range{pages}{11}
      \keyw{Object vision,Sensory systems,Working memory}
    \endentry
    \entry{pengRethinkingPositionalEncoding2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=3e013eef8348274ebf08c1fb922d0759}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Han},
           giveni={H\bibinitperiod}}}%
        {{hash=206b08a7847c93b8b4cc9a63e890133c}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Ge},
           giveni={G\bibinitperiod}}}%
        {{hash=de5efb2410f469f7114445dd634686e0}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Yunfei},
           giveni={Y\bibinitperiod}}}%
        {{hash=59ead1ce694d996ebe61da34178792ee}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Zhi},
           giveni={Z\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Abu Dhabi, United Arab Emirates}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{804bc85691e5549e70e5de3ccd2bf1a2}
      \strng{fullhash}{e307b26c04e0e0747bdbb5e1e1c875b8}
      \strng{bibnamehash}{e307b26c04e0e0747bdbb5e1e1c875b8}
      \strng{authorbibnamehash}{e307b26c04e0e0747bdbb5e1e1c875b8}
      \strng{authornamehash}{804bc85691e5549e70e5de3ccd2bf1a2}
      \strng{authorfullhash}{e307b26c04e0e0747bdbb5e1e1c875b8}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we propose a novel tree Transformer encoding node positions based on our new description method for tree structures.Technically, local and global soft bias shown in previous works is both introduced as positional encodings of our Transformer model.Our model finally outperforms strong baselines on code summarization and completion tasks across two languages, demonstrating our model's effectiveness.Besides, extensive experiments and ablation study shows that combining both local and global paradigms is still helpful in improving model performance. We release our code at https://github.com/AwdHanPeng/TreeTransformer.}
      \field{booktitle}{Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}}
      \field{month}{12}
      \field{title}{Rethinking {{Positional Encoding}} in {{Tree Transformer}} for {{Code Representation}}}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{3204\bibrangedash 3214}
      \range{pages}{11}
      \verb{file}
      \verb /Users/ron/Zotero/storage/BZWW6KKS/Peng et al. - 2022 - Rethinking Positional Encoding in Tree Transformer.pdf
      \endverb
    \endentry
    \entry{piantadosiComputationalOriginRepresentation2021}{article}{}
      \name{author}{1}{}{%
        {{hash=3f46a4a9c148a249e1f487bf63fb6c1c}{%
           family={Piantadosi},
           familyi={P\bibinitperiod},
           given={Steven\bibnamedelima T.},
           giveni={S\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{3f46a4a9c148a249e1f487bf63fb6c1c}
      \strng{fullhash}{3f46a4a9c148a249e1f487bf63fb6c1c}
      \strng{bibnamehash}{3f46a4a9c148a249e1f487bf63fb6c1c}
      \strng{authorbibnamehash}{3f46a4a9c148a249e1f487bf63fb6c1c}
      \strng{authornamehash}{3f46a4a9c148a249e1f487bf63fb6c1c}
      \strng{authorfullhash}{3f46a4a9c148a249e1f487bf63fb6c1c}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations—whatever they are—must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantification, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an ``assembly language'' for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems.}
      \field{issn}{0924-6495, 1572-8641}
      \field{journaltitle}{Minds and Machines}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{title}{The {{Computational Origin}} of {{Representation}}}
      \field{urlday}{12}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{volume}{31}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 58}
      \range{pages}{58}
      \verb{file}
      \verb /Users/ron/Zotero/storage/CYEV29MV/Piantadosi - 2021 - The Computational Origin of Representation.pdf
      \endverb
    \endentry
    \entry{pinker2003blank}{book}{}
      \name{author}{1}{}{%
        {{hash=026a9bbaeb03e57886733eb6c1f1be1c}{%
           family={Pinker},
           familyi={P\bibinitperiod},
           given={Steven},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Penguin}%
      }
      \strng{namehash}{026a9bbaeb03e57886733eb6c1f1be1c}
      \strng{fullhash}{026a9bbaeb03e57886733eb6c1f1be1c}
      \strng{bibnamehash}{026a9bbaeb03e57886733eb6c1f1be1c}
      \strng{authorbibnamehash}{026a9bbaeb03e57886733eb6c1f1be1c}
      \strng{authornamehash}{026a9bbaeb03e57886733eb6c1f1be1c}
      \strng{authorfullhash}{026a9bbaeb03e57886733eb6c1f1be1c}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{The Blank Slate: {{The}} Modern Denial of Human Nature}
      \field{year}{2003}
    \endentry
    \entry{Pouget_Beck_Ma_Latham_2013}{article}{}
      \name{author}{4}{}{%
        {{hash=330aaa9645b14ab62852c0a3bb1c7a9a}{%
           family={Pouget},
           familyi={P\bibinitperiod},
           given={Alexandre},
           giveni={A\bibinitperiod}}}%
        {{hash=64e229bf6d63c5842268d809d0df5448}{%
           family={Beck},
           familyi={B\bibinitperiod},
           given={Jeffrey\bibnamedelima M.},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=fd75c282ef5c8d05357464d48f7d4988}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Wei\bibnamedelima Ji},
           giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=5d03d7d07ef2c4d938fad107d5b2919b}{%
           family={Latham},
           familyi={L\bibinitperiod},
           given={Peter\bibnamedelima E.},
           giveni={P\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \strng{namehash}{20d7f82c350f4b4474f405748cac9a40}
      \strng{fullhash}{0dc029ffe38fc3f87ca74303c0d92afb}
      \strng{bibnamehash}{0dc029ffe38fc3f87ca74303c0d92afb}
      \strng{authorbibnamehash}{0dc029ffe38fc3f87ca74303c0d92afb}
      \strng{authornamehash}{20d7f82c350f4b4474f405748cac9a40}
      \strng{authorfullhash}{0dc029ffe38fc3f87ca74303c0d92afb}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1546-1726}
      \field{journaltitle}{Nature Neuroscience}
      \field{month}{9}
      \field{number}{9}
      \field{title}{Probabilistic brains: knowns and unknowns}
      \field{volume}{16}
      \field{year}{2013}
      \field{pages}{1170\bibrangedash 1178}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1038/nn.3495
      \endverb
    \endentry
    \entry{raussWhatBottomUpWhat2013}{article}{}
      \name{author}{2}{}{%
        {{hash=9f7fd694b4d466a66f6657a4ddd13f36}{%
           family={Rauss},
           familyi={R\bibinitperiod},
           given={Karsten},
           giveni={K\bibinitperiod}}}%
        {{hash=e2816423282b73e1c11a6b61f146cf68}{%
           family={Pourtois},
           familyi={P\bibinitperiod},
           given={Gilles},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{c37fb499c479720ddc30a42b450fe478}
      \strng{fullhash}{c37fb499c479720ddc30a42b450fe478}
      \strng{bibnamehash}{c37fb499c479720ddc30a42b450fe478}
      \strng{authorbibnamehash}{c37fb499c479720ddc30a42b450fe478}
      \strng{authornamehash}{c37fb499c479720ddc30a42b450fe478}
      \strng{authorfullhash}{c37fb499c479720ddc30a42b450fe478}
      \field{sortinit}{R}
      \field{sortinithash}{b9c68a358aea118dfa887b6e902414a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Everyone knows what bottom-up is, and how it is different from top-down. At least one is tempted to think so, given that both terms are ubiquitously used, but only rarely defined in the psychology and neuroscience literature. In this review, we highlight the problems and limitations of our current understanding of bottom-up and top-down processes, and we propose a reformulation of this distinction in terms of predictive coding.}
      \field{issn}{1664-1078}
      \field{journaltitle}{Frontiers in Psychology}
      \field{title}{What Is {{Bottom-Up}} and {{What}} Is {{Top-Down}} in {{Predictive Coding}}?}
      \field{urlday}{14}
      \field{urlmonth}{5}
      \field{urlyear}{2022}
      \field{volume}{4}
      \field{year}{2013}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/ron/Zotero/storage/ZJLPPHZL/Rauss and Pourtois - 2013 - What is Bottom-Up and What is Top-Down in Predicti.pdf
      \endverb
    \endentry
    \entry{ruleChildHacker2020}{article}{}
      \name{author}{3}{}{%
        {{hash=544238b07e7dedf8d9530ec3f97f2d7f}{%
           family={Rule},
           familyi={R\bibinitperiod},
           given={Joshua\bibnamedelima S.},
           giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=3f46a4a9c148a249e1f487bf63fb6c1c}{%
           family={Piantadosi},
           familyi={P\bibinitperiod},
           given={Steven\bibnamedelima T.},
           giveni={S\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{5ce2655371ccc510700df9e6827e7ba5}
      \strng{fullhash}{f437c0d477e73a3d448cdf95f80adbf7}
      \strng{bibnamehash}{f437c0d477e73a3d448cdf95f80adbf7}
      \strng{authorbibnamehash}{f437c0d477e73a3d448cdf95f80adbf7}
      \strng{authornamehash}{5ce2655371ccc510700df9e6827e7ba5}
      \strng{authorfullhash}{f437c0d477e73a3d448cdf95f80adbf7}
      \field{sortinit}{R}
      \field{sortinithash}{b9c68a358aea118dfa887b6e902414a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{13646613}
      \field{journaltitle}{Trends in Cognitive Sciences}
      \field{langid}{english}
      \field{month}{11}
      \field{number}{11}
      \field{title}{The {{Child}} as {{Hacker}}}
      \field{urlday}{3}
      \field{urlmonth}{5}
      \field{urlyear}{2022}
      \field{volume}{24}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{900\bibrangedash 915}
      \range{pages}{16}
      \verb{file}
      \verb /Users/ron/Zotero/storage/TVAQQX5F/Rule et al. - 2020 - The Child as Hacker.pdf
      \endverb
    \endentry
    \entry{sable-meyerLanguageThoughtMental2022}{article}{}
      \name{author}{4}{}{%
        {{hash=ac45893539ec3765f93da94b267f6402}{%
           family={{Sabl{é}-Meyer}},
           familyi={S\bibinitperiod},
           given={Mathias},
           giveni={M\bibinitperiod}}}%
        {{hash=5404603ffd5dfdff4037647ecf8c172f}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=48de56ec34034a2f537d9cc56f0774d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=14ff7c970ba747f58e07de2e42e7c5bb}{%
           family={Dehaene},
           familyi={D\bibinitperiod},
           given={Stanislas},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{4a0baf87803c3ab81423c680df51a084}
      \strng{fullhash}{ae7b43413b51af2509dfb0dc856f17ce}
      \strng{bibnamehash}{ae7b43413b51af2509dfb0dc856f17ce}
      \strng{authorbibnamehash}{ae7b43413b51af2509dfb0dc856f17ce}
      \strng{authornamehash}{4a0baf87803c3ab81423c680df51a084}
      \strng{authorfullhash}{ae7b43413b51af2509dfb0dc856f17ce}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In various cultures and at all spatial scales, humans produce a rich complexity of geometric shapes such as lines, circles or spirals. Here, we propose that humans possess a language of thought for geometric shapes that can produce line drawings as recursive combinations of a minimal set of geometric primitives. We present a programming language, similar to Logo, that combines discrete numbers and continuous integration to form higher-level structures based on repetition, concatenation and embedding, and we show that the simplest programs in this lan\- guage generate the fundamental geometric shapes observed in human cultures. On the perceptual side, we propose that shape perception in humans involves searching for the shortest program that correctly draws the image (program induction). A consequence of this framework is that the mental difficulty of remembering a shape should depend on its minimum description length (MDL) in the proposed language. In two experiments, we show that encoding and processing of geometric shapes is well predicted by MDL. Furthermore, our hypotheses predict additive laws for the psychological complexity of repeated, concatenated or embedded shapes, which we confirm experimentally.}
      \field{issn}{00100285}
      \field{journaltitle}{Cognitive Psychology}
      \field{langid}{english}
      \field{month}{12}
      \field{title}{A Language of Thought for the Mental Representation of Geometric Shapes}
      \field{urlday}{7}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{volume}{139}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{101527}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/CL5R2SDF/Sablé-Meyer et al. - 2022 - A language of thought for the mental representatio.pdf
      \endverb
    \endentry
    \entry{scholkopfCausalRepresentationLearning2021}{article}{}
      \name{author}{7}{}{%
        {{hash=ca31cc11ec9370460148c3a9c48fce45}{%
           family={Sch{ö}lkopf},
           familyi={S\bibinitperiod},
           given={Bernhard},
           giveni={B\bibinitperiod}}}%
        {{hash=c1fe97b077a5720d9fd9483464391fe6}{%
           family={Locatello},
           familyi={L\bibinitperiod},
           given={Francesco},
           giveni={F\bibinitperiod}}}%
        {{hash=6e0afe0dd7f80126ec663a279b8d6212}{%
           family={Bauer},
           familyi={B\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=ae67f83953c6a086ab60e4c5a8518aa4}{%
           family={Ke},
           familyi={K\bibinitperiod},
           given={Nan\bibnamedelima Rosemary},
           giveni={N\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=d2d0778c1cdd451c75b874b58eec7564}{%
           family={Kalchbrenner},
           familyi={K\bibinitperiod},
           given={Nal},
           giveni={N\bibinitperiod}}}%
        {{hash=eb6a43771a55bde951c0119ac5c7ad13}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Anirudh},
           giveni={A\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{a70215356a8de56cfef0850e9b43f897}
      \strng{fullhash}{a13bce3edb2dd54c17a7e63025399c7c}
      \strng{bibnamehash}{a70215356a8de56cfef0850e9b43f897}
      \strng{authorbibnamehash}{a70215356a8de56cfef0850e9b43f897}
      \strng{authornamehash}{a70215356a8de56cfef0850e9b43f897}
      \strng{authorfullhash}{a13bce3edb2dd54c17a7e63025399c7c}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.}
      \field{issn}{1558-2256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{month}{5}
      \field{number}{5}
      \field{title}{Toward {{Causal Representation Learning}}}
      \field{urlday}{26}
      \field{urlmonth}{10}
      \field{urlyear}{2023}
      \field{volume}{109}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{612\bibrangedash 634}
      \range{pages}{23}
      \verb{file}
      \verb /Users/ron/Zotero/storage/FEDKDCNE/Schölkopf et al. - 2021 - Toward Causal Representation Learning.pdf
      \endverb
    \endentry
    \entry{schwartzBehavioralNeuralConstraints2017}{article}{}
      \name{author}{2}{}{%
        {{hash=84a9d3a7a66b6d3d587907fbecb0af8e}{%
           family={Schwartz},
           familyi={S\bibinitperiod},
           given={Odelia},
           giveni={O\bibinitperiod}}}%
        {{hash=1282de9cbb604fcda47c42cf1b751642}{%
           family={Giraldo},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelimb Gonzalo\bibnamedelima Sanchez},
           giveni={L\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{805a21b8a447b64824262ac7b3d8c0ca}
      \strng{fullhash}{805a21b8a447b64824262ac7b3d8c0ca}
      \strng{bibnamehash}{805a21b8a447b64824262ac7b3d8c0ca}
      \strng{authorbibnamehash}{805a21b8a447b64824262ac7b3d8c0ca}
      \strng{authornamehash}{805a21b8a447b64824262ac7b3d8c0ca}
      \strng{authorfullhash}{805a21b8a447b64824262ac7b3d8c0ca}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Biederman (1987) proposed that primitive parts (termed ``geons,'' from geometrical eons) of blocks, cylinders, spheres, and wedges, are important for object recognition. He further argued that recognizing objects, similar to recognizing speech from phonemes, relies on a modest number of such geon parts, and the arrangement of these parts. Although there are an infinite number of ways that wholes can be constructed from their lower level parts, the allowable constructions respect particular constraints (see discussion on compositionality in Bienenstock \& Geman, 1995; Bienenstock, Geman, \& Potter, 1997). Statistical constraints arise because of coherence between wholes and parts, a prominent example of which is the geometric arrangement of the parts (Felzenszwalb \& Huttenlocher, 2005; Felzenszwalb, Girshick, McAllester, \& Ramanan, 2010; Felzenszwalb, McAllester, \& Ramanan, 2008; Fergus, Perona, \& Zisserman, 2003; Fischler \& Elschlager, 1973; Sudderth, Torralba, Freeman, \& Willsky, 2005). That generation follows particular rules implies that its recognition inverse will possess certain properties. We describe this as a partnering principle. For the case of part-whole generation, the partnering recognition principle is a form of binding. This determines how to align parts with the roles they play in the putative wholes, and thereby represent them appropriately. This process thus identifies the parts (e.g., wings, a beak, or legs) and their coherence to infer the whole (bird), a process which can proceed hierarchically up to the whole forest scene Also associated with part-whole generation is reuse, since many parts can be made the same, or at least have similar constructs (e.g., two related wings for each bird; many organized feathers on each wing).}
      \field{issn}{1534-7362}
      \field{journaltitle}{Journal of Vision}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{3}
      \field{title}{Behavioral and Neural Constraints on Hierarchical Representations}
      \field{urlday}{23}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{17}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{13}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/QHNBM48X/Schwartz and Giraldo - 2017 - Behavioral and neural constraints on hierarchical .pdf
      \endverb
    \endentry
    \entry{silverRewardEnough2021}{article}{}
      \name{author}{4}{}{%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=74d941c38e5affcd359ed5814815805f}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Satinder},
           giveni={S\bibinitperiod}}}%
        {{hash=4428b76e1301b2db58587fb18bb59a38}{%
           family={Precup},
           familyi={P\bibinitperiod},
           given={Doina},
           giveni={D\bibinitperiod}}}%
        {{hash=eb920e5277d3d5fd0903f3cd41e11871}{%
           family={Sutton},
           familyi={S\bibinitperiod},
           given={Richard\bibnamedelima S.},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{fullhash}{5ee7cd12ab0a72649cfcb326014445ed}
      \strng{bibnamehash}{5ee7cd12ab0a72649cfcb326014445ed}
      \strng{authorbibnamehash}{5ee7cd12ab0a72649cfcb326014445ed}
      \strng{authornamehash}{4ccacac34637df06f749428b6ec5052e}
      \strng{authorfullhash}{5ee7cd12ab0a72649cfcb326014445ed}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{00043702}
      \field{journaltitle}{Artificial Intelligence}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Reward Is Enough}
      \field{urlday}{27}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{299}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{103535}
      \range{pages}{1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/KX9C7A8R/Silver et al. - 2021 - Reward is enough.pdf
      \endverb
    \endentry
    \entry{silverCognitiveNeuroscienceFunctional2018}{article}{}
      \name{author}{1}{}{%
        {{hash=58e6d91592d6863a643450d773588706}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={Michael\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{58e6d91592d6863a643450d773588706}
      \strng{fullhash}{58e6d91592d6863a643450d773588706}
      \strng{bibnamehash}{58e6d91592d6863a643450d773588706}
      \strng{authorbibnamehash}{58e6d91592d6863a643450d773588706}
      \strng{authornamehash}{58e6d91592d6863a643450d773588706}
      \strng{authorfullhash}{58e6d91592d6863a643450d773588706}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A new brain imaging study reveals that the human cerebellum contains a region that represents visual space that is dissociable from a region displaying visual memory-related activity, with both regions exhibiting precise functional coupling with corresponding cerebral cortical areas.}
      \field{issn}{0960-9822}
      \field{journaltitle}{Current biology : CB}
      \field{month}{11}
      \field{number}{21}
      \field{shorttitle}{Cognitive {{Neuroscience}}}
      \field{title}{Cognitive {{Neuroscience}}: {{Functional Specialization}} in {{Human Cerebellum}}}
      \field{urlday}{23}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{volume}{28}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{R1256\bibrangedash R1258}
      \range{pages}{-1}
      \verb{file}
      \verb /Users/ron/Zotero/storage/CAKZJ87X/Silver - 2018 - Cognitive Neuroscience Functional Specialization .pdf
      \endverb
    \endentry
    \entry{Strick_Dum_Fiez_2009}{article}{}
      \name{author}{3}{}{%
        {{hash=d74b453a229fc7bcac80faa4d0ebbd72}{%
           family={Strick},
           familyi={S\bibinitperiod},
           given={Peter\bibnamedelima L.},
           giveni={P\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=85a4a5247401f986a3572cb0f929c3ee}{%
           family={Dum},
           familyi={D\bibinitperiod},
           given={Richard\bibnamedelima P.},
           giveni={R\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=33f41f729491b388d0aaeae190dd04a8}{%
           family={Fiez},
           familyi={F\bibinitperiod},
           given={Julie\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \strng{namehash}{b09329b86dcd74fe0df4df757ee8d1a9}
      \strng{fullhash}{161f87b8277bedc7b97470a346b638af}
      \strng{bibnamehash}{161f87b8277bedc7b97470a346b638af}
      \strng{authorbibnamehash}{161f87b8277bedc7b97470a346b638af}
      \strng{authornamehash}{b09329b86dcd74fe0df4df757ee8d1a9}
      \strng{authorfullhash}{161f87b8277bedc7b97470a346b638af}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1545-4126}
      \field{journaltitle}{Annual Review of Neuroscience}
      \field{title}{Cerebellum and nonmotor function}
      \field{volume}{32}
      \field{year}{2009}
      \field{pages}{413\bibrangedash 434}
      \range{pages}{22}
      \verb{doi}
      \verb 10.1146/annurev.neuro.31.060407.125606
      \endverb
    \endentry
    \entry{Tee_Taylor_2019}{article}{}
      \name{author}{2}{}{%
        {{hash=07874dfbeabc21762fd1fef2350afb00}{%
           family={Tee},
           familyi={T\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=4a40011350152f4f924b2a02d689a3c7}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Desmond\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{7732b91288f5c8ca0f8648055d1c37cc}
      \strng{fullhash}{7732b91288f5c8ca0f8648055d1c37cc}
      \strng{bibnamehash}{7732b91288f5c8ca0f8648055d1c37cc}
      \strng{authorbibnamehash}{7732b91288f5c8ca0f8648055d1c37cc}
      \strng{authornamehash}{7732b91288f5c8ca0f8648055d1c37cc}
      \strng{authorfullhash}{7732b91288f5c8ca0f8648055d1c37cc}
      \field{sortinit}{T}
      \field{sortinithash}{51f9faf24c60c62ca764a77f78cf5666}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2332-7804}
      \field{journaltitle}{IEEE transactions on molecular, biological, and multi-scale communications}
      \field{month}{10}
      \field{number}{1}
      \field{title}{A Quantized Representation of Probability in the Brain}
      \field{volume}{5}
      \field{year}{2019}
      \field{pages}{19\bibrangedash 29}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/tmbmc.2019.2950182
      \endverb
    \endentry
    \entry{ullmanTheoryLearningStochastic2012}{article}{}
      \name{author}{3}{}{%
        {{hash=e226fb1da66be94e73828778e6eb8af8}{%
           family={Ullman},
           familyi={U\bibinitperiod},
           given={Tomer\bibnamedelima D.},
           giveni={T\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=17c6b9edbad988db7d426fba005cecff}{%
           family={Goodman},
           familyi={G\bibinitperiod},
           given={Noah\bibnamedelima D.},
           giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{71032796cf0cdedd7bd6f8dbed3cecf9}
      \strng{fullhash}{d20f70fcc13e72bfbbf6965c06af1226}
      \strng{bibnamehash}{d20f70fcc13e72bfbbf6965c06af1226}
      \strng{authorbibnamehash}{d20f70fcc13e72bfbbf6965c06af1226}
      \strng{authornamehash}{71032796cf0cdedd7bd6f8dbed3cecf9}
      \strng{authorfullhash}{d20f70fcc13e72bfbbf6965c06af1226}
      \field{extraname}{1}
      \field{sortinit}{U}
      \field{sortinithash}{77a6935510e008adcf5b555e7b4f0711}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present an algorithmic model for the development of children's intuitive theories within a hierarchical Bayesian framework, where theories are described as sets of logical laws generated by a probabilistic context-free grammar. We contrast our approach with connectionist and other emergentist approaches to modeling cognitive development. While their subsymbolic representations provide a smooth error surface that supports efficient gradient-based learning, our symbolic representations are better suited to capturing children's intuitive theories but give rise to a harder learning problem, which can only be solved by exploratory search. Our algorithm attempts to discover the theory that best explains a set of observed data by performing stochastic search at two levels of abstraction: an outer loop in the space of theories and an inner loop in the space of explanations or models generated by each theory given a particular dataset. We show that this stochastic search is capable of learning appropriate theories in several everyday domains and discuss its dynamics in the context of empirical studies of children's learning.}
      \field{issn}{0885-2014}
      \field{journaltitle}{Cognitive Development}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{4}
      \field{series}{The {{Potential Contribution}} of {{Computational Modeling}} to the {{Study}} of {{Cognitive Development}}: {{When}}, and for {{What Topics}}?}
      \field{title}{Theory Learning as Stochastic Search in the Language of Thought}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{27}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{455\bibrangedash 480}
      \range{pages}{26}
      \verb{file}
      \verb /Users/ron/Zotero/storage/XVSR5YLQ/Ullman et al. - 2012 - Theory learning as stochastic search in the langua.pdf;/Users/ron/Zotero/storage/TG7Z99T3/S0885201412000445.html
      \endverb
      \keyw{Algorithms,Bayesian models,Conceptual change,Intuitive theories,Language of thought}
    \endentry
    \entry{ullmanMindGamesGame2017}{article}{}
      \name{author}{4}{}{%
        {{hash=e226fb1da66be94e73828778e6eb8af8}{%
           family={Ullman},
           familyi={U\bibinitperiod},
           given={Tomer\bibnamedelima D.},
           giveni={T\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=35030632b4cf28193f5a8f28f139249a}{%
           family={Spelke},
           familyi={S\bibinitperiod},
           given={Elizabeth},
           giveni={E\bibinitperiod}}}%
        {{hash=5a63429a6e1733e8f3f4dc71cbb6eec9}{%
           family={Battaglia},
           familyi={B\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=5c7ff02e1f8b4c4aa7bfddece14f23d2}{%
           family={Tenenbaum},
           familyi={T\bibinitperiod},
           given={Joshua\bibnamedelima B.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{71032796cf0cdedd7bd6f8dbed3cecf9}
      \strng{fullhash}{c3e2e025b25b3f927fe09410144fc2d1}
      \strng{bibnamehash}{c3e2e025b25b3f927fe09410144fc2d1}
      \strng{authorbibnamehash}{c3e2e025b25b3f927fe09410144fc2d1}
      \strng{authornamehash}{71032796cf0cdedd7bd6f8dbed3cecf9}
      \strng{authorfullhash}{c3e2e025b25b3f927fe09410144fc2d1}
      \field{extraname}{2}
      \field{sortinit}{U}
      \field{sortinithash}{77a6935510e008adcf5b555e7b4f0711}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{issn}{13646613}
      \field{journaltitle}{Trends in Cognitive Sciences}
      \field{langid}{english}
      \field{month}{9}
      \field{number}{9}
      \field{shorttitle}{Mind {{Games}}}
      \field{title}{Mind {{Games}}: {{Game Engines}} as an {{Architecture}} for {{Intuitive Physics}}}
      \field{urlday}{12}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{volume}{21}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{649\bibrangedash 665}
      \range{pages}{17}
      \verb{file}
      \verb /Users/ron/Zotero/storage/J3GZVJY2/Ullman et al. - 2017 - Mind Games Game Engines as an Architecture for In.pdf
      \endverb
    \endentry
    \entry{vanrooijTractableCognitionThesis2008}{article}{}
      \name{author}{1}{}{%
        {{hash=a6fae169feaf9fcba30c9e8869827f3b}{%
           family={Van\bibnamedelima Rooij},
           familyi={V\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Iris},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{a6fae169feaf9fcba30c9e8869827f3b}
      \strng{fullhash}{a6fae169feaf9fcba30c9e8869827f3b}
      \strng{bibnamehash}{a6fae169feaf9fcba30c9e8869827f3b}
      \strng{authorbibnamehash}{a6fae169feaf9fcba30c9e8869827f3b}
      \strng{authornamehash}{a6fae169feaf9fcba30c9e8869827f3b}
      \strng{authorfullhash}{a6fae169feaf9fcba30c9e8869827f3b}
      \field{sortinit}{V}
      \field{sortinithash}{02432525618c08e2b03cac47c19764af}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The recognition that human minds/brains are finite systems with limited resources for computation has led some researchers to advance the Tractable Cognition thesis: Human cognitive capacities are constrained by computational tractability. This thesis, if true, serves cognitive psychology by constraining the space of computational-level theories of cognition. To utilize this constraint, a precise and workable definition of ``computational tractability'' is needed. Following computer science tradition, many cognitive scientists and psychologists define computational tractability as polynomial-time computability, leading to the P-Cognition thesis. This article explains how and why the P-Cognition thesis may be overly restrictive, risking the exclusion of veridical computational-level theories from scientific investigation. An argument is made to replace the P-Cognition thesis by the FPT-Cognition thesis as an alternative formalization of the Tractable Cognition thesis (here, FPT stands for fixed-parameter tractable). Possible objections to the Tractable Cognition thesis, and its proposed formalization, are discussed, and existing misconceptions are clarified.}
      \field{issn}{1551-6709}
      \field{journaltitle}{Cognitive Science}
      \field{langid}{english}
      \field{number}{6}
      \field{title}{The {{Tractable Cognition Thesis}}}
      \field{urlday}{22}
      \field{urlmonth}{12}
      \field{urlyear}{2023}
      \field{volume}{32}
      \field{year}{2008}
      \field{urldateera}{ce}
      \field{pages}{939\bibrangedash 984}
      \range{pages}{46}
      \verb{file}
      \verb /Users/ron/Zotero/storage/KC7IHKGL/Van Rooij - 2008 - The Tractable Cognition Thesis.pdf;/Users/ron/Zotero/storage/UDA496KV/03640210801897856.html
      \endverb
      \keyw{Cognitive modeling,Complexity theory,Computational-level theory,Constraint satisfaction,Intractability,NP-hard,Philosophy of computation,Philosophy of mind}
    \endentry
    \entry{vaswaniAttentionAllYou2017}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=7f28e84700536646dd6620a0db07ad09}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=62efade83d70f0323fe248755e6c90c5}{%
           family={Shazeer},
           familyi={S\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=06649ebab1ea5cac0250746a19764975}{%
           family={Parmar},
           familyi={P\bibinitperiod},
           given={Niki},
           giveni={N\bibinitperiod}}}%
        {{hash=831027ee0ebf22375e2a86afc1881909}{%
           family={Uszkoreit},
           familyi={U\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
        {{hash=27b07e4eacbf4ef7a1438e3badb7dd8d}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Aidan\bibnamedelima N},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=540fcd72e1fa4bbed46604f4e6cff817}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={{Ł}ukasz},
           giveni={Ł\bibinitperiod}}}%
        {{hash=95595a0fefb86187cbc36e551017d332}{%
           family={Polosukhin},
           familyi={P\bibinitperiod},
           given={Illia},
           giveni={I\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{fullhash}{cb26e47f6b8133865271fc8483132297}
      \strng{bibnamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authorbibnamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authornamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authorfullhash}{cb26e47f6b8133865271fc8483132297}
      \field{sortinit}{V}
      \field{sortinithash}{02432525618c08e2b03cac47c19764af}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Attention Is {{All}} You {{Need}}}
      \field{urlday}{19}
      \field{urlmonth}{10}
      \field{urlyear}{2023}
      \field{volume}{30}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/ron/Zotero/storage/BN69XYB3/Vaswani et al. - 2017 - Attention is All you Need.pdf
      \endverb
    \endentry
    \entry{velickovicCLRSAlgorithmicReasoning2022}{misc}{}
      \name{author}{8}{}{%
        {{hash=b1208a71c9b067bcef8fb29c15a09b5d}{%
           family={Veli{č}kovi{ć}},
           familyi={V\bibinitperiod},
           given={Petar},
           giveni={P\bibinitperiod}}}%
        {{hash=db6afdf70ea61bf8ed973a75b90ff818}{%
           family={Badia},
           familyi={B\bibinitperiod},
           given={Adri{à}\bibnamedelima Puigdom{è}nech},
           giveni={A\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=c23f8012677b40de82f339368c393522}{%
           family={Budden},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod}}}%
        {{hash=0fd3bfe4060c9d9c7a6d19c9dfb59e12}{%
           family={Banino},
           familyi={B\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod}}}%
        {{hash=d2c04c9d3d204e19f02b9d07d44354d8}{%
           family={Dashevskiy},
           familyi={D\bibinitperiod},
           given={Misha},
           giveni={M\bibinitperiod}}}%
        {{hash=dc7c14ebfc2b431292d1ff4ded63d5e7}{%
           family={Hadsell},
           familyi={H\bibinitperiod},
           given={Raia},
           giveni={R\bibinitperiod}}}%
        {{hash=24fe1af011227491e54e299ba4cb24b5}{%
           family={Blundell},
           familyi={B\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{a923132f6a9d7d93c40facfd855c7dc4}
      \strng{fullhash}{c5637202ebb92d28c97b475c63a26b32}
      \strng{bibnamehash}{a923132f6a9d7d93c40facfd855c7dc4}
      \strng{authorbibnamehash}{a923132f6a9d7d93c40facfd855c7dc4}
      \strng{authornamehash}{a923132f6a9d7d93c40facfd855c7dc4}
      \strng{authorfullhash}{c5637202ebb92d28c97b475c63a26b32}
      \field{sortinit}{V}
      \field{sortinithash}{02432525618c08e2b03cac47c19764af}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. Several important works have investigated whether neural networks can effectively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorithmic data to evaluate specific hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards unified evaluation, we propose the CLRS Algorithmic Reasoning Benchmark, covering classical algorithms from the Introduction to Algorithms textbook. Our benchmark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{6}
      \field{number}{arXiv:2205.15659}
      \field{title}{The {{CLRS Algorithmic Reasoning Benchmark}}}
      \field{urlday}{14}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2205.15659
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/FA8F8CC8/Veličković et al. - 2022 - The CLRS Algorithmic Reasoning Benchmark.pdf;/Users/ron/Zotero/storage/X9YCWTLB/2205.html
      \endverb
      \keyw{Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{wang2017dynamic}{article}{}
      \name{author}{3}{}{%
        {{hash=9977c03c4025bd86cbe014ceb976aabe}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ke},
           giveni={K\bibinitperiod}}}%
        {{hash=975baf0ae00cfc98fb6c09483005986b}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Rishabh},
           giveni={R\bibinitperiod}}}%
        {{hash=3995a0c731177d2aa23cbb3a6126be27}{%
           family={Su},
           familyi={S\bibinitperiod},
           given={Zhendong},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{e15cb31d7d0d0de4490b56433c9a75fb}
      \strng{fullhash}{98fad9ccebc4973b8535b4adb73748d1}
      \strng{bibnamehash}{98fad9ccebc4973b8535b4adb73748d1}
      \strng{authorbibnamehash}{98fad9ccebc4973b8535b4adb73748d1}
      \strng{authornamehash}{e15cb31d7d0d0de4490b56433c9a75fb}
      \strng{authorfullhash}{98fad9ccebc4973b8535b4adb73748d1}
      \field{sortinit}{W}
      \field{sortinithash}{1af34bd8c148ffb32de1494636b49713}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv preprint arXiv:1711.07163}
      \field{title}{Dynamic Neural Program Embedding for Program Repair}
      \field{year}{2017}
      \verb{eprint}
      \verb 1711.07163
      \endverb
    \endentry
    \entry{windridgeRepresentationalFluidityEmbodied2018}{article}{}
      \name{author}{2}{}{%
        {{hash=7636490f9aef2e71f0b6e9d44c25a40d}{%
           family={Windridge},
           familyi={W\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=6120ab99ae777fd07b1a0fce02992819}{%
           family={Thill},
           familyi={T\bibinitperiod},
           given={Serge},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{704a50275a46102414dfe48f55c6fb03}
      \strng{fullhash}{704a50275a46102414dfe48f55c6fb03}
      \strng{bibnamehash}{704a50275a46102414dfe48f55c6fb03}
      \strng{authorbibnamehash}{704a50275a46102414dfe48f55c6fb03}
      \strng{authornamehash}{704a50275a46102414dfe48f55c6fb03}
      \strng{authorfullhash}{704a50275a46102414dfe48f55c6fb03}
      \field{sortinit}{W}
      \field{sortinithash}{1af34bd8c148ffb32de1494636b49713}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Theories of embodied cognition agree that the body plays some role in human cognition, but disagree on the precise nature of this role. While it is (together with the environment) fundamentally engrained in the so-called 4E (or multi-E) cognition stance, there also exists interpretations wherein the body is merely an input/output interface for cognitive processes that are entirely computational. In the present paper, we show that even if one takes such a strong computationalist position, the role of the body must be more than an interface to the world. To achieve human cognition, the computational mechanisms of a cognitive agent must be capable not only of appropriate reasoning over a given set of symbolic representations; they must in addition be capable of updating the representational framework itself (leading to the titular representational fluidity). We demonstrate this by considering the necessary properties that an artificial agent with these abilities need to possess. The core of the argument is that these updates must be falsifiable in the Popperian sense while simultaneously directing representational shifts in a direction that benefits the agent. We show that this is achieved by the progressive, bottom-up symbolic abstraction of low-level sensorimotor connections followed by top-down instantiation of testable perception-action hypotheses. We then discuss the fundamental limits of this representational updating capacity, concluding that only fully embodied learners exhibiting such a priori perception-action linkages are able to sufficiently ground spontaneously-generated symbolic representations and exhibit the full range of human cognitive capabilities. The present paper therefore has consequences both for the theoretical understanding of human cognition, and for the design of autonomous artificial agents.}
      \field{issn}{0303-2647}
      \field{journaltitle}{Biosystems}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Representational Fluidity in Embodied (Artificial) Cognition}
      \field{urlday}{27}
      \field{urlmonth}{6}
      \field{urlyear}{2022}
      \field{volume}{172}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{9\bibrangedash 17}
      \range{pages}{9}
      \verb{file}
      \verb /Users/ron/Zotero/storage/J64CY8ND/Windridge and Thill - 2018 - Representational fluidity in embodied (artificial).pdf;/Users/ron/Zotero/storage/R2M6VGV3/S0303264718302028.html
      \endverb
      \keyw{Computationalism,Embodied cognition,Representational frameworks,Representational updating}
    \endentry
    \entry{wolfTransformersStateoftheArtNatural2020}{inproceedings}{}
      \name{author}{22}{}{%
        {{hash=c34c67badfd5b3624027e9c8c77a69f6}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=4996c576ebaf91c8655cee3341764e3b}{%
           family={Debut},
           familyi={D\bibinitperiod},
           given={Lysandre},
           giveni={L\bibinitperiod}}}%
        {{hash=b17a6e4a2e8a51a0d77017d50c2ef700}{%
           family={Sanh},
           familyi={S\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod}}}%
        {{hash=79bf176bccb31f7489fcf1895b225764}{%
           family={Chaumond},
           familyi={C\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod}}}%
        {{hash=e62b33de0b0f2c7b4ac5e9dc6b77b754}{%
           family={Delangue},
           familyi={D\bibinitperiod},
           given={Clement},
           giveni={C\bibinitperiod}}}%
        {{hash=a106785413050c01a6a0ac6f0abef459}{%
           family={Moi},
           familyi={M\bibinitperiod},
           given={Anthony},
           giveni={A\bibinitperiod}}}%
        {{hash=d37a5a08104030cc7da7f707c8cca77f}{%
           family={Cistac},
           familyi={C\bibinitperiod},
           given={Pierric},
           giveni={P\bibinitperiod}}}%
        {{hash=bc8b8c6f2d75eb26cbbd360ec7400fd5}{%
           family={Rault},
           familyi={R\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=f8c2f1a6f93e56ceeed1aba8d5419cd3}{%
           family={Louf},
           familyi={L\bibinitperiod},
           given={Remi},
           giveni={R\bibinitperiod}}}%
        {{hash=91d34245e8eaf3e4cb599b6cb1c7b6aa}{%
           family={Funtowicz},
           familyi={F\bibinitperiod},
           given={Morgan},
           giveni={M\bibinitperiod}}}%
        {{hash=7915e7c2f5406f21e3071c706ba303fe}{%
           family={Davison},
           familyi={D\bibinitperiod},
           given={Joe},
           giveni={J\bibinitperiod}}}%
        {{hash=1ded257dc5971d5c24baab3098ad8cbf}{%
           family={Shleifer},
           familyi={S\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=76b0cd59a5d311782bf3511473d79fa5}{%
           family={{von Platen}},
           familyi={v\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=1287b8a94fef838bbc4c5082b35115ad}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Clara},
           giveni={C\bibinitperiod}}}%
        {{hash=7ac0678b9ae57db7ec09752056af6dd4}{%
           family={Jernite},
           familyi={J\bibinitperiod},
           given={Yacine},
           giveni={Y\bibinitperiod}}}%
        {{hash=7eb8f72d050b556d9efd79bc5915bb23}{%
           family={Plu},
           familyi={P\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod}}}%
        {{hash=85c8ce7059401b93c2b0e6525a0f7732}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Canwen},
           giveni={C\bibinitperiod}}}%
        {{hash=7cfeecd888d1cb971a1711b99011122e}{%
           family={Le\bibnamedelima Scao},
           familyi={L\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Teven},
           giveni={T\bibinitperiod}}}%
        {{hash=e8a58125af15e56d1db521647af99086}{%
           family={Gugger},
           familyi={G\bibinitperiod},
           given={Sylvain},
           giveni={S\bibinitperiod}}}%
        {{hash=e52f4e4fdc397218d0c31660a7bef492}{%
           family={Drame},
           familyi={D\bibinitperiod},
           given={Mariama},
           giveni={M\bibinitperiod}}}%
        {{hash=6d3a340cadbc002ee63f9d77109d8d59}{%
           family={Lhoest},
           familyi={L\bibinitperiod},
           given={Quentin},
           giveni={Q\bibinitperiod}}}%
        {{hash=1dd218ae435ec5c0143b3fbbc5da192e}{%
           family={Rush},
           familyi={R\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=e51b2feac8da559d0f1d95788a6eaea4}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Qun},
           giveni={Q\bibinitperiod}}}%
        {{hash=8145edc752707c44d42fd5a4a90049ff}{%
           family={Schlangen},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{255f44073360cf5497333775e34f00d6}
      \strng{fullhash}{9e87b6813415f0baebb812c87fd45874}
      \strng{bibnamehash}{255f44073360cf5497333775e34f00d6}
      \strng{authorbibnamehash}{255f44073360cf5497333775e34f00d6}
      \strng{authornamehash}{255f44073360cf5497333775e34f00d6}
      \strng{authorfullhash}{9e87b6813415f0baebb812c87fd45874}
      \strng{editorbibnamehash}{402fbdb4bdf6cc1bbd540793f0a161d7}
      \strng{editornamehash}{402fbdb4bdf6cc1bbd540793f0a161d7}
      \strng{editorfullhash}{402fbdb4bdf6cc1bbd540793f0a161d7}
      \field{sortinit}{W}
      \field{sortinithash}{1af34bd8c148ffb32de1494636b49713}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.}
      \field{booktitle}{Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}}
      \field{month}{10}
      \field{shorttitle}{Transformers}
      \field{title}{Transformers: {{State-of-the-Art Natural Language Processing}}}
      \field{urlday}{23}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{38\bibrangedash 45}
      \range{pages}{8}
      \verb{file}
      \verb /Users/ron/Zotero/storage/L9LLKBX4/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf
      \endverb
    \endentry
    \entry{wolfram2023chatgpt}{book}{}
      \name{author}{1}{}{%
        {{hash=8ea507c3b48972c396b73f0701b75ca4}{%
           family={Wolfram},
           familyi={W\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Wolfram Media, Incorporated}%
      }
      \strng{namehash}{8ea507c3b48972c396b73f0701b75ca4}
      \strng{fullhash}{8ea507c3b48972c396b73f0701b75ca4}
      \strng{bibnamehash}{8ea507c3b48972c396b73f0701b75ca4}
      \strng{authorbibnamehash}{8ea507c3b48972c396b73f0701b75ca4}
      \strng{authornamehash}{8ea507c3b48972c396b73f0701b75ca4}
      \strng{authorfullhash}{8ea507c3b48972c396b73f0701b75ca4}
      \field{sortinit}{W}
      \field{sortinithash}{1af34bd8c148ffb32de1494636b49713}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-1-57955-081-3}
      \field{title}{What Is {{ChatGPT}} Doing ... and Why Does It Work?}
      \field{year}{2023}
    \endentry
    \entry{zecevicCausalParrotsLarge2023}{misc}{}
      \name{author}{4}{}{%
        {{hash=9d5cf02a5091ac19301ad76511d9d390}{%
           family={Ze{č}evi{ć}},
           familyi={Z\bibinitperiod},
           given={Matej},
           giveni={M\bibinitperiod}}}%
        {{hash=c18a616e997c266164728fc39edead03}{%
           family={Willig},
           familyi={W\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod}}}%
        {{hash=588cb69755d8868479c9af92fca3d891}{%
           family={Dhami},
           familyi={D\bibinitperiod},
           given={Devendra\bibnamedelima Singh},
           giveni={D\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=231ca1992b128d48dbc73100fd196d87}{%
           family={Kersting},
           familyi={K\bibinitperiod},
           given={Kristian},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{a1cecd6d830aa895d0788c956ac90abf}
      \strng{fullhash}{4c0dfb2750c819cde7c23276e22cd5c6}
      \strng{bibnamehash}{4c0dfb2750c819cde7c23276e22cd5c6}
      \strng{authorbibnamehash}{4c0dfb2750c819cde7c23276e22cd5c6}
      \strng{authornamehash}{a1cecd6d830aa895d0788c956ac90abf}
      \strng{authorfullhash}{4c0dfb2750c819cde7c23276e22cd5c6}
      \field{sortinit}{Z}
      \field{sortinithash}{8f7b480688e809b50b6f6577b16f3db5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{8}
      \field{number}{arXiv:2308.13067}
      \field{shorttitle}{Causal {{Parrots}}}
      \field{title}{Causal {{Parrots}}: {{Large Language Models May Talk Causality But Are Not Causal}}}
      \field{urlday}{22}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2308.13067
      \endverb
      \verb{file}
      \verb /Users/ron/Zotero/storage/273VYIQK/Zečević et al. - 2023 - Causal Parrots Large Language Models May Talk Cau.pdf;/Users/ron/Zotero/storage/AIGU2IXB/2308.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
    \endentry
    \entry{zhangHippocampalSpatialRepresentations2023}{article}{}
      \name{author}{4}{}{%
        {{hash=688942a0fb5a0d18edd9449e5d6ce5ca}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Huanqiu},
           giveni={H\bibinitperiod}}}%
        {{hash=f34f212a9c32e2685c8ac6f74b18b389}{%
           family={Rich},
           familyi={R\bibinitperiod},
           given={P.\bibnamedelimi Dylan},
           giveni={P\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=af02e981ace789ea317e2fee1da373ff}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Albert\bibnamedelima K.},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=e8fad1b01309c60abe8feb704ffbd09b}{%
           family={Sharpee},
           familyi={S\bibinitperiod},
           given={Tatyana\bibnamedelima O.},
           giveni={T\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{bfd7a88cce45360c0858c2a085fa5d3f}
      \strng{fullhash}{faa35d1b420e2bce1224038091be94a5}
      \strng{bibnamehash}{faa35d1b420e2bce1224038091be94a5}
      \strng{authorbibnamehash}{faa35d1b420e2bce1224038091be94a5}
      \strng{authornamehash}{bfd7a88cce45360c0858c2a085fa5d3f}
      \strng{authorfullhash}{faa35d1b420e2bce1224038091be94a5}
      \field{extraname}{1}
      \field{sortinit}{Z}
      \field{sortinithash}{8f7b480688e809b50b6f6577b16f3db5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the CA1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of CA1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.}
      \field{issn}{1546-1726}
      \field{journaltitle}{Nature Neuroscience}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{1}
      \field{title}{Hippocampal Spatial Representations Exhibit a Hyperbolic Geometry That Expands with Experience}
      \field{urlday}{25}
      \field{urlmonth}{7}
      \field{urlyear}{2023}
      \field{volume}{26}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{131\bibrangedash 139}
      \range{pages}{9}
      \verb{file}
      \verb /Users/ron/Zotero/storage/IYQFNVWL/Zhang et al. - 2023 - Hippocampal spatial representations exhibit a hype.pdf
      \endverb
      \keyw{Learning and memory,Neural encoding}
    \endentry
    \entry{zhangNovelNeuralSource2019}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=112faefb37293f52cd92c3bb36fcc08b}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
        {{hash=d629ba40b56b1f3070362206453e1639}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xu},
           giveni={X\bibinitperiod}}}%
        {{hash=d7086ec21f72e8e904e3dfb11f97caa6}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Hongyu},
           giveni={H\bibinitperiod}}}%
        {{hash=06ba7cd2fcfda99fd3d43fa17148ea93}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Hailong},
           giveni={H\bibinitperiod}}}%
        {{hash=85b33fef2c2e6fb22f2a34b1f79f276d}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Kaixuan},
           giveni={K\bibinitperiod}}}%
        {{hash=b65e195c2f4fa1de9da518e52422d864}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Xudong},
           giveni={X\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Montreal, QC, Canada}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{e45b8b432900977cd9cc4e6630ca865e}
      \strng{fullhash}{23ff5716519516e005a39eba907dffb8}
      \strng{bibnamehash}{e45b8b432900977cd9cc4e6630ca865e}
      \strng{authorbibnamehash}{e45b8b432900977cd9cc4e6630ca865e}
      \strng{authornamehash}{e45b8b432900977cd9cc4e6630ca865e}
      \strng{authorfullhash}{23ff5716519516e005a39eba907dffb8}
      \field{extraname}{2}
      \field{sortinit}{Z}
      \field{sortinithash}{8f7b480688e809b50b6f6577b16f3db5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.}
      \field{booktitle}{2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})}
      \field{isbn}{978-1-72810-869-8}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{A {{Novel Neural Source Code Representation Based}} on {{Abstract Syntax Tree}}}
      \field{urlday}{8}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{783\bibrangedash 794}
      \range{pages}{12}
      \verb{file}
      \verb /Users/ron/Zotero/storage/39Q8L7T8/Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf
      \endverb
    \endentry
  \enddatalist
  \missing{malkin_trajectory_2022}
\endrefsection
\endinput

