
@InCollection{sep-leibniz-logic-influence,  
author       =	{Peckhaus, Volker},  
title        =	{{Leibniz's Influence on 19th Century Logic}},  
booktitle    =	{The {Stanford} Encyclopedia of Philosophy},  
editor       =	{Edward N. Zalta},  
howpublished =	{\url{[https://plato.stanford.edu/archives/win2018/entries/leibniz-logic-influence/](https://plato.stanford.edu/archives/win2018/entries/leibniz-logic-influence/)}},  
year         =	{2018},  
edition      =	{{W}inter 2018},  
publisher    =	{Metaphysics Research Lab, Stanford University}  
}

@InCollection{sep-language-thought,  
author = {Rescorla, Michael},  
title = {{The Language of Thought Hypothesis}},  
booktitle = {The {Stanford} Encyclopedia of Philosophy},  
editor = {Edward N. Zalta},  
howpublished = {\url{https:[//plato.stanford.edu/archives/sum2019/entries/language-thought/](https:////plato.stanford.edu/archives/sum2019/entries/language-thought/)}},  
year = {2019},  
edition = {{S}ummer 2019},  
publisher =	{Metaphysics Research Lab, Stanford University}  
}

@inproceedings{ellis_dreamcoder_2021,  
address = {Virtual Canada},  
title = {{DreamCoder}: bootstrapping inductive program synthesis with wake-sleep library learning},  
isbn = {978-1-4503-8391-2},  
shorttitle = {{DreamCoder}},  
url = {[https://dl.acm.org/doi/10.1145/3453483.3454080](https://dl.acm.org/doi/10.1145/3453483.3454080)},  
doi = {10.1145/3453483.3454080},  
abstract = {We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of łwake-sleepž approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.},  
language = {en},  
urldate = {2022-05-04},  
booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},  
publisher = {ACM},  
author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sablé-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B.},  
month = jun,  
year = {2021},  
pages = {835--850},  
file = {Ellis et al. - 2021 - DreamCoder bootstrapping inductive program synthe.pdf:/Users/ron/Zotero/storage/IADX3AGN/Ellis et al. - 2021 - DreamCoder bootstrapping inductive program synthe.pdf:application/pdf},  
}

@article{dehaene_symbols_2022,  
title = {Symbols and mental programs: a hypothesis about human singularity},  
volume = {26},  
issn = {13646613},  
shorttitle = {Symbols and mental programs},  
url = {[https://linkinghub.elsevier.com/retrieve/pii/S1364661322001413](https://linkinghub.elsevier.com/retrieve/pii/S1364661322001413)},  
doi = {10.1016/j.tics.2022.06.010},  
language = {en},  
number = {9},  
urldate = {2022-08-22},  
journal = {Trends in Cognitive Sciences},  
author = {Dehaene, Stanislas and Al Roumi, Fosca and Lakretz, Yair and Planton, Samuel and Sablé-Meyer, Mathias},  
month = sep,  
year = {2022},  
pages = {751--766},  
file = {Dehaene et al. - 2022 - Symbols and mental programs a hypothesis about hu.pdf:/Users/ron/Zotero/storage/3QSTG9EM/Dehaene et al. - 2022 - Symbols and mental programs a hypothesis about hu.pdf:application/pdf},  
}

@inproceedings{bengio_flow_2021,  
title = {Flow {Network} based {Generative} {Models} for {Non}-{Iterative} {Diverse} {Candidate} {Generation}},  
volume = {34},  
url = {[https://papers.nips.cc/paper/2021/hash/e614f646836aaed9f89ce58e837e2310-Abstract.html](https://papers.nips.cc/paper/2021/hash/e614f646836aaed9f89ce58e837e2310-Abstract.html)},  
abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation.  Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},  
urldate = {2022-04-13},  
booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},  
publisher = {Curran Associates, Inc.},  
author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},  
year = {2021},  
pages = {27381--27394},  
file = {Full Text PDF:/Users/ron/Zotero/storage/RXZEKNQV/Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf:application/pdf},  
}

@article{bengio_gflownet_2022,  
title = {{GFlowNet} {Foundations}},  
url = {[http://arxiv.org/abs/2111.09266](http://arxiv.org/abs/2111.09266)},  
abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},  
urldate = {2022-04-13},  
journal = {arXiv:2111.09266 [cs, stat]},  
author = {Bengio, Yoshua and Deleu, Tristan and Hu, Edward J. and Lahlou, Salem and Tiwari, Mo and Bengio, Emmanuel},  
month = apr,  
year = {2022},  
note = {arXiv: 2111.09266},  
keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},  
file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/JWITNEXM/Bengio et al. - 2022 - GFlowNet Foundations.pdf:application/pdf;[arXiv.org](http://arXiv.org) Snapshot:/Users/ron/Zotero/storage/VNGUT7WA/2111.html:text/html},  
}

@book{JCopeland2004-JCOTET,  
	title = {The Essential Turing},  
	year = {2004},  
	publisher = {Oxford University Press UK},  
	author = {B. J. Copeland}  
}  

@article{rule_child_2020,  
title = {The {Child} as {Hacker}},  
volume = {24},  
issn = {13646613},  
url = {[https://linkinghub.elsevier.com/retrieve/pii/S1364661320301741](https://linkinghub.elsevier.com/retrieve/pii/S1364661320301741)},  
doi = {10.1016/j.tics.2020.07.005},  
language = {en},  
number = {11},  
urldate = {2022-05-03},  
journal = {Trends in Cognitive Sciences},  
author = {Rule, Joshua S. and Tenenbaum, Joshua B. and Piantadosi, Steven T.},  
month = nov,  
year = {2020},  
pages = {900--915},  
file = {Rule et al. - 2020 - The Child as Hacker.pdf:/Users/ron/Zotero/storage/TVAQQX5F/Rule et al. - 2020 - The Child as Hacker.pdf:application/pdf},  
}

@article{al_roumi_mental_2021,  
title = {Mental compression of spatial sequences in human working memory using numerical and geometrical primitives},  
volume = {109},  
issn = {0896-6273},  
url = {[https://www.sciencedirect.com/science/article/pii/S0896627321004244](https://www.sciencedirect.com/science/article/pii/S0896627321004244)},  
doi = {10.1016/j.neuron.2021.06.009},  
abstract = {How does the human brain store sequences of spatial locations? We propose that each sequence is internally compressed using an abstract, language-like code that captures its numerical and geometrical regularities. We exposed participants to spatial sequences of fixed length but variable regularity while their brain activity was recorded using magneto-encephalography. Using multivariate decoders, each successive location could be decoded from brain signals, and upcoming locations were anticipated prior to their actual onset. Crucially, sequences with lower complexity, defined as the minimal description length provided by the formal language, led to lower error rates and to increased anticipations. Furthermore, neural codes specific to the numerical and geometrical primitives of the postulated language could be detected, both in isolation and within the sequences. These results suggest that the human brain detects sequence regularities at multiple nested levels and uses them to compress long sequences in working memory.},  
language = {en},  
number = {16},  
urldate = {2022-10-29},  
journal = {Neuron},  
author = {Al Roumi, Fosca and Marti, Sébastien and Wang, Liping and Amalric, Marie and Dehaene, Stanislas},  
month = aug,  
year = {2021},  
keywords = {Geometry, Language of Thought, Magnetoencephalography, Memory, Ordinal Knowledge, Primitive Operations, Sequence Processing, Sequence Structure, Syntax},  
pages = {2627--2639.e4},  
}

@article{piantadosi2021computational,  
  title={The computational origin of representation},  
  author={Piantadosi, Steven T},  
  journal={Minds and machines},  
  volume={31},  
  number={1},  
  pages={1--58},  
  year={2021},  
  publisher={Springer}  
}

@article{piantasodi2022meaning,  
  title={Meaning without reference in large language models},  
  author={Piantasodi, Steven T and Hill, Felix},  
  journal={arXiv preprint arXiv:2208.02957},  
  year={2022}  
}

@article{do2021neural,  
  title={Neural circuits and symbolic processing},  
  author={Do, Quan and Hasselmo, Michael E},  
  journal={Neurobiology of Learning and Memory},  
  volume={186},  
  pages={107552},  
  year={2021},  
  publisher={Elsevier}  
}

@article{santoro2021symbolic,  
  title={Symbolic behaviour in artificial intelligence},  
  author={Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},  
  journal={arXiv preprint arXiv:2102.03406},  
  year={2021}  
}

@article{allamanis2017learning,  
  title={Learning to represent programs with graphs},  
  author={Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},  
  journal={arXiv preprint arXiv:1711.00740},  
  year={2017}  
}

@article{wang2017dynamic,  
title={Dynamic neural program embedding for program repair},  
author={Wang, Ke and Singh, Rishabh and Su, Zhendong},  
journal={arXiv preprint arXiv:1711.07163},  
year={2017}  
}

@article{ibarz2022generalist,  
  title={A generalist neural algorithmic learner},  
  author={Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csord{\'a}s, R{\'o}bert and Dudzik, Andrew and Bo{\v{s}}njak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and others},  
  journal={arXiv preprint arXiv:2209.11142},  
  year={2022}  
}

@book{zadra_stickgold_2022,   
place={New York, NY},   
title={When brains dream: Understanding the science and Mystery of our dreaming minds}, 
publisher={W.W. Norton \& Company, Inc.},   
author={Zadra, Antonio and Stickgold, R.},   
year={2022}  
} 

@article{zhang2020connecting,  
  title={Connecting concepts in the brain by mapping cortical representations of semantic relations},
  author={Zhang, Yizhen and Han, Kuan and Worth, Robert and Liu, Zhongming},  
  journal={Nature communications},  
  volume={11},  
  number={1},  
  pages={1--13},  
  year={2020},  
  publisher={Nature Publishing Group}  
}

@article{lewis_how_2018,  
title = {How {Memory} {Replay} in {Sleep} {Boosts} {Creative} {Problem}-{Solving}},  
volume = {22},  
issn = {13646613},  
url = {[https://linkinghub.elsevier.com/retrieve/pii/S1364661318300706](https://linkinghub.elsevier.com/retrieve/pii/S1364661318300706)},  
doi = {10.1016/j.tics.2018.03.009},  
language = {en},  
number = {6},  
urldate = {2022-05-04},  
journal = {Trends in Cognitive Sciences},  
author = {Lewis, Penelope A. and Knoblich, Günther and Poe, Gina},  
month = jun,  
year = {2018},  
pages = {491--503},
}

@InCollection{sep-goedel-incompleteness,
	author       =	{Raatikainen, Panu},
	title        =	{{Gödel’s Incompleteness Theorems}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2022/entries/goedel-incompleteness/}},
	year         =	{2022},
	edition      =	{{S}pring 2022},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{chomsky1959certain,
  title={On certain formal properties of grammars},
  author={Chomsky, Noam},
  journal={Information and control},
  volume={2},
  number={2},
  pages={137--167},
  year={1959},
  publisher={Elsevier}
}

@book{hofstadter_gdel_1979,
  added-at = {2009-02-19T14:19:12.000+0100},
  author = {Hofstadter, Douglas R.},
  biburl = {https://www.bibsonomy.org/bibtex/28e4bfa972541e430f9520006e053ef9b/ivan},
  interhash = {311d5d81ee9dfd0fccabd4beafdc671c},
  intrahash = {8e4bfa972541e430f9520006e053ef9b},
  keywords = {logic mathematics},
  publisher = {Basic Books Inc.},
  timestamp = {2009-02-19T14:19:14.000+0100},
  title = {Gödel, Escher, Bach: an Eternal Golden Braid},
  year = 1979
}

@article{garcez2020neurosymbolic,
  title={Neurosymbolic AI: the 3rd wave},
  author={Garcez, Artur d'Avila and Lamb, Luis C},
  journal={arXiv preprint arXiv:2012.05876},
  year={2020}
}

@article{hinton1995wake,
  title={The" wake-sleep" algorithm for unsupervised neural networks},
  author={Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
  journal={Science},
  volume={268},
  number={5214},
  pages={1158--1161},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

@book{10.5555/1593511, 
 author = {Van Rossum, Guido and Drake, Fred L.}, 
 title = {Python 3 Reference Manual}, 
 year = {2009}, 
 isbn = {1441412697}, 
 publisher = {CreateSpace}, 
 address = {Scotts Valley, CA} 
}

@incollection{NEURIPS2019_9015, 
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, 
booktitle = {Advances in Neural Information Processing Systems 32}, 
pages = {8024--8035}, 
year = {2019}, 
publisher = {Curran Associates, Inc.}, 
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} 
}


@inproceedings{liu_discrete-valued_2021,
	title = {Discrete-{Valued} {Neural} {Communication}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/10907813b97e249163587e6246612e21-Abstract.html},
	abstract = {Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that discrete-valued neural communication (DVNC) substantially improves systematic generalization in a variety of architectures—transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method useful in practice.},
	urldate = {2022-04-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Dianbo and Lamb, Alex M and Kawaguchi, Kenji and ALIAS PARTH GOYAL, Anirudh Goyal and Sun, Chen and Mozer, Michael C and Bengio, Yoshua},
	year = {2021},
	pages = {2109--2121},
	file = {Full Text PDF:/Users/ron/Zotero/storage/5CTB3K88/Liu et al. - 2021 - Discrete-Valued Neural Communication.pdf:application/pdf},
}

@article{bengio_consciousness_2019,
	title = {The {Consciousness} {Prior}},
	url = {http://arxiv.org/abs/1709.08568},
	abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
	urldate = {2022-04-11},
	journal = {arXiv:1709.08568 [cs, stat]},
	author = {Bengio, Yoshua},
	month = dec,
	year = {2019},
	note = {arXiv: 1709.08568},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/4WD9F3RB/Bengio - 2019 - The Consciousness Prior.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/MQ3DLVT6/1709.html:text/html},
}

@inproceedings{zhao_consciousness-inspired_2021,
	title = {A {Consciousness}-{Inspired} {Planning} {Agent} for {Model}-{Based} {Reinforcement} {Learning}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/0c215f194276000be6a6df6528067151-Abstract.html},
	abstract = {We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance.},
	urldate = {2022-04-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Mingde and Liu, Zhen and Luan, Sitao and Zhang, Shuyuan and Precup, Doina and Bengio, Yoshua},
	year = {2021},
	pages = {1569--1581},
	file = {Full Text PDF:/Users/ron/Zotero/storage/CZ2Y6BRT/Zhao et al. - 2021 - A Consciousness-Inspired Planning Agent for Model-.pdf:application/pdf},
}

@article{hartwigsen_how_2021,
	title = {How does hemispheric specialization contribute to human-defining cognition?},
	volume = {109},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627321002907},
	doi = {10.1016/j.neuron.2021.04.024},
	abstract = {Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman’s System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.},
	language = {en},
	number = {13},
	urldate = {2022-04-11},
	journal = {Neuron},
	author = {Hartwigsen, Gesa and Bengio, Yoshua and Bzdok, Danilo},
	month = jul,
	year = {2021},
	keywords = {artificial general intelligence, computational design principles, deep learning, global workspace theory, human intelligence, language},
	pages = {2075--2090},
	file = {Hartwigsen et al. - 2021 - How does hemispheric specialization contribute to .pdf:/Users/ron/Zotero/storage/IDD8EDKX/Hartwigsen et al. - 2021 - How does hemispheric specialization contribute to .pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/FMMQTTF6/S0896627321002907.html:text/html},
}

@article{blokpoel_journal_2018,
	title = {Journal of {Problem} {Solving}},
	volume = {11},
	language = {en},
	author = {Blokpoel, Mark and Wareham, Todd and Haselager, Pim and Toni, Ivan and van Rooij, Iris},
	year = {2018},
	pages = {24},
	file = {Deep Analogical Inference as the Origin of Hypotheses.pdf:/Users/ron/Documents/Artificial Intelligence/Master/1. Year/2. Semester/Theoretical Foundations of Cognitive Agents/Literature/Block IV - Applications/Deep Analogical Inference as the Origin of Hypotheses.pdf:application/pdf},
}

@article{van_gerven_computational_2017,
	title = {Computational {Foundations} of {Natural} {Intelligence}},
	volume = {11},
	issn = {1662-5188},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2017.00112/full},
	doi = {10.3389/fncom.2017.00112},
	abstract = {New developments in AI and neuroscience are revitalizing the quest to understanding natural intelligence, offering insight about how to equip machines with human-like capabilities. This paper reviews some of the computational principles relevant for understanding natural intelligence and, ultimately, achieving strong AI. After reviewing basic principles, a variety of computational modeling approaches is discussed. Subsequently, I concentrate on the use of artiﬁcial neural networks as a framework for modeling cognitive processes. This paper ends by outlining some of the challenges that remain to fulﬁll the promise of machines that show human-like intelligence.},
	language = {en},
	urldate = {2022-04-13},
	journal = {Frontiers in Computational Neuroscience},
	author = {van Gerven, Marcel},
	month = dec,
	year = {2017},
	pages = {112},
	file = {van Gerven - 2017 - Computational Foundations of Natural Intelligence.pdf:/Users/ron/Zotero/storage/2D2EH52L/van Gerven - 2017 - Computational Foundations of Natural Intelligence.pdf:application/pdf},
}

@article{hasson_direct_2020,
	title = {Direct {Fit} to {Nature}: {An} {Evolutionary} {Perspective} on {Biological} and {Artificial} {Neural} {Networks}},
	volume = {105},
	issn = {08966273},
	shorttitle = {Direct {Fit} to {Nature}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731931044X},
	doi = {10.1016/j.neuron.2019.12.002},
	language = {en},
	number = {3},
	urldate = {2022-04-13},
	journal = {Neuron},
	author = {Hasson, Uri and Nastase, Samuel A. and Goldstein, Ariel},
	month = feb,
	year = {2020},
	pages = {416--434},
	file = {Hasson et al. - 2020 - Direct Fit to Nature An Evolutionary Perspective .pdf:/Users/ron/Zotero/storage/2MASAIZ3/Hasson et al. - 2020 - Direct Fit to Nature An Evolutionary Perspective .pdf:application/pdf},
}


@article{campero_learning_2021,
	title = {Learning with {AMIGo}: {Adversarially} {Motivated} {Intrinsic} {Goals}},
	shorttitle = {Learning with {AMIGo}},
	url = {http://arxiv.org/abs/2006.12122},
	abstract = {A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating -- as form of meta-learning -- a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned "student" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective "constructively adversarial" objective, the teacher learns to propose increasingly challenging -- yet achievable -- goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.},
	urldate = {2022-05-03},
	journal = {arXiv:2006.12122 [cs, stat]},
	author = {Campero, Andres and Raileanu, Roberta and Küttler, Heinrich and Tenenbaum, Joshua B. and Rocktäschel, Tim and Grefenstette, Edward},
	month = feb,
	year = {2021},
	note = {arXiv: 2006.12122},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/7MN9NYWE/Campero et al. - 2021 - Learning with AMIGo Adversarially Motivated Intri.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/DEPP4BFU/2006.html:text/html},
}

@article{du_improved_2021,
	title = {Improved {Contrastive} {Divergence} {Training} of {Energy} {Based} {Models}},
	url = {http://arxiv.org/abs/2012.01316},
	abstract = {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases,such as image generation, OOD detection, and compositional generation.},
	urldate = {2022-05-03},
	journal = {arXiv:2012.01316 [cs]},
	author = {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
	month = jun,
	year = {2021},
	note = {arXiv: 2012.01316},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/8UNGBQJ3/Du et al. - 2021 - Improved Contrastive Divergence Training of Energy.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/WFSJ6VMD/2012.html:text/html},
}

@article{sun_stochastic_2019,
	title = {Stochastic {Prediction} of {Multi}-{Agent} {Interactions} from {Partial} {Observations}},
	url = {http://arxiv.org/abs/1902.09641},
	abstract = {We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.},
	urldate = {2022-05-03},
	journal = {arXiv:1902.09641 [cs, stat]},
	author = {Sun, Chen and Karlsson, Per and Wu, Jiajun and Tenenbaum, Joshua B. and Murphy, Kevin},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09641},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/9Y8U84ZE/Sun et al. - 2019 - Stochastic Prediction of Multi-Agent Interactions .pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/DMNZWJGF/1902.html:text/html},
}

@article{whitney_understanding_2016,
	title = {Understanding {Visual} {Concepts} with {Continuation} {Learning}},
	url = {http://arxiv.org/abs/1602.06822},
	abstract = {We introduce a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.},
	urldate = {2022-05-03},
	journal = {arXiv:1602.06822 [cs]},
	author = {Whitney, William F. and Chang, Michael and Kulkarni, Tejas and Tenenbaum, Joshua B.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.06822},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/VT5H7LK3/Whitney et al. - 2016 - Understanding Visual Concepts with Continuation Le.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/4JFZYCT3/1602.html:text/html},
}

@article{deleu_bayesian_2022,
	title = {Bayesian {Structure} {Learning} with {Generative} {Flow} {Networks}},
	url = {http://arxiv.org/abs/2202.13903},
	abstract = {In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.},
	urldate = {2022-05-03},
	journal = {arXiv:2202.13903 [cs, stat]},
	author = {Deleu, Tristan and Góis, António and Emezue, Chris and Rankawat, Mansi and Lacoste-Julien, Simon and Bauer, Stefan and Bengio, Yoshua},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.13903},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/PUJ45TDP/Deleu et al. - 2022 - Bayesian Structure Learning with Generative Flow N.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/9M2SQGIG/2202.html:text/html},
}

@article{ke_learning_2022,
	title = {Learning to {Induce} {Causal} {Structure}},
	url = {http://arxiv.org/abs/2204.04875},
	abstract = {The fundamental challenge in causal induction is to infer the underlying graph structure given observational and/or interventional data. Most existing causal induction algorithms operate by generating candidate graphs and then evaluating them using either score-based methods (including continuous optimization) or independence tests. In this work, instead of proposing scoring function or independence tests, we treat the inference process as a black box and design a neural network architecture that learns the mapping from both observational and interventional data to graph structures via supervised training on synthetic graphs. We show that the proposed model generalizes not only to new synthetic graphs but also to naturalistic graphs.},
	urldate = {2022-05-03},
	journal = {arXiv:2204.04875 [cs, stat]},
	author = {Ke, Nan Rosemary and Chiappa, Silvia and Wang, Jane and Bornschein, Jorg and Weber, Theophane and Goyal, Anirudh and Botvinic, Matthew and Mozer, Michael and Rezende, Danilo Jimenez},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.04875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/C4ECNTTI/Ke et al. - 2022 - Learning to Induce Causal Structure.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/8CXBU29F/2204.html:text/html},
}


@article{garcez_neurosymbolic_2020,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {Neurosymbolic {AI}},
	url = {http://arxiv.org/abs/2012.05876},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	urldate = {2022-05-03},
	journal = {arXiv:2012.05876 [cs]},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.05876},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/XZHCS3J2/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/WYKZKFLG/2012.html:text/html},
}

@article{christen_cybernetical_2019,
	title = {Cybernetical {Concepts} for {Cellular} {Automaton} and {Artificial} {Neural} {Network} {Modelling} and {Implementation}},
	url = {http://arxiv.org/abs/2001.02037},
	doi = {10.1109/SMC.2019.8913839},
	abstract = {As a discipline cybernetics has a long and rich history. In its first generation it not only had a worldwide span, in the area of computer modelling, for example, its proponents such as John von Neumann, Stanislaw Ulam, Warren McCulloch and Walter Pitts, also came up with models and methods such as cellular automata and artificial neural networks, which are still the foundation of most modern modelling approaches. At the same time, cybernetics also got the attention of philosophers, such as the Frenchman Gilbert Simondon, who made use of cybernetical concepts in order to establish a metaphysics and a natural philosophy of individuation, giving cybernetics thereby a philosophical interpretation, which he baptised allagmatic. In this paper, we emphasise this allagmatic theory by showing how Simondon's philosophical concepts can be used to formulate a generic computer model or metamodel for complex systems modelling and its implementation in program code, according to generic programming. We also present how the developed allagmatic metamodel is capable of building simple cellular automata and artificial neural networks.},
	urldate = {2022-05-03},
	journal = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
	author = {Christen, Patrik and Del Fabbro, Olivier},
	month = oct,
	year = {2019},
	note = {arXiv: 2001.02037},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Other Computer Science},
	pages = {4124--4130},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/5X4KAZA8/Christen and Del Fabbro - 2019 - Cybernetical Concepts for Cellular Automaton and A.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/4MMMGGLL/2001.html:text/html},
}

@article{ellis_dreamcoder_nodate,
	title = {{DreamCoder}: {Building} interpretable hierarchical knowledge representations with wake-sleep {Bayesian} program learning},
	language = {en},
	author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},
	pages = {68},
	file = {Ellis et al. - DreamCoder Building interpretable hierarchical kn.pdf:/Users/ron/Zotero/storage/LSZ8KD7V/Ellis et al. - DreamCoder Building interpretable hierarchical kn.pdf:application/pdf},
}

@article{ellis_dreamcoder_2020,
	title = {{DreamCoder}: {Growing} generalizable, interpretable knowledge with wake-sleep {Bayesian} program learning},
	shorttitle = {{DreamCoder}},
	url = {http://arxiv.org/abs/2006.08381},
	abstract = {Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.},
	language = {en},
	urldate = {2022-05-04},
	journal = {arXiv:2006.08381 [cs]},
	author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf:/Users/ron/Zotero/storage/S8XHGWRJ/Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf:application/pdf},
}


@article{lewis_how_2018,
	title = {How {Memory} {Replay} in {Sleep} {Boosts} {Creative} {Problem}-{Solving}},
	volume = {22},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661318300706},
	doi = {10.1016/j.tics.2018.03.009},
	language = {en},
	number = {6},
	urldate = {2022-05-04},
	journal = {Trends in Cognitive Sciences},
	author = {Lewis, Penelope A. and Knoblich, Günther and Poe, Gina},
	month = jun,
	year = {2018},
	pages = {491--503},
	file = {Lewis et al. - 2018 - How Memory Replay in Sleep Boosts Creative Problem.pdf:/Users/ron/Zotero/storage/GA6G7JVB/Lewis et al. - 2018 - How Memory Replay in Sleep Boosts Creative Problem.pdf:application/pdf},
}

@article{graziano_attention_2017,
	title = {The {Attention} {Schema} {Theory}: {A} {Foundation} for {Engineering} {Artificial} {Consciousness}},
	volume = {4},
	issn = {2296-9144},
	shorttitle = {The {Attention} {Schema} {Theory}},
	url = {https://www.frontiersin.org/article/10.3389/frobt.2017.00060},
	abstract = {The purpose of the attention schema theory is to explain how an information-processing device, the brain, arrives at the claim that it possesses a non-physical, subjective awareness and assigns a high degree of certainty to that extraordinary claim. The theory does not address how the brain might actually possess a non-physical essence. It is not a theory that deals in the non-physical. It is about the computations that cause a machine to make a claim and to assign a high degree of certainty to the claim. The theory is offered as a possible starting point for building artificial consciousness. Given current technology, it should be possible to build a machine that contains a rich internal model of what consciousness is, attributes that property of consciousness to itself and to the people it interacts with, and uses that attribution to make predictions about human behavior. Such a machine would “believe” it is conscious and act like it is conscious, in the same sense that the human machine believes and acts.},
	urldate = {2022-05-05},
	journal = {Frontiers in Robotics and AI},
	author = {Graziano, Michael S. A.},
	year = {2017},
	file = {Full Text PDF:/Users/ron/Zotero/storage/HG4N7L3F/Graziano - 2017 - The Attention Schema Theory A Foundation for Engi.pdf:application/pdf},
}

@inproceedings{grattarola_learning_2021,
	title = {Learning {Graph} {Cellular} {Automata}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/af87f7cdcda223c41c3f3ef05a3aaeea-Abstract.html},
	abstract = {Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.},
	urldate = {2022-05-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
	year = {2021},
	pages = {20983--20994},
	file = {Full Text PDF:/Users/ron/Zotero/storage/Q53QTD55/Grattarola et al. - 2021 - Learning Graph Cellular Automata.pdf:application/pdf},
}

@article{wolff_information_2019,
	title = {Information {Compression} as a {Unifying} {Principle} in {Human} {Learning}, {Perception}, and {Cognition}},
	volume = {2019},
	issn = {1076-2787, 1099-0526},
	url = {https://www.hindawi.com/journals/complexity/2019/1879746/},
	doi = {10.1155/2019/1879746},
	abstract = {This paper reviews evidence for the idea that much of human learning, perception, and cognition may be understood as information compression and often more specifically as “information compression via the matching and unification of patterns” (ICMUP). Evidence includes the following: information compression can mean selective advantage for any creature; the storage and utilisation of the relatively enormous quantities of sensory information would be made easier if the redundancy of incoming information was to be reduced; content words in natural languages, with their meanings, may be seen as ICMUP; other techniques for compression of information—such as class-inclusion hierarchies, schema-plus-correction, run-length coding, and part-whole hierarchies—may be seen in psychological phenomena; ICMUP may be seen in how we merge multiple views to make one, in recognition, in binocular vision, in how we can abstract object concepts via motion, in adaptation of sensory units in the eye of
              Limulus
              , the horseshoe crab, and in other examples of adaptation; the discovery of the segmental structure of language (words and phrases), grammatical inference, and the correction of over- and undergeneralisations in learning may be understood in terms of ICMUP; information compression may be seen in the perceptual
              constancies
              ; there is indirect evidence for ICMUP in human cognition via kinds of redundancy such as the decimal expansion of
              
                
                  π
                
              
              which are difficult for people to detect; much of the structure and workings of mathematics—an aid to human thinking—may be understood in terms of ICMUP; and there is additional evidence via the
              SP Theory of Intelligence
              and its realisation in the
              SP Computer Model
              . Three objections to the main thesis of this paper are described, with suggested answers. These ideas may be seen to be part of a “Big Picture” with six components, outlined in the paper.},
	language = {en},
	urldate = {2022-05-12},
	journal = {Complexity},
	author = {Wolff, J. Gerard},
	month = feb,
	year = {2019},
	pages = {1--38},
	file = {Wolff - 2019 - Information Compression as a Unifying Principle in.pdf:/Users/ron/Zotero/storage/4UZLRVGM/Wolff - 2019 - Information Compression as a Unifying Principle in.pdf:application/pdf},
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	issn = {1471-003X, 1471-0048},
	shorttitle = {The free-energy principle},
	url = {https://www.nature.com/articles/nrn2787},
	doi = {10.1038/nrn2787},
	abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories — optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
	language = {en},
	number = {2},
	urldate = {2022-05-12},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	pages = {127--138},
	file = {Friston - 2010 - The free-energy principle a unified brain theory.pdf:/Users/ron/Zotero/storage/HH9RIKYL/Friston - 2010 - The free-energy principle a unified brain theory.pdf:application/pdf},
}

@article{hurley_shared_2008,
	title = {The shared circuits model ({SCM}): how control, mirroring, and simulation can enable imitation, deliberation, and mindreading},
	volume = {31},
	issn = {1469-1825},
	shorttitle = {The shared circuits model ({SCM})},
	doi = {10.1017/S0140525X07003123},
	abstract = {Imitation, deliberation, and mindreading are characteristically human sociocognitive skills. Research on imitation and its role in social cognition is flourishing across various disciplines. Imitation is surveyed in this target article under headings of behavior, subpersonal mechanisms, and functions of imitation. A model is then advanced within which many of the developments surveyed can be located and explained. The shared circuits model (SCM) explains how imitation, deliberation, and mindreading can be enabled by subpersonal mechanisms of control, mirroring, and simulation. It is cast at a middle, functional level of description, that is, between the level of neural implementation and the level of conscious perceptions and intentional actions. The SCM connects shared informational dynamics for perception and action with shared informational dynamics for self and other, while also showing how the action/perception, self/other, and actual/possible distinctions can be overlaid on these shared informational dynamics. It avoids the common conception of perception and action as separate and peripheral to central cognition. Rather, it contributes to the situated cognition movement by showing how mechanisms for perceiving action can be built on those for active perception.;{\textgreater};{\textgreater}The SCM is developed heuristically, in five layers that can be combined in various ways to frame specific ontogenetic or phylogenetic hypotheses. The starting point is dynamic online motor control, whereby an organism is closely attuned to its embedding environment through sensorimotor feedback. Onto this are layered functions of prediction and simulation of feedback, mirroring, simulation of mirroring, monitored inhibition of motor output, and monitored simulation of input. Finally, monitored simulation of input specifying possible actions plus inhibited mirroring of such possible actions can generate information about the possible as opposed to actual instrumental actions of others, and the possible causes and effects of such possible actions, thereby enabling strategic social deliberation. Multiple instances of such shared circuits structures could be linked into a network permitting decomposition and recombination of elements, enabling flexible control, imitative learning, understanding of other agents, and instrumental and strategic deliberation. While more advanced forms of social cognition, which require tracking multiple others and their multiple possible actions, may depend on interpretative theorizing or language, the SCM shows how layered mechanisms of control, mirroring, and simulation can enable distinctively human cognitive capacities for imitation, deliberation, and mindreading.},
	language = {eng},
	number = {1},
	journal = {The Behavioral and Brain Sciences},
	author = {Hurley, Susan},
	month = feb,
	year = {2008},
	pmid = {18394222},
	keywords = {Consciousness, Brain, Choice Behavior, Cognition, Feedback, Humans, Imitative Behavior, Nerve Net, Neurons, Social Perception},
	pages = {1--22; discussion 22--58},
	file = {Submitted Version:/Users/ron/Zotero/storage/CZ938SZY/Hurley - 2008 - The shared circuits model (SCM) how control, mirr.pdf:application/pdf},
}

@article{rauss_what_2013,
	title = {What is {Bottom}-{Up} and {What} is {Top}-{Down} in {Predictive} {Coding}?},
	volume = {4},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2013.00276},
	abstract = {Everyone knows what bottom-up is, and how it is different from top-down. At least one is tempted to think so, given that both terms are ubiquitously used, but only rarely defined in the psychology and neuroscience literature. In this review, we highlight the problems and limitations of our current understanding of bottom-up and top-down processes, and we propose a reformulation of this distinction in terms of predictive coding.},
	urldate = {2022-05-14},
	journal = {Frontiers in Psychology},
	author = {Rauss, Karsten and Pourtois, Gilles},
	year = {2013},
	file = {Full Text PDF:/Users/ron/Zotero/storage/ZJLPPHZL/Rauss and Pourtois - 2013 - What is Bottom-Up and What is Top-Down in Predicti.pdf:application/pdf},
}

@article{seth_theories_2022,
	title = {Theories of consciousness},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/s41583-022-00587-4},
	doi = {10.1038/s41583-022-00587-4},
	abstract = {Recent years have seen a blossoming of theories about the biological and physical basis of consciousness. Good theories guide empirical research, allowing us to interpret data, develop new experimental techniques and expand our capacity to manipulate the phenomenon of interest. Indeed, it is only when couched in terms of a theory that empirical discoveries can ultimately deliver a satisfying understanding of a phenomenon. However, in the case of consciousness, it is unclear how current theories relate to each other, or whether they can be empirically distinguished. To clarify this complicated landscape, we review four prominent theoretical approaches to consciousness: higher-o rder theories, global workspace theories, re-e ntry and predictive processing theories and integrated information theory. We describe the key characteristics of each approach by identifying which aspects of consciousness they propose to explain, what their neurobiological commitments are and what empirical data are adduced in their support. We consider how some prominent empirical debates might distinguish among these theories, and we outline three ways in which theories need to be developed to deliver a mature regimen of theory-t esting in the neuroscience of consciousness. There are good reasons to think that the iterative development, testing and comparison of theories of consciousness will lead to a deeper understanding of this most profound of mysteries.},
	language = {en},
	urldate = {2022-05-18},
	journal = {Nature Reviews Neuroscience},
	author = {Seth, Anil K. and Bayne, Tim},
	month = may,
	year = {2022},
	file = {Seth and Bayne - 2022 - Theories of consciousness.pdf:/Users/ron/Zotero/storage/R5AJQRL3/Seth and Bayne - 2022 - Theories of consciousness.pdf:application/pdf},
}

@article{mordatch_concept_2018,
	title = {Concept {Learning} with {Energy}-{Based} {Models}},
	url = {http://arxiv.org/abs/1811.02486},
	abstract = {Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at sites.google.com/site/energyconceptmodels},
	urldate = {2022-05-19},
	journal = {arXiv:1811.02486 [cs]},
	author = {Mordatch, Igor},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.02486},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/K9ALTIGW/Mordatch - 2018 - Concept Learning with Energy-Based Models.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/SE73GNA3/1811.html:text/html},
}

@techreport{luo_diffusion_2021,
	title = {Diffusion {Probabilistic} {Models} for {3D} {Point} {Cloud} {Generation}},
	url = {http://arxiv.org/abs/2103.01458},
	abstract = {We present a probabilistic model for point cloud generation, which is fundamental for various 3D vision tasks such as shape completion, upsampling, synthesis and data augmentation. Inspired by the diffusion process in non-equilibrium thermodynamics, we view points in point clouds as particles in a thermodynamic system in contact with a heat bath, which diffuse from the original distribution to a noise distribution. Point cloud generation thus amounts to learning the reverse diffusion process that transforms the noise distribution to the distribution of a desired shape. Specifically, we propose to model the reverse diffusion process for point clouds as a Markov chain conditioned on certain shape latent. We derive the variational bound in closed form for training and provide implementations of the model. Experimental results demonstrate that our model achieves competitive performance in point cloud generation and auto-encoding. The code is available at {\textbackslash}url\{https://github.com/luost26/diffusion-point-cloud\}.},
	number = {arXiv:2103.01458},
	urldate = {2022-06-27},
	institution = {arXiv},
	author = {Luo, Shitong and Hu, Wei},
	month = jun,
	year = {2021},
	note = {arXiv:2103.01458 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/788P3YQR/Luo and Hu - 2021 - Diffusion Probabilistic Models for 3D Point Cloud .pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/4ZH4XWJE/2103.html:text/html},
}

@article{windridge_representational_2018,
	title = {Representational fluidity in embodied (artificial) cognition},
	volume = {172},
	issn = {0303-2647},
	url = {https://www.sciencedirect.com/science/article/pii/S0303264718302028},
	doi = {10.1016/j.biosystems.2018.07.007},
	abstract = {Theories of embodied cognition agree that the body plays some role in human cognition, but disagree on the precise nature of this role. While it is (together with the environment) fundamentally engrained in the so-called 4E (or multi-E) cognition stance, there also exists interpretations wherein the body is merely an input/output interface for cognitive processes that are entirely computational. In the present paper, we show that even if one takes such a strong computationalist position, the role of the body must be more than an interface to the world. To achieve human cognition, the computational mechanisms of a cognitive agent must be capable not only of appropriate reasoning over a given set of symbolic representations; they must in addition be capable of updating the representational framework itself (leading to the titular representational fluidity). We demonstrate this by considering the necessary properties that an artificial agent with these abilities need to possess. The core of the argument is that these updates must be falsifiable in the Popperian sense while simultaneously directing representational shifts in a direction that benefits the agent. We show that this is achieved by the progressive, bottom-up symbolic abstraction of low-level sensorimotor connections followed by top-down instantiation of testable perception-action hypotheses. We then discuss the fundamental limits of this representational updating capacity, concluding that only fully embodied learners exhibiting such a priori perception-action linkages are able to sufficiently ground spontaneously-generated symbolic representations and exhibit the full range of human cognitive capabilities. The present paper therefore has consequences both for the theoretical understanding of human cognition, and for the design of autonomous artificial agents.},
	language = {en},
	urldate = {2022-06-27},
	journal = {Biosystems},
	author = {Windridge, David and Thill, Serge},
	month = oct,
	year = {2018},
	keywords = {Computationalism, Embodied cognition, Representational frameworks, Representational updating},
	pages = {9--17},
	file = {ScienceDirect Full Text PDF:/Users/ron/Zotero/storage/J64CY8ND/Windridge and Thill - 2018 - Representational fluidity in embodied (artificial).pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/R2M6VGV3/S0303264718302028.html:text/html},
}

@article{thill_whats_2016,
	title = {What's on the {Inside} {Counts}: {A} {Grounded} {Account} of {Concept} {Acquisition} and {Development}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {What's on the {Inside} {Counts}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2016.00402},
	abstract = {Understanding the factors which affect the age of acquisition (AoA) of words and concepts is fundamental to understanding cognitive development more broadly. Traditionally, studies of AoA have taken two approaches, either exploring the effect of linguistic variables such as input frequency (e.g., Naigles and Hoff-Ginsberg, 1998) or the semantics of the underlying concept, such as concreteness or imageability (e.g., Bird et al., 2001). Embodied theories of cognition, meanwhile, assume that concepts, even relatively abstract ones, can be grounded in the embodied experience. While the focus of such discussions has been mainly on grounding in external modalities, more recently some have argued for the importance of interoceptive features, or grounding in complex modalities such as social interaction. In this paper, we argue for the integration and extension of these two strands of research. We demonstrate that the psycholinguistic factors traditionally considered to determine AoA are far from sufficient to account for the variability observed in AoA data. Given this gap, we propose groundability as a new conceptual tool that can measure the degree to which concepts are grounded both in external and, critically, internal modalities. We then present a mechanistic theory of conceptual representation that can account for groundability in addition to the existing variables argued to influence concept acquisition in both the developmental and embodied cognition literatures, and discuss its implications for future work in concept and cognitive development.},
	urldate = {2022-06-27},
	journal = {Frontiers in Psychology},
	author = {Thill, Serge and Twomey, Katherine E.},
	year = {2016},
	file = {Full Text PDF:/Users/ron/Zotero/storage/4JCAG82P/Thill and Twomey - 2016 - What's on the Inside Counts A Grounded Account of.pdf:application/pdf},
}

@article{thill_importance_2014,
	title = {On the importance of a rich embodiment in the grounding of concepts: perspectives from embodied cognitive science and computational linguistics},
	volume = {6},
	issn = {1756-8765},
	shorttitle = {On the importance of a rich embodiment in the grounding of concepts},
	doi = {10.1111/tops.12093},
	abstract = {The recent trend in cognitive robotics experiments on language learning, symbol grounding, and related issues necessarily entails a reduction of sensorimotor aspects from those provided by a human body to those that can be realized in machines, limiting robotic models of symbol grounding in this respect. Here, we argue that there is a need for modeling work in this domain to explicitly take into account the richer human embodiment even for concrete concepts that prima facie relate merely to simple actions, and illustrate this using distributional methods from computational linguistics which allow us to investigate grounding of concepts based on their actual usage. We also argue that these techniques have applications in theories and models of grounding, particularly in machine implementations thereof. Similarly, considering the grounding of concepts in human terms may be of benefit to future work in computational linguistics, in particular in going beyond "grounding" concepts in the textual modality alone. Overall, we highlight the overall potential for a mutually beneficial relationship between the two fields.},
	language = {eng},
	number = {3},
	journal = {Topics in Cognitive Science},
	author = {Thill, Serge and Padó, Sebastian and Ziemke, Tom},
	month = jul,
	year = {2014},
	pmid = {24948385},
	keywords = {Cognition, Humans, Cognitive Science, Concept Formation, Concept grounding, Distributional semantics, Embodiment, Language, Linguistics, Machine language understanding, Models, Theoretical, Robotics},
	pages = {545--558},
	file = {Full Text:/Users/ron/Zotero/storage/S4ZH2WF8/Thill et al. - 2014 - On the importance of a rich embodiment in the grou.pdf:application/pdf},
}

@techreport{hill_learning_2019,
	title = {Learning to {Make} {Analogies} by {Contrasting} {Abstract} {Relational} {Structure}},
	url = {http://arxiv.org/abs/1902.00120},
	abstract = {Analogical reasoning has been a principal focus of various waves of AI research. Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience. Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model. The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features. Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.},
	number = {arXiv:1902.00120},
	urldate = {2022-06-27},
	institution = {arXiv},
	author = {Hill, Felix and Santoro, Adam and Barrett, David G. T. and Morcos, Ari S. and Lillicrap, Timothy},
	month = jan,
	year = {2019},
	note = {arXiv:1902.00120 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/FNEW3ZAV/Hill et al. - 2019 - Learning to Make Analogies by Contrasting Abstract.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/VWVSN6EH/1902.html:text/html},
}

@techreport{wong_leveraging_2022,
	title = {Leveraging {Language} to {Learn} {Program} {Abstractions} and {Search} {Heuristics}},
	url = {http://arxiv.org/abs/2106.11053},
	abstract = {Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, and generalizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains -- string editing, image composition, and abstract reasoning about scenes -- even when no natural language hints are available at test time.},
	number = {arXiv:2106.11053},
	urldate = {2022-08-22},
	institution = {arXiv},
	author = {Wong, Catherine and Ellis, Kevin and Tenenbaum, Joshua B. and Andreas, Jacob},
	month = may,
	year = {2022},
	note = {arXiv:2106.11053 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/HGKZ74HV/Wong et al. - 2022 - Leveraging Language to Learn Program Abstractions .pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/CFGZHYKF/2106.html:text/html},
}


@techreport{bengio_gflownet_2022,
	title = {{GFlowNet} {Foundations}},
	url = {http://arxiv.org/abs/2111.09266},
	abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	number = {arXiv:2111.09266},
	urldate = {2022-08-22},
	institution = {arXiv},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	month = aug,
	year = {2022},
	note = {arXiv:2111.09266 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/L98TVLFT/Bengio et al. - 2022 - GFlowNet Foundations.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/TMVDNQJ6/2111.html:text/html},
}


@misc{malkin_trajectory_2022,
	title = {Trajectory balance: {Improved} credit assignment in {GFlowNets}},
	shorttitle = {Trajectory balance},
	url = {http://arxiv.org/abs/2201.13259},
	abstract = {Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets, flow matching and detailed balance, which are analogous to temporal difference learning, to be prone to inefficient credit propagation across long action sequences. We thus propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Malkin, Nikolay and Jain, Moksh and Bengio, Emmanuel and Sun, Chen and Bengio, Yoshua},
	month = oct,
	year = {2022},
	note = {arXiv:2201.13259 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/7MSINDRW/Malkin et al. - 2022 - Trajectory balance Improved credit assignment in .pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/IY683DI3/2201.html:text/html},
}

@article{morales_representation_nodate,
	title = {On the {Representation} and {Learning} of {Concepts}: {Programs}, {Types}, and {Bayes}},
	abstract = {This thesis develops computational models of cognition with a focus on concept representation and learning. We start with brief philosophical discourse accompanied by empirical ﬁndings and theories from developmental science. We review many formal foundations of computation as well as modern approaches to the problem of program induction — the learning of structure within those representations. We show our own research on program induction focused on its application for language bootstrapping. We then demonstrate our approach for augmenting a class of machine learning algorithms to enable domain-general learning by applying it to a program induction algorithm. Finally, we present our own computational account of concepts and cognition.},
	language = {en},
	author = {Morales, Lucas Eduardo},
	pages = {145},
	file = {Morales - On the Representation and Learning of Concepts Pr.pdf:/Users/ron/Zotero/storage/D4S42XJM/Morales - On the Representation and Learning of Concepts Pr.pdf:application/pdf},
}

@misc{acquaviva_communicating_2022,
	title = {Communicating {Natural} {Programs} to {Humans} and {Machines}},
	url = {http://arxiv.org/abs/2106.07824},
	abstract = {The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of {\textbackslash}emph\{language\}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the {\textbackslash}textit\{Language-complete ARC\}: a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88{\textbackslash}\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Acquaviva, Samuel and Pu, Yewen and Kryven, Marta and Sechopoulos, Theodoros and Wong, Catherine and Ecanow, Gabrielle E. and Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B.},
	month = oct,
	year = {2022},
	note = {arXiv:2106.07824 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/27HK5RZ2/Acquaviva et al. - 2022 - Communicating Natural Programs to Humans and Machi.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/9I8NF4E8/2106.html:text/html},
}

@misc{hewitt_learning_2020,
	title = {Learning to learn generative programs with {Memoised} {Wake}-{Sleep}},
	url = {http://arxiv.org/abs/2007.03132},
	abstract = {We study a class of neuro-symbolic generative models in which neural networks are used both for inference and as priors over symbolic, data-generating programs. As generative models, these programs capture compositional structures in a naturally explainable form. To tackle the challenge of performing program induction as an 'inner-loop' to learning, we propose the Memoised Wake-Sleep (MWS) algorithm, which extends Wake Sleep by explicitly storing and reusing the best programs discovered by the inference network throughout training. We use MWS to learn accurate, explainable models in three challenging domains: stroke-based character modelling, cellular automata, and few-shot learning in a novel dataset of real-world string concepts.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Hewitt, Luke B. and Le, Tuan Anh and Tenenbaum, Joshua B.},
	month = jul,
	year = {2020},
	note = {arXiv:2007.03132 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/LUEKB2Y4/Hewitt et al. - 2020 - Learning to learn generative programs with Memoise.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/Y9GL7QID/2007.html:text/html},
}

@misc{alford_neural-guided_2021,
	title = {Neural-guided, {Bidirectional} {Program} {Search} for {Abstraction} and {Reasoning}},
	url = {http://arxiv.org/abs/2110.11536},
	abstract = {One of the challenges facing artificial intelligence research today is designing systems capable of utilizing systematic reasoning to generalize to new tasks. The Abstraction and Reasoning Corpus (ARC) measures such a capability through a set of visual reasoning tasks. In this paper we report incremental progress on ARC and lay the foundations for two approaches to abstraction and reasoning not based in brute-force search. We first apply an existing program synthesis system called DreamCoder to create symbolic abstractions out of tasks solved so far, and show how it enables solving of progressively more challenging ARC tasks. Second, we design a reasoning algorithm motivated by the way humans approach ARC. Our algorithm constructs a search graph and reasons over this graph structure to discover task solutions. More specifically, we extend existing execution-guided program synthesis approaches with deductive reasoning based on function inverse semantics to enable a neural-guided bidirectional search algorithm. We demonstrate the effectiveness of the algorithm on three domains: ARC, 24-Game tasks, and a 'double-and-add' arithmetic puzzle.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Alford, Simon and Gandhi, Anshula and Rangamani, Akshay and Banburski, Andrzej and Wang, Tony and Dandekar, Sylee and Chin, John and Poggio, Tomaso and Chin, Peter},
	month = oct,
	year = {2021},
	note = {arXiv:2110.11536 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/XTSXRHR6/Alford et al. - 2021 - Neural-guided, Bidirectional Program Search for Ab.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/WZ7FVWZD/2110.html:text/html},
}

@article{ellis_algorithms_nodate,
	title = {Algorithms for {Learning} to {Induce} {Programs}},
	abstract = {The future of machine learning should have a knowledge representation that supports, at a minimum, several features: Expressivity, interpretability, the potential for reuse by both humans and machines, while also enabling sample-efﬁcient generalization. Here we argue that programs–i.e., source code–are a knowledge representation which can contribute to the project of capturing these elements of intelligence. This research direction however requires new program synthesis algorithms which can induce programs solving a range of AI tasks. This program induction challenge confronts two primary obstacles: the space of all programs is inﬁnite, so we need a strong inductive bias or prior to steer us toward the correct programs; and even if we have that prior, effectively searching through the vast combinatorial space of all programs is generally intractable. We introduce algorithms that learn to induce programs, with the goal of addressing these two primary obstacles. Focusing on case studies in vision, computational linguistics, and learning-to-learn, we develop an algorithmic toolkit for learning inductive biases over programs as well as learning to search for programs, drawing on probabilistic, neural, and symbolic methods. Together this toolkit suggests ways in which program induction can contribute to AI, and how we can use learning to improve program synthesis technologies.},
	language = {en},
	author = {Ellis, Kevin},
	pages = {224},
	file = {Ellis - Algorithms for Learning to Induce Programs.pdf:/Users/ron/Zotero/storage/BXXXACWT/Ellis - Algorithms for Learning to Induce Programs.pdf:application/pdf},
}

@misc{kumar_using_2022,
	title = {Using {Natural} {Language} and {Program} {Abstractions} to {Instill} {Human} {Inductive} {Biases} in {Machines}},
	url = {http://arxiv.org/abs/2205.11558},
	abstract = {Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Kumar, Sreejan and Correa, Carlos G. and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y. and Hawkins, Robert D. and Daw, Nathaniel D. and Cohen, Jonathan D. and Narasimhan, Karthik and Griffiths, Thomas L.},
	month = oct,
	year = {2022},
	note = {arXiv:2205.11558 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/EEFFSPWE/Kumar et al. - 2022 - Using Natural Language and Program Abstractions to.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/VACQBILB/2205.html:text/html},
}

@article{bowers_top-down_nodate,
	title = {Top-{Down} {Synthesis} {For} {Library} {Learning}},
	volume = {1},
	abstract = {This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder [Ellis et al. 2021]. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch’s scalability on corpora containing hundreds of complex programs, which are intractable with prior deductive approaches, and show empirically that it is robust to terminating the search procedure early—further allowing it to scale to challenging datasets by means of early stopping. CCS Concepts: • Software and its engineering → Automatic programming.},
	language = {en},
	author = {Bowers, Matthew and Olausson, Theo X and Wong, Catherine and Grand, Gabriel and Tenenbaum, Joshua B and Ellis, Kevin and Solar-Lezama, Armando},
	pages = {29},
	file = {Bowers et al. - Top-Down Synthesis For Library Learning.pdf:/Users/ron/Zotero/storage/ENNCMZAI/Bowers et al. - Top-Down Synthesis For Library Learning.pdf:application/pdf},
}

@misc{zhang_unifying_2022,
	title = {Unifying {Generative} {Models} with {GFlowNets}},
	url = {http://arxiv.org/abs/2209.02606},
	abstract = {There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. We present a short note on the connections between existing deep generative models and the GFlowNet framework, shedding light on their overlapping traits and providing a unifying viewpoint through the lens of learning with Markovian trajectories. This provides a means for unifying training and inference algorithms, and provides a route to construct an agglomeration of generative models.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Zhang, Dinghuai and Chen, Ricky T. Q. and Malkin, Nikolay and Bengio, Yoshua},
	month = sep,
	year = {2022},
	note = {arXiv:2209.02606 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/BYSAQEUS/Zhang et al. - 2022 - Unifying Generative Models with GFlowNets.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/KVY25EUG/2209.html:text/html},
}

@misc{zhang_generative_2022,
	title = {Generative {Flow} {Networks} for {Discrete} {Probabilistic} {Modeling}},
	url = {http://arxiv.org/abs/2202.01361},
	abstract = {We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN's effectiveness on various probabilistic modeling tasks. Code is publicly available at https://github.com/zdhNarsil/EB\_GFN.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Zhang, Dinghuai and Malkin, Nikolay and Liu, Zhen and Volokhova, Alexandra and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2022},
	note = {arXiv:2202.01361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/GN9QUBE5/Zhang et al. - 2022 - Generative Flow Networks for Discrete Probabilisti.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/59QWYK38/2202.html:text/html},
}

@misc{jain_multi-objective_2022,
	title = {Multi-{Objective} {GFlowNets}},
	url = {http://arxiv.org/abs/2210.12765},
	abstract = {In many applications of machine learning, like drug discovery and material design, the goal is to generate candidates that simultaneously maximize a set of objectives. As these objectives are often conflicting, there is no single candidate that simultaneously maximizes all objectives, but rather a set of Pareto-optimal candidates where one objective cannot be improved without worsening another. Moreover, in practice, these objectives are often under-specified, making the diversity of candidates a key consideration. The existing multi-objective optimization methods focus predominantly on covering the Pareto front, failing to capture diversity in the space of candidates. Motivated by the success of GFlowNets for generation of diverse candidates in a single objective setting, in this paper we consider Multi-Objective GFlowNets (MOGFNs). MOGFNs consist of a novel Conditional GFlowNet which models a family of single-objective sub-problems derived by decomposing the multi-objective optimization problem. Our work is the first to empirically demonstrate conditional GFlowNets. Through a series of experiments on synthetic and benchmark tasks, we empirically demonstrate that MOGFNs outperform existing methods in terms of Hypervolume, R2-distance and candidate diversity. We also demonstrate the effectiveness of MOGFNs over existing methods in active learning settings. Finally, we supplement our empirical results with a careful analysis of each component of MOGFNs.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Jain, Moksh and Raparthy, Sharath Chandra and Hernandez-Garcia, Alex and Rector-Brooks, Jarrid and Bengio, Yoshua and Miret, Santiago and Bengio, Emmanuel},
	month = oct,
	year = {2022},
	note = {arXiv:2210.12765 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/WNT8E293/Jain et al. - 2022 - Multi-Objective GFlowNets.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/MGJGN8SW/2210.html:text/html},
}

@misc{noauthor_language_nodate,
	title = {A language of thought for the mental representation of geometric shapes {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0010028522000639?token=C9C9681E195BACE41B1FF7B518CCDC4DE7CD52EF31FA2D0F70D136A46D453E8B08012F9DAF2BBC4DE8E9333E78632CA5&originRegion=eu-west-1&originCreation=20221207181703},
	language = {en},
	urldate = {2022-12-07},
	doi = {10.1016/j.cogpsych.2022.101527},
	file = {Snapshot:/Users/ron/Zotero/storage/MU57Y7VE/S0010028522000639.html:text/html;Submitted Version:/Users/ron/Zotero/storage/58A4WVT6/A language of thought for the mental representatio.pdf:application/pdf},
}

@article{sable-meyer_language_2022,
	title = {A language of thought for the mental representation of geometric shapes},
	volume = {139},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028522000639},
	doi = {10.1016/j.cogpsych.2022.101527},
	abstract = {In various cultures and at all spatial scales, humans produce a rich complexity of geometric shapes such as lines, circles or spirals. Here, we propose that humans possess a language of thought for geometric shapes that can produce line drawings as recursive combinations of a minimal set of geometric primitives. We present a programming language, similar to Logo, that combines discrete numbers and continuous integration to form higher-level structures based on repetition, concatenation and embedding, and we show that the simplest programs in this lan­ guage generate the fundamental geometric shapes observed in human cultures. On the perceptual side, we propose that shape perception in humans involves searching for the shortest program that correctly draws the image (program induction). A consequence of this framework is that the mental difficulty of remembering a shape should depend on its minimum description length (MDL) in the proposed language. In two experiments, we show that encoding and processing of geometric shapes is well predicted by MDL. Furthermore, our hypotheses predict additive laws for the psychological complexity of repeated, concatenated or embedded shapes, which we confirm experimentally.},
	language = {en},
	urldate = {2022-12-07},
	journal = {Cognitive Psychology},
	author = {Sablé-Meyer, Mathias and Ellis, Kevin and Tenenbaum, Josh and Dehaene, Stanislas},
	month = dec,
	year = {2022},
	pages = {101527},
	file = {Sablé-Meyer et al. - 2022 - A language of thought for the mental representatio.pdf:/Users/ron/Zotero/storage/CL5R2SDF/Sablé-Meyer et al. - 2022 - A language of thought for the mental representatio.pdf:application/pdf},
}

@article{yang_one_2022,
	title = {One model for the learning of language},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2021865119},
	doi = {10.1073/pnas.2021865119},
	abstract = {Significance
            It has long been hypothesized that language acquisition may be impossible without innate knowledge of the structures that occur in natural language. Here, we show that a domain general learning setup, originally developed in cognitive psychology to model rule learning, is able to acquire key pieces of natural language from relatively few examples of sentences. This develops a new approach to formalizing linguistic learning and highlights some features of language and language acquisition that may arise from general cognitive processes.
          , 
            
              A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,
              
                a
                n
              
              ,
              
                
                  
                    
                      
                        (
                        a
                        b
                        )
                      
                      n
                    
                  
                
              
              , and
              
                
                  
                    
                      
                        \{\vphantom{\}}
                        a
                        ,
                        b
                        \vphantom{\{}\}
                      
                      +
                    
                  
                
              
              ), context-free (e.g.,
              
                
                  
                    
                      a
                      n
                    
                    
                      b
                      n
                    
                    ,
                     
                    
                      a
                      n
                    
                    
                      b
                      
                        n
                        +
                        m
                      
                    
                  
                
              
              , and
              
                
                  
                    x
                    
                      x
                      R
                    
                  
                
              
              ), and context-sensitive (e.g.,
              
                
                  
                    
                      a
                      n
                    
                    
                      b
                      n
                    
                    
                      c
                      n
                    
                    ,
                     
                    
                      a
                      n
                    
                    
                      b
                      m
                    
                    
                      c
                      n
                    
                    
                      d
                      m
                    
                  
                
              
              , and
              xx
              ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
	language = {en},
	number = {5},
	urldate = {2022-12-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Yuan and Piantadosi, Steven T.},
	month = feb,
	year = {2022},
	pages = {e2021865119},
	file = {Yang and Piantadosi - 2022 - One model for the learning of language:/Users/ron/Zotero/storage/NYF4E3VI/Yang and Piantadosi - 2022 - One model for the learning of language:application/pdf},
}

@misc{ibarz_generalist_2022,
	title = {A {Generalist} {Neural} {Algorithmic} {Learner}},
	url = {http://arxiv.org/abs/2209.11142},
	abstract = {The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner -- a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20\% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csordás, Róbert and Dudzik, Andrew and Bošnjak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and Deac, Andreea and Bevilacqua, Beatrice and Ganin, Yaroslav and Blundell, Charles and Veličković, Petar},
	month = dec,
	year = {2022},
	note = {arXiv:2209.11142 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/FKK3VIMS/Ibarz et al. - 2022 - A Generalist Neural Algorithmic Learner.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/TXAYCS7K/2209.html:text/html},
}

@inproceedings{ellis_learning_2018,
	title = {Learning {Libraries} of {Subroutines} for {Neurally}– {Guided} {Bayesian} {Program} {Induction}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/7aa685b3b1dc1d6780bf36f7340078c9-Abstract.html},
	abstract = {Successful approaches to program induction require a hand-engineered
  domain-specific language (DSL), constraining the space of allowed
  programs and imparting prior knowledge of the domain.  We contribute
  a program induction algorithm that learns a DSL while
  jointly training a neural network to efficiently search for programs
  in the learned DSL.  We use our model to synthesize functions on lists,
  edit text, and solve symbolic regression problems, showing how the
  model learns a domain-specific library of program components for
  expressing solutions to problems in the domain.},
	urldate = {2022-12-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ellis, Kevin and Morales, Lucas and Sablé-Meyer, Mathias and Solar-Lezama, Armando and Tenenbaum, Josh},
	year = {2018},
	file = {Full Text PDF:/Users/ron/Zotero/storage/F9XGBE7H/Ellis et al. - 2018 - Learning Libraries of Subroutines for Neurally– Gu.pdf:application/pdf},
}

@misc{nye_learning_2019,
	title = {Learning to {Infer} {Program} {Sketches}},
	url = {http://arxiv.org/abs/1902.06349},
	abstract = {Our goal is to build systems which write code automatically from the kinds of specifications humans can most easily provide, such as examples and natural language instruction. The key idea of this work is that a flexible combination of pattern recognition and explicit reasoning can be used to solve these complex programming problems. We propose a method for dynamically integrating these types of information. Our novel intermediate representation and training algorithm allow a program synthesis system to learn, without direct supervision, when to rely on pattern recognition and when to perform symbolic search. Our model matches the memorization and generalization performance of neural synthesis and symbolic search, respectively, and achieves state-of-the-art performance on a dataset of simple English description-to-code programming problems.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Nye, Maxwell and Hewitt, Luke and Tenenbaum, Joshua and Solar-Lezama, Armando},
	month = jun,
	year = {2019},
	note = {arXiv:1902.06349 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/UU6X29X5/Nye et al. - 2019 - Learning to Infer Program Sketches.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/BI2Y8GYL/1902.html:text/html},
}

@misc{lake_building_2016,
	title = {Building {Machines} {That} {Learn} and {Think} {Like} {People}},
	url = {http://arxiv.org/abs/1604.00289},
	abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	month = nov,
	year = {2016},
	note = {arXiv:1604.00289 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, MUST READ, DONE},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/YZBPK4UV/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/JRJRYXI6/1604.html:text/html},
}

@article{evans_making_2021,
	title = {Making sense of sensory input},
	volume = {293},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370220301855},
	doi = {10.1016/j.artint.2020.103438},
	abstract = {This paper attempts to answer a central question in unsupervised learning: what does it mean to “make sense” of a sensory sequence? In our formalization, making sense involves constructing a symbolic causal theory that both explains the sensory sequence and also satisfies a set of unity conditions. The unity conditions insist that the constituents of the causal theory – objects, properties, and laws – must be integrated into a coherent whole. On our account, making sense of sensory input is a type of program synthesis, but it is unsupervised program synthesis. Our second contribution is a computer implementation, the Apperception Engine, that was designed to satisfy the above requirements. Our system is able to produce interpretable human-readable causal theories from very small amounts of data, because of the strong inductive bias provided by the unity conditions. A causal theory produced by our system is able to predict future sensor readings, as well as retrodict earlier readings, and impute (fill in the blanks of) missing sensory readings, in any combination. In fact, it is able to do all three tasks simultaneously. We tested the engine in a diverse variety of domains, including cellular automata, rhythms and simple nursery tunes, multi-modal binding problems, occlusion tasks, and sequence induction intelligence tests. In each domain, we test our engine's ability to predict future sensor values, retrodict earlier sensor values, and impute missing sensory data. The Apperception Engine performs well in all these domains, significantly out-performing neural net baselines. We note in particular that in the sequence induction intelligence tests, our system achieved human-level performance. This is notable because our system is not a bespoke system designed specifically to solve intelligence tests, but a general-purpose system that was designed to make sense of any sensory sequence.},
	language = {en},
	urldate = {2022-12-12},
	journal = {Artificial Intelligence},
	author = {Evans, Richard and Hernández-Orallo, José and Welbl, Johannes and Kohli, Pushmeet and Sergot, Marek},
	month = apr,
	year = {2021},
	keywords = {Learning dynamical models, Unsupervised program synthesis},
	pages = {103438},
	file = {Full Text:/Users/ron/Zotero/storage/D55RGCDU/Evans et al. - 2021 - Making sense of sensory input.pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/YPMIYGPZ/S0004370220301855.html:text/html},
}

@misc{kant_recent_2018,
	title = {Recent {Advances} in {Neural} {Program} {Synthesis}},
	url = {http://arxiv.org/abs/1802.02353},
	doi = {10.48550/arXiv.1802.02353},
	abstract = {In recent years, deep learning has made tremendous progress in a number of fields that were previously out of reach for artificial intelligence. The successes in these problems has led researchers to consider the possibilities for intelligent systems to tackle a problem that humans have only recently themselves considered: program synthesis. This challenge is unlike others such as object recognition and speech translation, since its abstract nature and demand for rigor make it difficult even for human minds to attempt. While it is still far from being solved or even competitive with most existing methods, neural program synthesis is a rapidly growing discipline which holds great promise if completely realized. In this paper, we start with exploring the problem statement and challenges of program synthesis. Then, we examine the fascinating evolution of program induction models, along with how they have succeeded, failed and been reimagined since. Finally, we conclude with a contrastive look at program synthesis and future research recommendations for the field.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Kant, Neel},
	month = feb,
	year = {2018},
	note = {arXiv:1802.02353 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/4C9X543I/Kant - 2018 - Recent Advances in Neural Program Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/W8KP7KJ8/1802.html:text/html},
}

@article{piantadosi_computational_2021,
	title = {The {Computational} {Origin} of {Representation}},
	volume = {31},
	issn = {0924-6495, 1572-8641},
	url = {http://link.springer.com/10.1007/s11023-020-09540-9},
	doi = {10.1007/s11023-020-09540-9},
	abstract = {Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations—whatever they are—must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantiﬁcation, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an “assembly language” for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems.},
	language = {en},
	number = {1},
	urldate = {2022-12-12},
	journal = {Minds and Machines},
	author = {Piantadosi, Steven T.},
	month = mar,
	year = {2021},
	pages = {1--58},
	file = {Piantadosi - 2021 - The Computational Origin of Representation.pdf:/Users/ron/Zotero/storage/CYEV29MV/Piantadosi - 2021 - The Computational Origin of Representation.pdf:application/pdf},
}

@article{allamanis_learning_2018,
	title = {{LEARNING} {TO} {REPRESENT} {PROGRAMS} {WITH} {GRAPHS}},
	abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code’s known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.},
	language = {en},
	author = {Allamanis, Miltiadis and Khademi, Mahmoud and Brockschmidt, Marc},
	year = {2018},
	file = {Allamanis et al. - 2018 - LEARNING TO REPRESENT PROGRAMS WITH GRAPHS.pdf:/Users/ron/Zotero/storage/T68DFH4N/Allamanis et al. - 2018 - LEARNING TO REPRESENT PROGRAMS WITH GRAPHS.pdf:application/pdf},
}

@misc{velickovic_clrs_2022,
	title = {The {CLRS} {Algorithmic} {Reasoning} {Benchmark}},
	url = {http://arxiv.org/abs/2205.15659},
	abstract = {Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. Several important works have investigated whether neural networks can effectively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorithmic data to evaluate specific hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards unified evaluation, we propose the CLRS Algorithmic Reasoning Benchmark, covering classical algorithms from the Introduction to Algorithms textbook. Our benchmark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Veličković, Petar and Badia, Adrià Puigdomènech and Budden, David and Pascanu, Razvan and Banino, Andrea and Dashevskiy, Misha and Hadsell, Raia and Blundell, Charles},
	month = jun,
	year = {2022},
	note = {arXiv:2205.15659 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/FA8F8CC8/Veličković et al. - 2022 - The CLRS Algorithmic Reasoning Benchmark.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/X9YCWTLB/2205.html:text/html},
}

@article{zhang_connecting_2020,
	title = {Connecting concepts in the brain by mapping cortical representations of semantic relations},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-15804-w},
	doi = {10.1038/s41467-020-15804-w},
	abstract = {In the brain, the semantic system is thought to store concepts. However, little is known about how it connects different concepts and infers semantic relations. To address this question, we collected hours of functional magnetic resonance imaging data from human subjects listening to natural stories. We developed a predictive model of the voxel-wise response and further applied it to thousands of new words. Our results suggest that both semantic categories and relations are represented by spatially overlapping cortical patterns, instead of anatomically segregated regions. Semantic relations that reflect conceptual progression from concreteness to abstractness are represented by cortical patterns of activation in the default mode network and deactivation in the frontoparietal attention network. We conclude that the human brain uses distributed networks to encode not only concepts but also relationships between concepts. In particular, the default mode network plays a central role in semantic processing for abstraction of concepts.},
	language = {en},
	number = {1},
	urldate = {2022-12-22},
	journal = {Nature Communications},
	author = {Zhang, Yizhen and Han, Kuan and Worth, Robert and Liu, Zhongming},
	month = apr,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Engineering, Neuroscience},
	pages = {1877},
	file = {Full Text PDF:/Users/ron/Zotero/storage/3WKPX2X3/Zhang et al. - 2020 - Connecting concepts in the brain by mapping cortic.pdf:application/pdf},
}

@article{piantadosi_logical_nodate,
	title = {The logical primitives of thought: {Empirical} foundations for compositional cognitive models.},
	volume = {123},
	issn = {1939-1471},
	shorttitle = {The logical primitives of thought},
	url = {https://psycnet.apa.org/fulltext/2016-18127-001.pdf},
	doi = {10.1037/a0039980},
	number = {4},
	urldate = {2022-12-25},
	journal = {Psychological Review},
	author = {Piantadosi, Steven T. and Tenenbaum, Joshua B. and Goodman, Noah D.},
	note = {Publisher: US: American Psychological Association},
	pages = {392},
	file = {Snapshot:/Users/ron/Zotero/storage/3A6BFDZZ/2016-18127-001.html:text/html},
}

@misc{colas_autotelic_2022,
	title = {Autotelic {Agents} with {Intrinsically} {Motivated} {Goal}-{Conditioned} {Reinforcement} {Learning}: a {Short} {Survey}},
	shorttitle = {Autotelic {Agents} with {Intrinsically} {Motivated} {Goal}-{Conditioned} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2012.09830},
	abstract = {Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by \$autotelic\$ \$agents\$: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: \$developmental\$ \$reinforcement\$ \$learning\$. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem -- the \$intrinsically\$ \$motivated\$ \$acquisition\$ \$of\$ \$open\$-\$ended\$ \$repertoires\$ \$of\$ \$skills\$. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Colas, Cédric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	month = jul,
	year = {2022},
	note = {arXiv:2012.09830 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/Y5J5KQZC/Colas et al. - 2022 - Autotelic Agents with Intrinsically Motivated Goal.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/IP6HTU2U/2012.html:text/html},
}

@article{goyal_inductive_2022,
	title = {Inductive biases for deep learning of higher-level cognition},
	volume = {478},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068},
	doi = {10.1098/rspa.2021.0068},
	abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
	number = {2266},
	urldate = {2023-02-09},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Goyal, Anirudh and Bengio, Yoshua},
	month = oct,
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {deep learning, causality, reasoning, system 2, systematic and out-of-distribution generalization},
	pages = {20210068},
	file = {Full Text PDF:/Users/ron/Zotero/storage/N28TYQQK/Goyal and Bengio - 2022 - Inductive biases for deep learning of higher-level.pdf:application/pdf},
}

@misc{nye_improving_2021,
	title = {Improving {Coherence} and {Consistency} in {Neural} {Sequence} {Models} with {Dual}-{System}, {Neuro}-{Symbolic} {Reasoning}},
	url = {http://arxiv.org/abs/2107.02794},
	abstract = {Human reasoning can often be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models -- which have been increasingly successful at performing complex, structured tasks -- exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B. and Lake, Brenden M.},
	month = dec,
	year = {2021},
	note = {arXiv:2107.02794 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/8L35HG43/Nye et al. - 2021 - Improving Coherence and Consistency in Neural Sequ.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/E86K25FW/2107.html:text/html},
}

@misc{yildirim_3d_2023,
	title = {{3D} {Shape} {Perception} {Integrates} {Intuitive} {Physics} and {Analysis}-by-{Synthesis}},
	url = {http://arxiv.org/abs/2301.03711},
	abstract = {Many surface cues support three-dimensional shape perception, but people can sometimes still see shape when these features are missing -- in extreme cases, even when an object is completely occluded, as when covered with a draped cloth. We propose a framework for 3D shape perception that explains perception in both typical and atypical cases as analysis-by-synthesis, or inference in a generative model of image formation: the model integrates intuitive physics to explain how shape can be inferred from deformations it causes to other objects, as in cloth-draping. Behavioral and computational studies comparing this account with several alternatives show that it best matches human observers in both accuracy and response times, and is the only model that correlates significantly with human performance on difficult discriminations. Our results suggest that bottom-up deep neural network models are not fully adequate accounts of human shape perception, and point to how machine vision systems might achieve more human-like robustness.},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Yildirim, Ilker and Siegel, Max H. and Soltani, Amir A. and Chaudhari, Shraman Ray and Tenenbaum, Joshua B.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03711 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/Z8NZDUDW/Yildirim et al. - 2023 - 3D Shape Perception Integrates Intuitive Physics a.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/57PDZQBS/2301.html:text/html},
}

@misc{hu_gflownet-em_2023,
	title = {{GFlowNet}-{EM} for learning compositional latent variable models},
	url = {http://arxiv.org/abs/2302.06576},
	abstract = {Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on images using discrete variational autoencoders (VAEs) without conditional independence enforced in the encoder.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Hu, Edward and Malkin, Nikolay and Jain, Moksh and Everett, Katie and Graikos, Alexandros and Bengio, Yoshua},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06576 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/TWUJFXJR/Hu et al. - 2023 - GFlowNet-EM for learning compositional latent vari.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/K4QP3UTB/2302.html:text/html},
}

@article{pastra_minimalist_2012,
	title = {The minimalist grammar of action},
	volume = {367},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3223786/},
	doi = {10.1098/rstb.2011.0123},
	abstract = {Language and action have been found to share a common neural basis and in particular a common ‘syntax’, an analogous hierarchical and compositional organization. While language structure analysis has led to the formulation of different ...},
	language = {en},
	number = {1585},
	urldate = {2023-03-14},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Pastra, Katerina and Aloimonos, Yiannis},
	month = jan,
	year = {2012},
	pmid = {22106430},
	note = {Publisher: The Royal Society},
	pages = {103},
	file = {Snapshot:/Users/ron/Zotero/storage/Q2MB57PB/PMC3223786.html:text/html},
}

@article{birle_cognitive_2021,
	title = {Cognitive function: holarchy or holacracy?},
	volume = {42},
	issn = {1590-1874, 1590-3478},
	shorttitle = {Cognitive function},
	url = {https://link.springer.com/10.1007/s10072-020-04737-3},
	doi = {10.1007/s10072-020-04737-3},
	abstract = {Cognition is the most complex function of the brain. When exploring the inner workings of cognitive processes, it is crucial to understand the complexity of the brain’s dynamics. This paper aims to describe the integrated framework of the cognitive function, seen as the result of organization and interactions between several systems and subsystems. We briefly describe several organizational concepts, spanning from the reductionist hierarchical approach, up to the more dynamic theory of open complex systems. The homeostatic regulation of the mechanisms responsible for cognitive processes is showcased as a dynamic interplay between several anticorrelated mechanisms, which can be found at every level of the brain’s organization, from molecular and cellular level to large-scale networks (e.g., excitation-inhibition, long-term plasticity-long-term depression, synchronizationdesynchronization, segregation-integration, order-chaos). We support the hypothesis that cognitive function is the consequence of multiple network interactions, integrating intricate relationships between several systems, in addition to neural circuits.},
	language = {en},
	number = {1},
	urldate = {2023-03-14},
	journal = {Neurological Sciences},
	author = {Birle, Codruta and Slavoaca, Dana and Balea, Maria and Livint Popa, Livia and Muresanu, Ioana and Stefanescu, Emanuel and Vacaras, Vitalie and Dina, Constantin and Strilciuc, Stefan and Popescu, Bogdan Ovidiu and Muresanu, Dafin F.},
	month = jan,
	year = {2021},
	pages = {89--99},
	file = {Birle et al. - 2021 - Cognitive function holarchy or holacracy.pdf:/Users/ron/Zotero/storage/RBCJ66JW/Birle et al. - 2021 - Cognitive function holarchy or holacracy.pdf:application/pdf},
}

@article{ha_world_2018,
	title = {World {Models}},
	url = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We consider the benefits of dream mechanisms – that is, the ability to simulate new experiences based on past ones – in a machine learning context. Specifically, we are interested in learning for artificial agents that act in the world, and operationalize ‘‘dreaming’’ as a mechanism by which such an agent can use its own model of the learning environment to generate new hypotheses and training data.},
	language = {en},
	urldate = {2023-03-14},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
	note = {arXiv:1803.10122 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ha and Schmidhuber - 2018 - World Models.pdf:/Users/ron/Zotero/storage/GKC9JENL/Ha and Schmidhuber - 2018 - World Models.pdf:application/pdf},
}

@article{piccinini_computation_2008,
	title = {Computation without {Representation}},
	volume = {137},
	issn = {1573-0883},
	url = {https://doi.org/10.1007/s11098-005-5385-4},
	doi = {10.1007/s11098-005-5385-4},
	abstract = {The received view is that computational states are individuated at least in part by their semantic properties. I offer an alternative, according to which computational states are individuated by their functional properties. Functional properties are specified by a mechanistic explanation without appealing to any semantic properties. The primary purpose of this paper is to formulate the alternative view of computational individuation, point out that it supports a robust notion of computational explanation, and defend it on the grounds of how computational states are individuated within computability theory and computer science. A secondary purpose is to show that existing arguments for the semantic view are defective.},
	language = {en},
	number = {2},
	urldate = {2023-03-14},
	journal = {Philosophical Studies},
	author = {Piccinini, Gualtiero},
	month = jan,
	year = {2008},
	keywords = {Computational State, Mechanistic Explanation, Semantic Property, Semantic View, Syntactic Property},
	pages = {205--241},
}

@article{bach_seven_nodate,
	title = {Seven {Principles} of {Synthetic} {Intelligence}},
	abstract = {Understanding why the original project of Artificial Intelligence is widely regarded as a failure and has been abandoned even by most of contemporary AI research itself may prove crucial to achieving synthetic intelligence. Here, we take a brief look at some principles that we might consider to be lessons from the past five decades of AI. The author’s own AI architecture –MicroPsi – attempts to contribute to that discussion.},
	language = {en},
	author = {Bach, Joscha},
	file = {Bach - Seven Principles of Synthetic Intelligence.pdf:/Users/ron/Zotero/storage/9GHW79HA/Bach - Seven Principles of Synthetic Intelligence.pdf:application/pdf},
}

@misc{liu_cones_2023,
	title = {Cones: {Concept} {Neurons} in {Diffusion} {Models} for {Customized} {Generation}},
	shorttitle = {Cones},
	url = {http://arxiv.org/abs/2303.05125},
	abstract = {Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. A few steps of further fine-tuning can enhance the multi-concept capability, which may be the first to manage to generate up to four different subjects in a single image. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 values of the parameters, which reduces storage consumption by 90{\textbackslash}\% compared with previous subject-driven generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Liu, Zhiheng and Feng, Ruili and Zhu, Kai and Zhang, Yifei and Zheng, Kecheng and Liu, Yu and Zhao, Deli and Zhou, Jingren and Cao, Yang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.05125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/4QJBZYT4/Liu et al. - 2023 - Cones Concept Neurons in Diffusion Models for Cus.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/92Y3EHLA/2303.html:text/html},
}

@article{schlegel_comparison_2022,
	title = {A comparison of vector symbolic architectures},
	volume = {55},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-021-10110-3},
	doi = {10.1007/s10462-021-10110-3},
	abstract = {Vector Symbolic Architectures combine a high-dimensional vector space with a set of carefully designed operators in order to perform symbolic computations with large numerical vectors. Major goals are the exploitation of their representational power and ability to deal with fuzziness and ambiguity. Over the past years, several VSA implementations have been proposed. The available implementations differ in the underlying vector space and the particular implementations of the VSA operators. This paper provides an overview of eleven available VSA implementations and discusses their commonalities and differences in the underlying vector space and operators. We create a taxonomy of available binding operations and show an important ramification for non self-inverse binding operations using an example from analogical reasoning. A main contribution is the experimental comparison of the available implementations in order to evaluate (1) the capacity of bundles, (2) the approximation quality of non-exact unbinding operations, (3) the influence of combining binding and bundling operations on the query answering performance, and (4) the performance on two example applications: visual place- and language-recognition. We expect this comparison and systematization to be relevant for development of VSAs, and to support the selection of an appropriate VSA for a particular task. The implementations are available.},
	language = {en},
	number = {6},
	urldate = {2023-03-14},
	journal = {Artificial Intelligence Review},
	author = {Schlegel, Kenny and Neubert, Peer and Protzel, Peter},
	month = aug,
	year = {2022},
	keywords = {High-dimensional computing, Hyperdimensional computing, Hypervectors, Vector symbolic architectures},
	pages = {4523--4555},
	file = {Full Text PDF:/Users/ron/Zotero/storage/8GFIAAHZ/Schlegel et al. - 2022 - A comparison of vector symbolic architectures.pdf:application/pdf},
}

@article{ha_collective_2022,
	title = {Collective intelligence for deep learning: {A} survey of recent developments},
	volume = {1},
	issn = {2633-9137},
	shorttitle = {Collective intelligence for deep learning},
	url = {https://doi.org/10.1177/26339137221114874},
	doi = {10.1177/26339137221114874},
	abstract = {In the past decade, we have witnessed the rise of deep learning to dominate the field of artificial intelligence. Advances in artificial neural networks alongside corresponding advances in hardware accelerators with large memory capacity, together with the availability of large datasets enabled practitioners to train and deploy sophisticated neural network models that achieve state-of-the-art performance on tasks across several fields spanning computer vision, natural language processing, and reinforcement learning. However, as these neural networks become bigger, more complex, and more widely used, fundamental problems with current deep learning models become more apparent. State-of-the-art deep learning models are known to suffer from issues that range from poor robustness, inability to adapt to novel task settings, to requiring rigid and inflexible configuration assumptions. Collective behavior, commonly observed in nature, tends to produce systems that are robust, adaptable, and have less rigid assumptions about the environment configuration. Collective intelligence, as a field, studies the group intelligence that emerges from the interactions of many individuals. Within this field, ideas such as self-organization, emergent behavior, swarm optimization, and cellular automata were developed to model and explain complex systems. It is therefore natural to see these ideas incorporated into newer deep learning methods. In this review, we will provide a historical context of neural network research?s involvement with complex systems, and highlight several active areas in modern deep learning research that incorporate the principles of collective intelligence to advance its capabilities. We hope this review can serve as a bridge between the complex systems and deep learning communities.},
	number = {1},
	urldate = {2023-03-14},
	journal = {Collective Intelligence},
	author = {Ha, David and Tang, Yujin},
	month = aug,
	year = {2022},
	note = {Publisher: SAGE Publications},
	pages = {26339137221114874},
	file = {SAGE PDF Full Text:/Users/ron/Zotero/storage/X898SSIT/Ha and Tang - 2022 - Collective intelligence for deep learning A surve.pdf:application/pdf},
}

@article{ball_dance_2021,
	title = {The {Dance} of the {Semantic} {Phoenix}: {Autopoietic} {Systems} of {Meaning} in {Finnegans} {Wake}},
	volume = {45},
	issn = {1086-329X},
	shorttitle = {The {Dance} of the {Semantic} {Phoenix}},
	url = {https://muse.jhu.edu/article/796836},
	doi = {10.1353/phl.2021.0011},
	abstract = {In this essay, I propose an interpretation of James Joyce’s Finnegans Wake that is informed by cybernetics and systems theory to show that the novel functions as a model of autopoiesis. Drawing in particular from the work of Niklas Luhmann, I argue that the novel and readers’ engagement with it demonstrates how we individually and socially produce meaning from the overwhelming complexity of the world. Contrary to those who would dismiss it as meaningless, I show that Finnegans Wake is a complex system that infinitely generates meaning through self-reference and networks of association.},
	language = {en},
	number = {1},
	urldate = {2023-03-14},
	journal = {Philosophy and Literature},
	author = {Ball, Andrew J.},
	year = {2021},
	pages = {172--184},
	file = {Ball - 2021 - The Dance of the Semantic Phoenix Autopoietic Sys.pdf:/Users/ron/Zotero/storage/4VSXDQT8/Ball - 2021 - The Dance of the Semantic Phoenix Autopoietic Sys.pdf:application/pdf},
}

@misc{bouizegarene_narrative_2020,
	title = {Narrative as active inference},
	url = {https://psyarxiv.com/47ub6/},
	doi = {10.31234/osf.io/47ub6},
	abstract = {The ubiquity and importance of narratives in human adaptation has been recognized by many scholars. Research has identified several functions of narratives that are conducive to individuals’ well-being and adaptation as well as to coordinated social practices and enculturation. In this paper, we characterize the social and cognitive functions of narratives in terms of the framework of active inference. Active inference depicts the fundamental tendency of living organisms to adapt by creating, updating, and maintaining inferences about their environment. We review the literature on the functions of narratives in identity, event segmentation, episodic memory, future projection, storytelling practices, and enculturation. We then re-cast these functions of narratives in terms of active inference, outlining a parsimonious model that can guide future developments in narrative theory, research, and clinical applications.},
	language = {en-us},
	urldate = {2023-03-14},
	publisher = {PsyArXiv},
	author = {Bouizegarene, Nabil and Ramstead, Maxwell and Constant, Axel and Friston, Karl and Kirmayer, Laurence},
	month = jul,
	year = {2020},
	keywords = {Memory, Active inference, Cultural affordances, Cultural Psychology, Future projection, Narratives, Social and Behavioral Sciences, Social and Personality Psychology},
	file = {Full Text PDF:/Users/ron/Zotero/storage/JHA49DAW/Bouizegarene et al. - 2020 - Narrative as active inference.pdf:application/pdf},
}

@article{vernon_embodied_2015,
	title = {Embodied cognition and circular causality: on the role of constitutive autonomy in the reciprocal coupling of perception and action},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Embodied cognition and circular causality},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01660},
	abstract = {The reciprocal coupling of perception and action in cognitive agents has been firmly established: perceptions guide action but so too do actions influence what is perceived. While much has been said on the implications of this for the agent's external behavior, less attention has been paid to what it means for the internal bodily mechanisms which underpin cognitive behavior. In this article, we wish to redress this by reasserting that the relationship between cognition, perception, and action involves a constitutive element as well as a behavioral element, emphasizing that the reciprocal link between perception and action in cognition merits a renewed focus on the system dynamics inherent in constitutive biological autonomy. Our argument centers on the idea that cognition, perception, and action are all dependent on processes focussed primarily on the maintenance of the agent's autonomy. These processes have an inherently circular nature—self-organizing, self-producing, and self-maintaining—and our goal is to explore these processes and suggest how they can explain the reciprocity of perception and action. Specifically, we argue that the reciprocal coupling is founded primarily on their endogenous roles in the constitutive autonomy of the agent and an associated circular causality of global and local processes of self-regulation, rather than being a mutual sensory-motor contingency that derives from exogenous behavior. Furthermore, the coupling occurs first and foremost via the internal milieu realized by the agent's organismic embodiment. Finally, we consider how homeostasis and the related concept of allostasis contribute to this circular self-regulation.},
	urldate = {2023-03-14},
	journal = {Frontiers in Psychology},
	author = {Vernon, David and Lowe, Robert and Thill, Serge and Ziemke, Tom},
	year = {2015},
	file = {Full Text PDF:/Users/ron/Zotero/storage/IQJ8PCHE/Vernon et al. - 2015 - Embodied cognition and circular causality on the .pdf:application/pdf},
}

@misc{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv:1806.01261 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/4HCM8TBT/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/F89M7858/1806.html:text/html},
}

@article{schwartz_behavioral_2017,
	title = {Behavioral and neural constraints on hierarchical representations},
	volume = {17},
	issn = {1534-7362},
	doi = {10.1167/17.3.13},
	abstract = {Central to behavior and cognition is the way that sensory stimuli are represented in neural systems. The distributions over such stimuli enjoy rich structure; however, how the brain captures and exploits these regularities is unclear. Here, we consider different sources of perhaps the most prevalent form of structure, namely hierarchies, in one of its most prevalent cases, namely the representation of images. We review experimental approaches across a range of subfields, spanning inference, memory recall, and visual adaptation, to investigate how these constrain hierarchical representations. We also discuss progress in building hierarchical models of the representation of images-this has the potential to clarify how the structure of the world is reflected in biological systems. We suggest there is a need for a closer embedding of recent advances in machine learning and computer vision into the design and interpretation of experiments, notably by utilizing the understanding of the structure of natural scenes and through the creation of hierarchically structured synthetic stimuli.},
	language = {eng},
	number = {3},
	journal = {Journal of Vision},
	author = {Schwartz, Odelia and Giraldo, Luis Gonzalo Sanchez},
	month = mar,
	year = {2017},
	pmid = {28319238},
	keywords = {Cognition, Humans, Animals, Memory, Short-Term, Motor Neurons, Visual Perception},
	pages = {13},
}

@article{windridge_utility_2021,
	title = {On the utility of dreaming: {A} general model for how learning in artificial agents can benefit from data hallucination},
	volume = {29},
	issn = {1059-7123},
	shorttitle = {On the utility of dreaming},
	url = {https://doi.org/10.1177/1059712319896489},
	doi = {10.1177/1059712319896489},
	abstract = {We consider the benefits of dream mechanisms ? that is, the ability to simulate new experiences based on past ones ? in a machine learning context. Specifically, we are interested in learning for artificial agents that act in the world, and operationalize ?dreaming? as a mechanism by which such an agent can use its own model of the learning environment to generate new hypotheses and training data.We first show that it is not necessarily a given that such a data-hallucination process is useful, since it can easily lead to a training set dominated by spurious imagined data until an ill-defined convergence point is reached. We then analyse a notably successful implementation of a machine learning-based dreaming mechanism by Ha and Schmidhuber (Ha, D., \& Schmidhuber, J. (2018). World models. arXiv e-prints, arXiv:1803.10122). On that basis, we then develop a general framework by which an agent can generate simulated data to learn from in a manner that is beneficial to the agent. This, we argue, then forms a general method for an operationalized dream-like mechanism.We finish by demonstrating the general conditions under which such mechanisms can be useful in machine learning, wherein the implicit simulator inference and extrapolation involved in dreaming act without reinforcing inference error even when inference is incomplete.},
	language = {en},
	number = {3},
	urldate = {2023-03-14},
	journal = {Adaptive Behavior},
	author = {Windridge, David and Svensson, Henrik and Thill, Serge},
	month = jun,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {267--280},
	file = {SAGE PDF Full Text:/Users/ron/Zotero/storage/VG9MU887/Windridge et al. - 2021 - On the utility of dreaming A general model for ho.pdf:application/pdf},
}

@article{hoel_overfitted_2021,
	title = {The overfitted brain: {Dreams} evolved to assist generalization},
	volume = {2},
	issn = {2666-3899},
	shorttitle = {The overfitted brain},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8134940/},
	doi = {10.1016/j.patter.2021.100244},
	abstract = {Understanding of the evolved biological function of sleep has advanced considerably in the past decade. However, no equivalent understanding of dreams has emerged. Contemporary neuroscientific theories often view dreams as epiphenomena, and many of the proposals for their biological function are contradicted by the phenomenology of dreams themselves. Now, the recent advent of deep neural networks (DNNs) has finally provided the novel conceptual framework within which to understand the evolved function of dreams. Notably, all DNNs face the issue of overfitting as they learn, which is when performance on one dataset increases but the network's performance fails to generalize (often measured by the divergence of performance on training versus testing datasets). This ubiquitous problem in DNNs is often solved by modelers via “noise injections” in the form of noisy or corrupted inputs. The goal of this paper is to argue that the brain faces a similar challenge of overfitting and that nightly dreams evolved to combat the brain's overfitting during its daily learning. That is, dreams are a biological mechanism for increasing generalizability via the creation of corrupted sensory inputs from stochastic activity across the hierarchy of neural structures. Sleep loss, specifically dream loss, leads to an overfitted brain that can still memorize and learn but fails to generalize appropriately. Herein this ”overfitted brain hypothesis” is explicitly developed and then compared and contrasted with existing contemporary neuroscientific theories of dreams. Existing evidence for the hypothesis is surveyed within both neuroscience and deep learning, and a set of testable predictions is put forward that can be pursued both in vivo and in silico., Dreaming remains a mystery to neuroscience. While various hypotheses of why brains evolved nightly dreaming have been put forward, many of these are contradicted by the sparse, hallucinatory, and narrative nature of dreams, a nature that seems to lack any particular function. Recently, research on artificial neural networks has shown that during learning, such networks face a ubiquitous problem: that of overfitting to a particular dataset, which leads to failures in generalization and therefore performance on novel datasets. Notably, the techniques that researchers employ to rescue overfitted artificial neural networks generally involve sampling from an out-of-distribution or randomized dataset. The overfitted brain hypothesis is that the brains of organisms similarly face the challenge of fitting too well to their daily distribution of stimuli, causing overfitting and poor generalization. By hallucinating out-of-distribution sensory stimulation every night, the brain is able to rescue the generalizability of its perceptual and cognitive abilities and increase task performance., Why do we dream? While it is known that dreams must be important for learning, it is unknown precisely how or why this is. This paper explores the many hypotheses around why organisms dream, eventually proposing that the evolved purpose of dreams can be identified from research on artificial neural networks. Specifically, the overfitted brain hypothesis claims that in our daily lives the brain learns its tasks too well, and dreams are necessary to stop this “overfitting.”},
	number = {5},
	urldate = {2023-03-15},
	journal = {Patterns},
	author = {Hoel, Erik},
	month = may,
	year = {2021},
	pmid = {34036289},
	pmcid = {PMC8134940},
	pages = {100244},
	file = {PubMed Central Full Text PDF:/Users/ron/Zotero/storage/8J92234K/Hoel - 2021 - The overfitted brain Dreams evolved to assist gen.pdf:application/pdf},
}

@misc{johnston_construction_2023,
	title = {The {Construction} of {Reality} in an {AI}: {A} {Review}},
	shorttitle = {The {Construction} of {Reality} in an {AI}},
	url = {http://arxiv.org/abs/2302.05448},
	abstract = {AI constructivism as inspired by Jean Piaget, described and surveyed by Frank Guerin, and representatively implemented by Gary Drescher seeks to create algorithms and knowledge structures that enable agents to acquire, maintain, and apply a deep understanding of the environment through sensorimotor interactions. This paper aims to increase awareness of constructivist AI implementations to encourage greater progress toward enabling lifelong learning by machines. It builds on Guerin's 2008 "Learning Like a Baby: A Survey of AI approaches." After briefly recapitulating that survey, it summarizes subsequent progress by the Guerin referents, numerous works not covered by Guerin (or found in other surveys), and relevant efforts in related areas. The focus is on knowledge representations and learning algorithms that have been used in practice viewed through lenses of Piaget's schemas, adaptation processes, and staged development. The paper concludes with a preview of a simple framework for constructive AI being developed by the author that parses concepts from sensory input and stores them in a semantic memory network linked to episodic data. Extensive references are provided.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Johnston, Jeffrey W.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05448 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/GGGGX6CK/Johnston - 2023 - The Construction of Reality in an AI A Review.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/PT5LTE3E/2302.html:text/html},
}

@article{levin_computational_2019,
	title = {The {Computational} {Boundary} of a “{Self}”: {Developmental} {Bioelectricity} {Drives} {Multicellularity} and {Scale}-{Free} {Cognition}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {The {Computational} {Boundary} of a “{Self}”},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.02688/full},
	doi = {10.3389/fpsyg.2019.02688},
	abstract = {All epistemic agents physically consist of parts that must somehow comprise an integrated cognitive self. Biological individuals consist of subunits (organs, cells, and molecular networks) that are themselves complex and competent in their own native contexts. How do coherent biological Individuals result from the activity of smaller sub-agents? To understand the evolution and function of metazoan creatures’ bodies and minds, it is essential to conceptually explore the origin of multicellularity and the scaling of the basal cognition of individual cells into a coherent larger organism. In this article, I synthesize ideas in cognitive science, evolutionary biology, and developmental physiology toward a hypothesis about the origin of Individuality: “Scale-Free Cognition.” I propose a fundamental definition of an Individual based on the ability to pursue goals at an appropriate level of scale and organization and suggest a formalism for defining and comparing the cognitive capacities of highly diverse types of agents. Any Self is demarcated by a computational surface – the spatio-temporal boundary of events that it can measure, model, and try to affect. This surface sets a functional boundary a cognitive “light cone” which defines the scale and limits of its cognition. I hypothesize that higher level goal-directed activity and agency, resulting in larger cognitive boundaries, evolve from the primal homeostatic drive of living things to reduce stress – the difference between current conditions and life-optimal conditions. The mechanisms of developmental bioelectricity - the ability of all cells to form electrical networks that process information suggest a plausible set of gradual evolutionary steps that naturally lead from physiological homeostasis in single cells to memory, prediction, and ultimately complex cognitive agents, via scale-up of the basic drive of infotaxis. Recent data on the molecular mechanisms of pre-neural bioelectricity suggest a model of how increasingly sophisticated cognitive functions emerge smoothly from cell-cell communication used to guide embryogenesis and regeneration. This set of hypotheses provides a novel perspective on numerous phenomena, such as cancer, and makes several unique, testable predictions for interdisciplinary research that have implications not only for evolutionary developmental biology but also for biomedicine and perhaps artificial intelligence and exobiology.},
	language = {en},
	urldate = {2023-04-04},
	journal = {Frontiers in Psychology},
	author = {Levin, Michael},
	month = dec,
	year = {2019},
	pages = {2688},
	file = {Levin - 2019 - The Computational Boundary of a “Self” Developmen.pdf:/Users/ron/Zotero/storage/JQWNT5SE/Levin - 2019 - The Computational Boundary of a “Self” Developmen.pdf:application/pdf},
}

@misc{cerna_anti-unification_2023,
	title = {Anti-unification and {Generalization}: {A} {Survey}},
	shorttitle = {Anti-unification and {Generalization}},
	url = {http://arxiv.org/abs/2302.00277},
	abstract = {Anti-unification (AU), also known as generalization, is a fundamental operation used for inductive inference and is the dual operation to unification, an operation at the foundation of theorem proving. Interest in AU from the AI and related communities is growing, but without a systematic study of the concept, nor surveys of existing work, investigations7 often resort to developing application-specific methods that may be covered by existing approaches. We provide the first survey of AU research and its applications, together with a general framework for categorizing existing and future developments.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Cerna, David M. and Kutsia, Temur},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00277 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:/Users/ron/Zotero/storage/Y8T3U7GV/Cerna and Kutsia - 2023 - Anti-unification and Generalization A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/ron/Zotero/storage/EELRQ6SN/2302.html:text/html},
}

@article{zhang_hippocampal_2023,
	title = {Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience},
	volume = {26},
	copyright = {2022 The Author(s)},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-022-01212-4},
	doi = {10.1038/s41593-022-01212-4},
	abstract = {Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the CA1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of CA1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.},
	language = {en},
	number = {1},
	urldate = {2023-05-03},
	journal = {Nature Neuroscience},
	author = {Zhang, Huanqiu and Rich, P. Dylan and Lee, Albert K. and Sharpee, Tatyana O.},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Learning and memory, Neural encoding},
	pages = {131--139},
	file = {Full Text PDF:/Users/ron/Zotero/storage/FVJI7ZYS/Zhang et al. - 2023 - Hippocampal spatial representations exhibit a hype.pdf:application/pdf},
}

@misc{cetin_hyperbolic_2022,
	title = {Hyperbolic {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2210.01542},
	abstract = {We propose a new class of deep reinforcement learning (RL) algorithms that model latent representations in hyperbolic space. Sequential decision-making requires reasoning about the possible future consequences of current behavior. Consequently, capturing the relationship between key evolving features for a given task is conducive to recovering effective policies. To this end, hyperbolic geometry provides deep RL models with a natural basis to precisely encode this inherently hierarchical information. However, applying existing methodologies from the hyperbolic deep learning literature leads to fatal optimization instabilities due to the non-stationarity and variance characterizing RL gradient estimators. Hence, we design a new general method that counteracts such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope future RL research will consider hyperbolic representations as a standard tool.},
	urldate = {2023-05-07},
	publisher = {arXiv},
	author = {Cetin, Edoardo and Chamberlain, Benjamin and Bronstein, Michael and Hunt, Jonathan J.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01542 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/Z3WZ6DRB/2210.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/FR2XBSAI/Cetin et al. - 2022 - Hyperbolic Deep Reinforcement Learning.pdf:application/pdf},
}

@article{friston_world_2021,
	title = {World model learning and inference},
	volume = {144},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608021003610},
	doi = {10.1016/j.neunet.2021.09.011},
	abstract = {Understanding information processing in the brain—and creating general-purpose artificial intelligence—are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Neural Networks},
	author = {Friston, Karl and Moran, Rosalyn J. and Nagai, Yukie and Taniguchi, Tadahiro and Gomi, Hiroaki and Tenenbaum, Josh},
	month = dec,
	year = {2021},
	keywords = {Bayesian inference, Cognitive development, Free energy principle, Generative model, Predictive coding, Probabilistic inference},
	pages = {573--590},
	file = {ScienceDirect Full Text PDF:/Users/ron/Zotero/storage/BHMH7H34/Friston et al. - 2021 - World model learning and inference.pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/HH7D6IZB/S0893608021003610.html:text/html},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/K36JWDWA/2301.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/35D2I936/Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf:application/pdf},
}

@article{ullman_theory_2012,
	series = {The {Potential} {Contribution} of {Computational} {Modeling} to the {Study} of {Cognitive} {Development}: {When}, and for {What} {Topics}?},
	title = {Theory learning as stochastic search in the language of thought},
	volume = {27},
	issn = {0885-2014},
	url = {https://www.sciencedirect.com/science/article/pii/S0885201412000445},
	doi = {10.1016/j.cogdev.2012.07.005},
	abstract = {We present an algorithmic model for the development of children's intuitive theories within a hierarchical Bayesian framework, where theories are described as sets of logical laws generated by a probabilistic context-free grammar. We contrast our approach with connectionist and other emergentist approaches to modeling cognitive development. While their subsymbolic representations provide a smooth error surface that supports efficient gradient-based learning, our symbolic representations are better suited to capturing children's intuitive theories but give rise to a harder learning problem, which can only be solved by exploratory search. Our algorithm attempts to discover the theory that best explains a set of observed data by performing stochastic search at two levels of abstraction: an outer loop in the space of theories and an inner loop in the space of explanations or models generated by each theory given a particular dataset. We show that this stochastic search is capable of learning appropriate theories in several everyday domains and discuss its dynamics in the context of empirical studies of children's learning.},
	language = {en},
	number = {4},
	urldate = {2023-05-08},
	journal = {Cognitive Development},
	author = {Ullman, Tomer D. and Goodman, Noah D. and Tenenbaum, Joshua B.},
	month = oct,
	year = {2012},
	keywords = {Algorithms, Bayesian models, Conceptual change, Intuitive theories, Language of thought},
	pages = {455--480},
	file = {ScienceDirect Snapshot:/Users/ron/Zotero/storage/TG7Z99T3/S0885201412000445.html:text/html;Submitted Version:/Users/ron/Zotero/storage/XVSR5YLQ/Ullman et al. - 2012 - Theory learning as stochastic search in the langua.pdf:application/pdf},
}

@article{akhlaghpour_rna-based_2022,
	title = {An {RNA}-based theory of natural universal computation},
	volume = {537},
	issn = {0022-5193},
	url = {https://www.sciencedirect.com/science/article/pii/S0022519321004045},
	doi = {10.1016/j.jtbi.2021.110984},
	abstract = {Life is confronted with computation problems in a variety of domains including animal behavior, single-cell behavior, and embryonic development. Yet we currently do not know of a naturally existing biological system that is capable of universal computation, i.e., Turing-equivalent in scope. Generic finite-dimensional dynamical systems (which encompass most models of neural networks, intracellular signaling cascades, and gene regulatory networks) fall short of universal computation, but are assumed to be capable of explaining cognition and development. I present a class of models that bridge two concepts from distant fields: combinatory logic (or, equivalently, lambda calculus) and RNA molecular biology. A set of basic RNA editing rules can make it possible to compute any computable function with identical algorithmic complexity to that of Turing machines. The models do not assume extraordinarily complex molecular machinery or any processes that radically differ from what we already know to occur in cells. Distinct independent enzymes can mediate each of the rules and RNA molecules solve the problem of parenthesis matching through their secondary structure. In the most plausible of these models all of the editing rules can be implemented with merely cleavage and ligation operations at fixed positions relative to predefined motifs. This demonstrates that universal computation is well within the reach of molecular biology. It is therefore reasonable to assume that life has evolved – or possibly began with – a universal computer that yet remains to be discovered. The variety of seemingly unrelated computational problems across many scales can potentially be solved using the same RNA-based computation system. Experimental validation of this theory may immensely impact our understanding of memory, cognition, development, disease, evolution, and the early stages of life.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Journal of Theoretical Biology},
	author = {Akhlaghpour, Hessameddin},
	month = mar,
	year = {2022},
	keywords = {Church-Turing thesis, Combinatory logic, Junk DNA, Lambda calculus, Molecular engram, RNA, Secondary structure, Turing-completeness, Turing-equivalence, Universal computation},
	pages = {110984},
	file = {ScienceDirect Snapshot:/Users/ron/Zotero/storage/EF326MVB/S0022519321004045.html:text/html;Submitted Version:/Users/ron/Zotero/storage/GIVX9YGL/Akhlaghpour - 2022 - An RNA-based theory of natural universal computati.pdf:application/pdf},
}

@article{do_neural_2021,
	title = {Neural circuits and symbolic processing},
	volume = {186},
	issn = {1074-7427},
	url = {https://www.sciencedirect.com/science/article/pii/S107474272100174X},
	doi = {10.1016/j.nlm.2021.107552},
	abstract = {The ability to use symbols is a defining feature of human intelligence. However, neuroscience has yet to explain the fundamental neural circuit mechanisms for flexibly representing and manipulating abstract concepts. This article will review the research on neural models for symbolic processing. The review first focuses on the question of how symbols could possibly be represented in neural circuits. The review then addresses how neural symbolic representations could be flexibly combined to meet a wide range of reasoning demands. Finally, the review assesses the research on program synthesis and proposes that the most flexible neural representation of symbolic processing would involve the capacity to rapidly synthesize neural operations analogous to lambda calculus to solve complex cognitive tasks.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Neurobiology of Learning and Memory},
	author = {Do, Quan and Hasselmo, Michael E.},
	month = dec,
	year = {2021},
	keywords = {Bayesian inference, Category theory, Conjunctive coding, Dynamic binding, Program synthesis, Reinforcement learning},
	pages = {107552},
	file = {Accepted Version:/Users/ron/Zotero/storage/CBEQBKQH/Do and Hasselmo - 2021 - Neural circuits and symbolic processing.pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/HLCDM7P8/S107474272100174X.html:text/html},
}

@misc{deletang_neural_2023,
	title = {Neural {Networks} and the {Chomsky} {Hierarchy}},
	url = {http://arxiv.org/abs/2207.02098},
	abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Delétang, Grégoire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
	month = feb,
	year = {2023},
	note = {arXiv:2207.02098 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/8GNSTLFM/2207.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/828FYZ2L/Delétang et al. - 2023 - Neural Networks and the Chomsky Hierarchy.pdf:application/pdf},
}

@article{han_abductive_2023,
	title = {Abductive subconcept learning},
	volume = {66},
	issn = {1674-733X, 1869-1919},
	url = {https://link.springer.com/10.1007/s11432-020-3569-0},
	doi = {10.1007/s11432-020-3569-0},
	abstract = {Bridging neural network learning and symbolic reasoning is crucial for strong AI. Few pioneering studies have made some progress on logical reasoning tasks that require partitioned inputs of instances (e.g., sequential data), from which a ﬁnal concept is formed based on the complex (perhaps logical) relationships between them. However, they cannot apply to low-level cognitive tasks that require unpartitioned inputs (e.g., raw images), such as object recognition and text classiﬁcation. In this paper, we propose abductive subconcept learning (ASL) to bridge neural network learning and symbolic reasoning on unsegmented image classiﬁcation tasks. ASL uses deep learning and abductive logical reasoning to jointly learn subconcept perception and secondary reasoning. Speciﬁcally, it ﬁrst employs meta-interpretive learning (MIL) to induce ﬁrst-order logical hypotheses capturing the relationships between the high-level subconcepts that account for the target concept. Then, it uses the groundings of the logical hypotheses as labels to train a deep learning model for identifying the subconcepts from unpartitioned data. ASL jointly trains the deep learning model and learns the MIL theory by minimizing the inconsistency between their grounded outputs. Experimental results show that ASL successfully integrates machine learning and logical reasoning with accurate and interpretable results in several object recognition tasks.},
	language = {en},
	number = {2},
	urldate = {2023-05-08},
	journal = {Science China Information Sciences},
	author = {Han, Zhongyi and Cai, Le-Wen and Dai, Wang-Zhou and Huang, Yu-Xuan and Wei, Benzheng and Wang, Wei and Yin, Yilong},
	month = feb,
	year = {2023},
	pages = {122103},
	file = {Han et al. - 2023 - Abductive subconcept learning.pdf:/Users/ron/Zotero/storage/WLLGSR5S/Han et al. - 2023 - Abductive subconcept learning.pdf:application/pdf},
}

@article{wang_representation_2019,
	title = {Representation of spatial sequences using nested rules in human prefrontal cortex},
	volume = {186},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811918320330},
	doi = {10.1016/j.neuroimage.2018.10.061},
	abstract = {Memory for spatial sequences does not depend solely on the number of locations to be stored, but also on the presence of spatial regularities. Here, we show that the human brain quickly stores spatial sequences by detecting geometrical regularities at multiple time scales and encoding them in a format akin to a programming language. We measured gaze-anticipation behavior while spatial sequences of variable regularity were repeated. Participants’ behavior suggested that they quickly discovered the most compact description of each sequence in a language comprising nested rules, and used these rules to compress the sequence in memory and predict the next items. Activity in dorsal inferior prefrontal cortex correlated with the amount of compression, while right dorsolateral prefrontal cortex encoded the presence of embedded structures. Sequence learning was accompanied by a progressive differentiation of multi-voxel activity patterns in these regions. We propose that humans are endowed with a simple “language of geometry” which recruits a dorsal prefrontal circuit for geometrical rules, distinct from but close to areas involved in natural language processing.},
	language = {en},
	urldate = {2023-05-08},
	journal = {NeuroImage},
	author = {Wang, Liping and Amalric, Marie and Fang, Wen and Jiang, Xinjian and Pallier, Christophe and Figueira, Santiago and Sigman, Mariano and Dehaene, Stanislas},
	month = feb,
	year = {2019},
	pages = {245--255},
	file = {Accepted Version:/Users/ron/Zotero/storage/CNCSVSSY/Wang et al. - 2019 - Representation of spatial sequences using nested r.pdf:application/pdf},
}

@inproceedings{zhang_novel_2019,
	address = {Montreal, QC, Canada},
	title = {A {Novel} {Neural} {Source} {Code} {Representation} {Based} on {Abstract} {Syntax} {Tree}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812062/},
	doi = {10.1109/ICSE.2019.00086},
	abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and ﬁnally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classiﬁcation and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
	language = {en},
	urldate = {2023-05-08},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
	month = may,
	year = {2019},
	pages = {783--794},
	file = {Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf:/Users/ron/Zotero/storage/39Q8L7T8/Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf:application/pdf},
}

@inproceedings{oliveira_abstract_2013,
	address = {Rome Italy},
	title = {Abstract syntax graphs for domain specific languages},
	isbn = {978-1-4503-1842-6},
	url = {https://dl.acm.org/doi/10.1145/2426890.2426909},
	doi = {10.1145/2426890.2426909},
	abstract = {An important problem in the context of embedded domain speciﬁc languages (EDSLs) is how to provide easy to use, yet expressive representations of abstract syntax. So far providing user-friendly encodings of abstract syntax that enable operations that observe or preserve sharing and recursion has proved to be quite elusive.},
	language = {en},
	urldate = {2023-05-08},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2013 workshop on {Partial} evaluation and program manipulation},
	publisher = {ACM},
	author = {Oliveira, Bruno C. D. S. and Löh, Andres},
	month = jan,
	year = {2013},
	pages = {87--96},
	file = {Oliveira and Löh - 2013 - Abstract syntax graphs for domain specific languag.pdf:/Users/ron/Zotero/storage/CE6IREWV/Oliveira and Löh - 2013 - Abstract syntax graphs for domain specific languag.pdf:application/pdf},
}

@misc{santoro_symbolic_2022,
	title = {Symbolic {Behaviour} in {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2102.03406},
	abstract = {The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
	month = jan,
	year = {2022},
	note = {arXiv:2102.03406 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/LMG7CQZY/2102.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/6YDV96L2/Santoro et al. - 2022 - Symbolic Behaviour in Artificial Intelligence.pdf:application/pdf},
}

@article{shindo_alphailp_2023,
	title = {\$\${\textbackslash}alpha\$\${ILP}: thinking visual scenes as differentiable logic programs},
	volume = {112},
	issn = {1573-0565},
	shorttitle = {\$\${\textbackslash}alpha\$\${ILP}},
	url = {https://doi.org/10.1007/s10994-023-06320-1},
	doi = {10.1007/s10994-023-06320-1},
	abstract = {Deep neural learning has shown remarkable performance at learning representations for visual object categorization. However, deep neural networks such as CNNs do not explicitly encode objects and relations among them. This limits their success on tasks that require a deep logical understanding of visual scenes, such as Kandinsky patterns and Bongard problems. To overcome these limitations, we introduce \$\${\textbackslash}alpha \{{\textbackslash}textit\{ILP\}\}\$\$, a novel differentiable inductive logic programming framework that learns to represent scenes as logic programs—intuitively, logical atoms correspond to objects, attributes, and relations, and clauses encode high-level scene information. \$\${\textbackslash}alpha\$\$ILP has an end-to-end reasoning architecture from visual inputs. Using it, \$\${\textbackslash}alpha\$\$ILP performs differentiable inductive logic programming on complex visual scenes, i.e., the logical rules are learned by gradient descent. Our extensive experiments on Kandinsky patterns and CLEVR-Hans benchmarks demonstrate the accuracy and efficiency of \$\${\textbackslash}alpha \{{\textbackslash}textit\{ILP\}\}\$\$in learning complex visual-logical concepts.},
	language = {en},
	number = {5},
	urldate = {2023-05-08},
	journal = {Machine Learning},
	author = {Shindo, Hikaru and Pfanschilling, Viktor and Dhami, Devendra Singh and Kersting, Kristian},
	month = may,
	year = {2023},
	keywords = {Differentiable reasoning, Inductive logic programming, Neuro-symbolic AI, Object-centric learning},
	pages = {1465--1497},
	file = {Full Text PDF:/Users/ron/Zotero/storage/UECDBWYK/Shindo et al. - 2023 - \$\$alpha\$\$ILP thinking visual scenes as different.pdf:application/pdf},
}

@misc{bhattarai_tsetlin_2023,
	title = {Tsetlin {Machine} {Embedding}: {Representing} {Words} {Using} {Logical} {Expressions}},
	shorttitle = {Tsetlin {Machine} {Embedding}},
	url = {http://arxiv.org/abs/2301.00709},
	abstract = {Embedding words in vector space is a fundamental first step in state-of-the-art natural language processing (NLP). Typical NLP solutions employ pre-defined vector representations to improve generalization by co-locating similar words in vector space. For instance, Word2Vec is a self-supervised predictive model that captures the context of words using a neural network. Similarly, GLoVe is a popular unsupervised model incorporating corpus-wide word co-occurrence statistics. Such word embedding has significantly boosted important NLP tasks, including sentiment analysis, document classification, and machine translation. However, the embeddings are dense floating-point vectors, making them expensive to compute and difficult to interpret. In this paper, we instead propose to represent the semantics of words with a few defining words that are related using propositional logic. To produce such logical embeddings, we introduce a Tsetlin Machine-based autoencoder that learns logical clauses self-supervised. The clauses consist of contextual words like "black," "cup," and "hot" to define other words like "coffee," thus being human-understandable. We evaluate our embedding approach on several intrinsic and extrinsic benchmarks, outperforming GLoVe on six classification tasks. Furthermore, we investigate the interpretability of our embedding using the logical representations acquired during training. We also visualize word clusters in vector space, demonstrating how our logical embedding co-locate similar words.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Bhattarai, Bimal and Granmo, Ole-Christoffer and Jiao, Lei and Yadav, Rohan and Sharma, Jivitesh},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00709 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/QGRU7ZHJ/2301.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/PTLM8GLL/Bhattarai et al. - 2023 - Tsetlin Machine Embedding Representing Words Usin.pdf:application/pdf},
}

@misc{hinton_how_2021,
	title = {How to represent part-whole hierarchies in a neural network},
	url = {http://arxiv.org/abs/2102.12627},
	abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Hinton, Geoffrey},
	month = feb,
	year = {2021},
	note = {arXiv:2102.12627 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.6, I.4.8},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/KS4BB9LM/2102.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/PXJDMPZG/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf:application/pdf},
}

@misc{culp_testing_2022,
	title = {Testing {GLOM}'s ability to infer wholes from ambiguous parts},
	url = {http://arxiv.org/abs/2211.16564},
	abstract = {The GLOM architecture proposed by Hinton [2021] is a recurrent neural network for parsing an image into a hierarchy of wholes and parts. When a part is ambiguous, GLOM assumes that the ambiguity can be resolved by allowing the part to make multi-modal predictions for the pose and identity of the whole to which it belongs and then using attention to similar predictions coming from other possibly ambiguous parts to settle on a common mode that is predicted by several different parts. In this study, we describe a highly simplified version of GLOM that allows us to assess the effectiveness of this way of dealing with ambiguity. Our results show that, with supervised training, GLOM is able to successfully form islands of very similar embedding vectors for all of the locations occupied by the same object and it is also robust to strong noise injections in the input and to out-of-distribution input transformations.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Culp, Laura and Sabour, Sara and Hinton, Geoffrey E.},
	month = nov,
	year = {2022},
	note = {arXiv:2211.16564 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/KPN5WGFW/2211.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/FIHPBJ5Z/Culp et al. - 2022 - Testing GLOM's ability to infer wholes from ambigu.pdf:application/pdf},
}

@misc{kissner_neural-symbolic_2020,
	title = {A {Neural}-{Symbolic} {Framework} for {Mental} {Simulation}},
	url = {http://arxiv.org/abs/2008.02356},
	abstract = {We present a neural-symbolic framework for observing the environment and continuously learning visual semantics and intuitive physics to reproduce them in an interactive simulation. The framework consists of five parts, a neural-symbolic hybrid network based on capsules for inverse graphics, an episodic memory to store observations, an interaction network for intuitive physics, a meta-learning agent that continuously improves the framework and a querying language that acts as the framework's interface for simulation. By means of lifelong meta-learning, the capsule network is expanded and trained continuously, in order to better adapt to its environment with each iteration. This enables it to learn new semantics using a few-shot approach and with minimal input from an oracle over its lifetime. From what it learned through observation, the part for intuitive physics infers all the required physical properties of the objects in a scene, enabling predictions. Finally, a custom query language ties all parts together, which allows to perform various mental simulation tasks, such as navigation, sorting and simulation of a game environment, with which we illustrate the potential of our novel approach.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Kissner, Michael},
	month = aug,
	year = {2020},
	note = {arXiv:2008.02356 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/ICBZUJ43/2008.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/3Y4CVFED/Kissner - 2020 - A Neural-Symbolic Framework for Mental Simulation.pdf:application/pdf},
}

@misc{kissner_adding_2019,
	title = {Adding {Intuitive} {Physics} to {Neural}-{Symbolic} {Capsules} {Using} {Interaction} {Networks}},
	url = {http://arxiv.org/abs/1905.09891},
	abstract = {Many current methods to learn intuitive physics are based on interaction networks and similar approaches. However, they rely on information that has proven difficult to estimate directly from image data in the past. We aim to narrow this gap by inferring all the semantic information needed from raw pixel data in the form of a scene-graph. Our approach is based on neural-symbolic capsules, which identify which objects in the scene are static, dynamic, elastic or rigid, possible joints between them, as well as their collision information. By integrating all this with interaction networks, we demonstrate how our method is able to learn intuitive physics directly from image sequences and apply its knowledge to new scenes and objects, resulting in an inverse-simulation pipeline.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Kissner, Michael and Mayer, Helmut},
	month = jun,
	year = {2019},
	note = {arXiv:1905.09891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/FR73HXTA/1905.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/J4DUZLBR/Kissner and Mayer - 2019 - Adding Intuitive Physics to Neural-Symbolic Capsul.pdf:application/pdf},
}

@misc{kosiorek_stacked_2019,
	title = {Stacked {Capsule} {Autoencoders}},
	url = {http://arxiv.org/abs/1906.06818},
	abstract = {Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55\%) and MNIST (98.7\%). The code is available at https://github.com/google-research/google-research/tree/master/stacked\_capsule\_autoencoders},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
	month = dec,
	year = {2019},
	note = {arXiv:1906.06818 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/R94Q6YLI/1906.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/WM3DUKLT/Kosiorek et al. - 2019 - Stacked Capsule Autoencoders.pdf:application/pdf},
}

@book{kissner_neural-symbolic_2019,
	title = {A {Neural}-{Symbolic} {Architecture} for {Inverse} {Graphics} {Improved} by {Lifelong} {Meta}-{Learning}},
	volume = {11824},
	url = {http://arxiv.org/abs/1905.08910},
	abstract = {We follow the idea of formulating vision as inverse graphics and propose a new type of element for this task, a neural-symbolic capsule. It is capable of de-rendering a scene into semantic information feed-forward, as well as rendering it feed-backward. An initial set of capsules for graphical primitives is obtained from a generative grammar and connected into a full capsule network. Lifelong meta-learning continuously improves this network’s detection capabilities by adding capsules for new and more complex objects it detects in a scene using few-shot learning. Preliminary results demonstrate the potential of our novel approach.},
	language = {en},
	urldate = {2023-05-08},
	author = {Kissner, Michael and Mayer, Helmut},
	year = {2019},
	doi = {10.1007/978-3-030-33676-9},
	note = {arXiv:1905.08910 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kissner and Mayer - 2019 - A Neural-Symbolic Architecture for Inverse Graphic.pdf:/Users/ron/Zotero/storage/NB744TDT/Kissner and Mayer - 2019 - A Neural-Symbolic Architecture for Inverse Graphic.pdf:application/pdf},
}

@article{jackson_reverse-engineering_2021,
	title = {Reverse-engineering the cortical architecture for controlled semantic cognition},
	volume = {5},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-01034-z},
	doi = {10.1038/s41562-020-01034-z},
	abstract = {We employ a reverse-engineering approach to illuminate the neurocomputational building blocks that combine to support controlled semantic cognition: the storage and context-appropriate use of conceptual knowledge. By systematically varying the structure of a computational model and assessing the functional consequences, we identified the architectural properties that best promote some core functions of the semantic system. Semantic cognition presents a challenging test case, as the brain must achieve two seemingly contradictory functions: abstracting context-invariant conceptual representations across time and modalities, while producing specific context-sensitive behaviours appropriate for the immediate task. These functions were best achieved in models possessing a single, deep multimodal hub with sparse connections from modality-specific regions, and control systems acting on peripheral rather than deep network layers. The reverse-engineered model provides a unifying account of core findings in the cognitive neuroscience of controlled semantic cognition, including evidence from anatomy, neuropsychology and functional brain imaging.},
	language = {en},
	number = {6},
	urldate = {2023-05-08},
	journal = {Nature Human Behaviour},
	author = {Jackson, Rebecca L. and Rogers, Timothy T. and Lambon Ralph, Matthew A.},
	month = jun,
	year = {2021},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Language, Cognitive control},
	pages = {774--786},
	file = {Accepted Version:/Users/ron/Zotero/storage/3IE3PPPU/Jackson et al. - 2021 - Reverse-engineering the cortical architecture for .pdf:application/pdf},
}

@article{pournaras_holarchic_2020,
	title = {Holarchic structures for decentralized deep learning: a performance analysis},
	volume = {23},
	issn = {1573-7543},
	shorttitle = {Holarchic structures for decentralized deep learning},
	url = {https://doi.org/10.1007/s10586-019-02906-4},
	doi = {10.1007/s10586-019-02906-4},
	abstract = {Structure plays a key role in learning performance. In centralized computational systems, hyperparameter optimization and regularization techniques such as dropout are computational means to enhance learning performance by adjusting the deep hierarchical structure. However, in decentralized deep learning by the Internet of Things, the structure is an actual network of autonomous interconnected devices such as smart phones that interact via complex network protocols. Self-adaptation of the learning structure is a challenge. Uncertainties such as network latency, node and link failures or even bottlenecks by limited processing capacity and energy availability can significantly downgrade learning performance. Network self-organization and self-management is complex, while it requires additional computational and network resources that hinder the feasibility of decentralized deep learning. In contrast, this paper introduces a self-adaptive learning approach based on holarchic learning structures for exploring, mitigating and boosting learning performance in distributed environments with uncertainties. A large-scale performance analysis with 864,000 experiments fed with synthetic and real-world data from smart grid and smart city pilot projects confirm the cost-effectiveness of holarchic structures for decentralized deep learning.},
	language = {en},
	number = {1},
	urldate = {2023-05-08},
	journal = {Cluster Computing},
	author = {Pournaras, Evangelos and Yadhunathan, Srivatsan and Diaconescu, Ada},
	month = mar,
	year = {2020},
	keywords = {Deep learning, Holarchy, Multi-agent system, Optimization, Resilience, Smart city},
	pages = {219--240},
	file = {Full Text PDF:/Users/ron/Zotero/storage/78WANKEM/Pournaras et al. - 2020 - Holarchic structures for decentralized deep learni.pdf:application/pdf},
}

@article{ditullio_dynamical_2021,
	series = {Computational {Neuroscience}},
	title = {Dynamical self-organization and efficient representation of space by grid cells},
	volume = {70},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438821001343},
	doi = {10.1016/j.conb.2021.11.007},
	abstract = {To plan trajectories and navigate, animals must maintain a mental representation of the environment and their own position within it. This “cognitive map” is thought to be supported in part by the entorhinal cortex, where grid cells are active when an animal occupies the vertices of a scaling hierarchy of periodic lattices of locations in an enclosure. Here, we review computational developments which suggest that the grid cell network is: (a) efficient, providing required spatial resolution with a minimum number of neurons, (b) self-organizing, dynamically coordinating the structure and scale of the responses, and (c) adaptive, re-organizing in response to changes in landmarks and the structure of the boundaries of spaces. We consider these ideas in light of recent discoveries of similar structures in the mental representation of abstract spaces of shapes and smells, and in other brain areas, and highlight promising directions for future research.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Current Opinion in Neurobiology},
	author = {DiTullio, Ronald W. and Balasubramanian, Vijay},
	month = oct,
	year = {2021},
	pages = {206--213},
	file = {ScienceDirect Full Text PDF:/Users/ron/Zotero/storage/6QSY8JJT/DiTullio and Balasubramanian - 2021 - Dynamical self-organization and efficient represen.pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/KB8VD8C4/S0959438821001343.html:text/html},
}

@misc{arsiwalla_morphospace_2018,
	title = {The {Morphospace} of {Consciousness}},
	url = {http://arxiv.org/abs/1705.11190},
	abstract = {We construct a complexity-based morphospace to study systems-level properties of conscious \& intelligent systems. The axes of this space label 3 complexity types: autonomous, cognitive \& social. Given recent proposals to synthesize consciousness, a generic complexity-based conceptualization provides a useful framework for identifying defining features of conscious \& synthetic systems. Based on current clinical scales of consciousness that measure cognitive awareness and wakefulness, we take a perspective on how contemporary artificially intelligent machines \& synthetically engineered life forms measure on these scales. It turns out that awareness \& wakefulness can be associated to computational \& autonomous complexity respectively. Subsequently, building on insights from cognitive robotics, we examine the function that consciousness serves, \& argue the role of consciousness as an evolutionary game-theoretic strategy. This makes the case for a third type of complexity for describing consciousness: social complexity. Having identified these complexity types, allows for a representation of both, biological \& synthetic systems in a common morphospace. A consequence of this classification is a taxonomy of possible conscious machines. We identify four types of consciousness, based on embodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii) group consciousness (resulting from group interactions), \& (iv) simulated consciousness (embodied by virtual agents within a simulated reality). This taxonomy helps in the investigation of comparative signatures of consciousness across domains, in order to highlight design principles necessary to engineer conscious machines. This is particularly relevant in the light of recent developments at the crossroads of cognitive neuroscience, biomedical engineering, artificial intelligence \& biomimetics.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Arsiwalla, Xerxes D. and Sole, Ricard and Moulin-Frier, Clement and Herreros, Ivan and Sanchez-Fibla, Marti and Verschure, Paul},
	month = nov,
	year = {2018},
	note = {arXiv:1705.11190 [cond-mat, physics:physics, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition, Condensed Matter - Disordered Systems and Neural Networks, Physics - Biological Physics},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/YPAUUA2Y/1705.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/44B9NLZM/Arsiwalla et al. - 2018 - The Morphospace of Consciousness.pdf:application/pdf},
}

@article{bates_modeling_2019,
	title = {Modeling human intuitions about liquid flow with particle-based simulation},
	volume = {15},
	issn = {1553-7358},
	url = {http://arxiv.org/abs/1809.01524},
	doi = {10.1371/journal.pcbi.1007210},
	abstract = {Humans can easily describe, imagine, and, crucially, predict a wide variety of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking, dripping, draining, trickling, pooling, and pouring--despite tremendous variability in their material and dynamical properties. Here we propose and test a computational model of how people perceive and predict these liquid dynamics, based on coarse approximate simulations of fluids as collections of interacting particles. Our model is analogous to a "game engine in the head", drawing on techniques for interactive simulations (as in video games) that optimize for efficiency and natural appearance rather than physical accuracy. In two behavioral experiments, we found that the model accurately captured people's predictions about how liquids flow among complex solid obstacles, and was significantly better than two alternatives based on simple heuristics and deep neural networks. Our model was also able to explain how people's predictions varied as a function of the liquids' properties (e.g., viscosity and stickiness). Together, the model and empirical results extend the recent proposal that human physical scene understanding for the dynamics of rigid, solid objects can be supported by approximate probabilistic simulation, to the more complex and unexplored domain of fluid dynamics.},
	number = {7},
	urldate = {2023-05-08},
	journal = {PLOS Computational Biology},
	author = {Bates, Christopher J. and Yildirim, Ilker and Tenenbaum, Joshua B. and Battaglia, Peter},
	month = jul,
	year = {2019},
	note = {arXiv:1809.01524 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	pages = {e1007210},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/VCGTSSWJ/1809.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/YKSZJXK6/Bates et al. - 2019 - Modeling human intuitions about liquid flow with p.pdf:application/pdf},
}

@misc{he_trees_2021,
	title = {Trees in transformers: a theoretical analysis of the {Transformer}'s ability to represent trees},
	shorttitle = {Trees in transformers},
	url = {http://arxiv.org/abs/2112.11913},
	abstract = {Transformer networks are the de facto standard architecture in natural language processing. To date, there are no theoretical analyses of the Transformer's ability to capture tree structures. We focus on the ability of Transformer networks to learn tree structures that are important for tree transduction problems. We first analyze the theoretical capability of the standard Transformer architecture to learn tree structures given enumeration of all possible tree backbones, which we define as trees without labels. We then prove that two linear layers with ReLU activation function can recover any tree backbone from any two nonzero, linearly independent starting backbones. This implies that a Transformer can learn tree structures well in theory. We conduct experiments with synthetic data and find that the standard Transformer achieves similar accuracy compared to a Transformer where tree position information is explicitly encoded, albeit with slower convergence. This confirms empirically that Transformers can learn tree structures.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {He, Qi and Sedoc, João and Rodu, Jordan},
	month = dec,
	year = {2021},
	note = {arXiv:2112.11913 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/PLW2GDFZ/2112.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/M8QIKVIP/He et al. - 2021 - Trees in transformers a theoretical analysis of t.pdf:application/pdf},
}

@article{cartuyvels_discrete_2021,
	title = {Discrete and continuous representations and processing in deep learning: {Looking} forward},
	volume = {2},
	issn = {26666510},
	shorttitle = {Discrete and continuous representations and processing in deep learning},
	url = {http://arxiv.org/abs/2201.01233},
	doi = {10.1016/j.aiopen.2021.07.002},
	abstract = {Discrete and continuous representations of content (e.g., of language or images) have interesting properties to be explored for the understanding of or reasoning with this content by machines. This position paper puts forward our opinion on the role of discrete and continuous representations and their processing in the deep learning field. Current neural network models compute continuous-valued data. Information is compressed into dense, distributed embeddings. By stark contrast, humans use discrete symbols in their communication with language. Such symbols represent a compressed version of the world that derives its meaning from shared contextual information. Additionally, human reasoning involves symbol manipulation at a cognitive level, which facilitates abstract reasoning, the composition of knowledge and understanding, generalization and efficient learning. Motivated by these insights, in this paper we argue that combining discrete and continuous representations and their processing will be essential to build systems that exhibit a general form of intelligence. We suggest and discuss several avenues that could improve current neural networks with the inclusion of discrete elements to combine the advantages of both types of representations.},
	urldate = {2023-05-08},
	journal = {AI Open},
	author = {Cartuyvels, Ruben and Spinks, Graham and Moens, Marie-Francine},
	year = {2021},
	note = {arXiv:2201.01233 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	pages = {143--159},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/7GXRL6ZA/2201.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/V6SN9229/Cartuyvels et al. - 2021 - Discrete and continuous representations and proces.pdf:application/pdf},
}

@inproceedings{nickel_poincare_2017,
	title = {Poincaré {Embeddings} for {Learning} {Hierarchical} {Representations}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html},
	abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
	urldate = {2023-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Nickel, Maximillian and Kiela, Douwe},
	year = {2017},
	file = {Full Text PDF:/Users/ron/Zotero/storage/V7QLFFDV/Nickel and Kiela - 2017 - Poincaré Embeddings for Learning Hierarchical Repr.pdf:application/pdf},
}

@article{auyespek_hyperbolic_nodate,
	title = {Hyperbolic {Embedding} for {Finding} {Syntax} in {BERT}},
	abstract = {Recent advances in natural language processing have improved our understanding of what kind of linguistic knowledge is encoded in modern word representations. For example, methods for testing the ability to extract syntax trees from a language model architecture were developed by Hewitt and Manning (2019)—they project word vectors into Euclidean subspace in such a way that the corresponding squared Euclidean distance approximates the tree distance between words in the syntax tree. This work proposes a method for assessing whether embedding word representations in hyperbolic space can better reflect the graph structure of syntax trees. We show that the tree distance between words in a syntax tree can be approximated well by the hyperbolic distance between corresponding word vectors.},
	language = {en},
	author = {Auyespek, Temirlan and Mach, Thomas and Assylbekov, Zhenisbek},
	file = {Auyespek et al. - Hyperbolic Embedding for Finding Syntax in BERT.pdf:/Users/ron/Zotero/storage/MGCIZMYJ/Auyespek et al. - Hyperbolic Embedding for Finding Syntax in BERT.pdf:application/pdf},
}

@article{lu_hyperbolic_2019,
	title = {Hyperbolic {Function} {Embedding}: {Learning} {Hierarchical} {Representation} for {Functions} of {Source} {Code} in {Hyperbolic} {Space}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	shorttitle = {Hyperbolic {Function} {Embedding}},
	url = {https://www.mdpi.com/2073-8994/11/2/254},
	doi = {10.3390/sym11020254},
	abstract = {Recently, source code mining has received increasing attention due to the rapid increase of open-sourced code repositories and the tremendous values implied in this large dataset, which can help us understand the organization of functions or classes in different software and analyze the impact of these organized patterns on the software behaviors. Hence, learning an effective representation model for the functions of source code, from a modern view, is a crucial problem. Considering the inherent hierarchy of functions, we propose a novel hyperbolic function embedding (HFE) method, which can learn a distributed and hierarchical representation for each function via the Poincaré ball model. To achieve this, a function call graph (FCG) is first constructed to model the call relationship among functions. To verify the underlying geometry of FCG, the Ricci curvature model is used. Finally, an HFE model is built to learn the representations that can capture the latent hierarchy of functions in the hyperbolic space, instead of the Euclidean space, which are usually used in those state-of-the-art methods. Moreover, HFE is more compact in terms of lower dimensionality than the existing graph embedding methods. Thus, HFE is more effective in terms of computation and storage. To experimentally evaluate the performance of HFE, two application scenarios, namely, function classification and link prediction, have been applied. HFE achieves up to 7.6\% performance improvement compared to the chosen state-of-the-art methods, namely, Node2vec and Struc2vec.},
	language = {en},
	number = {2},
	urldate = {2023-05-08},
	journal = {Symmetry},
	author = {Lu, Mingming and Liu, Yan and Li, Haifeng and Tan, Dingwu and He, Xiaoxian and Bi, Wenjie and Li, Wendbo},
	month = feb,
	year = {2019},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {function embedding representation, function-call graph, hyperbolic space, source code mining},
	pages = {254},
	file = {Full Text PDF:/Users/ron/Zotero/storage/TJ86UALI/Lu et al. - 2019 - Hyperbolic Function Embedding Learning Hierarchic.pdf:application/pdf},
}

@article{khan_hyperbolic_nodate,
	title = {Hyperbolic {Representations} of {Source} {Code}},
	abstract = {Learning effective representations of data is an important task in machine learning. Existing methods typically compute representations or embeddings in Euclidean space, which has shortcomings in representing hierarchical structures of the underlying data. Alternatively, hyperbolic geometry offers a representation scheme that is suited for robust, high-ﬁdelity representations of tree-structured data. In this paper, we explore hyperbolic graph convolutional models for learning hyperbolic representations of source code, which exhibit natural hierarchies. We leverage the abstract syntax tree (AST) of source code and learn its graph-based representation to predict the function name from its body. We compare Lorentz and Poincaré Disk models of hyperbolic geometry with Euclidean geometry. We also propose several readout schemes to compute the graph-level representations and apply them to the method name prediction task. Using a Lorentz hyperbolic model, we establish a new state-of-the-art result on the ogbg-code2 benchmark for the task.},
	language = {en},
	author = {Khan, Raiyan and Nguyen, Thanh V and Srinivasan, Sengamedu H},
	file = {Khan et al. - Hyperbolic Representations of Source Code.pdf:/Users/ron/Zotero/storage/UILJKTIW/Khan et al. - Hyperbolic Representations of Source Code.pdf:application/pdf},
}

@misc{noauthor_implementing_nodate,
	title = {Implementing {Poincaré} {Embeddings} {\textbar} {RARE} {Technologies}},
	url = {https://rare-technologies.com/implementing-poincare-embeddings/},
	urldate = {2023-05-08},
}

@article{guzdial_game_2016,
	title = {Game {Level} {Generation} from {Gameplay} {Videos}},
	volume = {12},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
	issn = {2334-0924},
	url = {https://ojs.aaai.org/index.php/AIIDE/article/view/12861},
	doi = {10.1609/aiide.v12i1.12861},
	abstract = {We present an unsupervised process to generate full video game levels from a model trained on gameplay video. The model represents probabilistic relationships between shapes properties, and relates the relationships to stylistic variance within a domain. We utilize the classic platformer game Super Mario Bros. to evaluate this process due to its highly-regarded level design. We evaluate the output in comparison to other data-driven level generation techniques via a user study and demonstrate its ability to produce novel output more stylistically similar to exemplar input.},
	language = {en},
	number = {1},
	urldate = {2023-05-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
	author = {Guzdial, Matthew and Riedl, Mark},
	year = {2016},
	note = {Number: 1},
	keywords = {game ai},
	pages = {44--50},
	file = {Full Text PDF:/Users/ron/Zotero/storage/QXFEJJU9/Guzdial and Riedl - 2016 - Game Level Generation from Gameplay Videos.pdf:application/pdf},
}

@article{manchin_program_nodate,
	title = {Program {Generation} from {Diverse} {Video} {Demonstrations}},
	abstract = {The ability to use inductive reasoning to extract general rules from multiple observations is a vital indicator of intelligence. As humans, we use this ability to not only interpret the world around us, but also to predict the outcomes of the various interactions we experience. Generalising over multiple observations is a task that has historically presented difficulties for machines to grasp, especially when requiring computer vision. In this paper, we propose a model that can extract general rules from video demonstrations by simultaneously performing summarisation and translation. Our approach differs from prior works by framing the problem as a multi-sequence-to-sequence task, wherein summarisation is learnt by the model. This allows our model to utilise edge cases that would otherwise be suppressed or discarded by traditional summarisation techniques. Additionally, we show that our approach can handle noisy specifications without the need for additional filtering methods. We evaluate our model by synthesising programs from video demonstrations in the Vizdoom environment achieving state-of-the-art results with a relative increase of 11.75\% program accuracy on prior works.},
	language = {en},
	author = {Manchin, Anthony},
	file = {Manchin - Program Generation from Diverse Video Demonstratio.pdf:/Users/ron/Zotero/storage/8VYKF75E/Manchin - Program Generation from Diverse Video Demonstratio.pdf:application/pdf},
}

@inproceedings{guzdial_game_2017,
	address = {Melbourne, Australia},
	title = {Game {Engine} {Learning} from {Video}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/518},
	doi = {10.24963/ijcai.2017/518},
	abstract = {Intelligent agents need to be able to make predictions about their environment. In this work we present a novel approach to learn a forward simulation model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is signiﬁcantly less complex than reality. We demonstrate the signiﬁcant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
	language = {en},
	urldate = {2023-05-08},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O.},
	month = aug,
	year = {2017},
	pages = {3707--3713},
	file = {Guzdial et al. - 2017 - Game Engine Learning from Video.pdf:/Users/ron/Zotero/storage/DYTW5Y5N/Guzdial et al. - 2017 - Game Engine Learning from Video.pdf:application/pdf},
}

@misc{kaiser_model-based_2020,
	title = {Model-{Based} {Reinforcement} {Learning} for {Atari}},
	url = {http://arxiv.org/abs/1903.00374},
	abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
	month = feb,
	year = {2020},
	note = {arXiv:1903.00374 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/FMVP5GR3/1903.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/GKPUFYK5/Kaiser et al. - 2020 - Model-Based Reinforcement Learning for Atari.pdf:application/pdf},
}

@book{gulwani_program_2017,
	address = {Hanover, MA Delft},
	series = {Foundations and trends in programming languages},
	title = {Program synthesis},
	isbn = {978-1-68083-292-1},
	language = {en},
	number = {4.2017, 1-2},
	publisher = {Now Publishers},
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	year = {2017},
	file = {Gulwani et al. - 2017 - Program synthesis.pdf:/Users/ron/Zotero/storage/9UZ68Y9G/Gulwani et al. - 2017 - Program synthesis.pdf:application/pdf},
}

@misc{simmons-edler_program_2018,
	title = {Program {Synthesis} {Through} {Reinforcement} {Learning} {Guided} {Tree} {Search}},
	url = {http://arxiv.org/abs/1806.02932},
	abstract = {Program Synthesis is the task of generating a program from a provided specification. Traditionally, this has been treated as a search problem by the programming languages (PL) community and more recently as a supervised learning problem by the machine learning community. Here, we propose a third approach, representing the task of synthesizing a given program as a Markov decision process solvable via reinforcement learning(RL). From observations about the states of partial programs, we attempt to find a program that is optimal over a provided reward metric on pairs of programs and states. We instantiate this approach on a subset of the RISC-V assembly language operating on floating point numbers, and as an optimization inspired by search-based techniques from the PL community, we combine RL with a priority search tree. We evaluate this instantiation and demonstrate the effectiveness of our combined method compared to a variety of baselines, including a pure RL ablation and a state of the art Markov chain Monte Carlo search method on this task.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Simmons-Edler, Riley and Miltner, Anders and Seung, Sebastian},
	month = jun,
	year = {2018},
	note = {arXiv:1806.02932 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/W6YSBV4A/1806.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/WEL8Z45U/Simmons-Edler et al. - 2018 - Program Synthesis Through Reinforcement Learning G.pdf:application/pdf},
}

@article{kumar_using_nodate,
	title = {Using natural language and program abstractions to instill human inductive biases in machines},
	abstract = {Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more humanlike inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.},
	language = {en},
	author = {Kumar, Sreejan and Correa, Carlos G and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y and Hawkins, Robert D and Daw, Nathaniel D and Cohen, Jonathan D and Narasimhan, Karthik and Griffiths, Thomas L},
	file = {Kumar et al. - Using natural language and program abstractions to.pdf:/Users/ron/Zotero/storage/LKCTKSNX/Kumar et al. - Using natural language and program abstractions to.pdf:application/pdf},
}

@inproceedings{zhang_neural_2018,
	title = {Neural {Guided} {Constraint} {Logic} {Programming} for {Program} {Synthesis}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html},
	abstract = {Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren's internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.},
	urldate = {2023-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Lisa and Rosenblatt, Gregory and Fetaya, Ethan and Liao, Renjie and Byrd, William and Might, Matthew and Urtasun, Raquel and Zemel, Richard},
	year = {2018},
	file = {Full Text PDF:/Users/ron/Zotero/storage/ZL54PXSA/Zhang et al. - 2018 - Neural Guided Constraint Logic Programming for Pro.pdf:application/pdf},
}

@misc{bieber_learning_2020,
	title = {Learning to {Execute} {Programs} with {Instruction} {Pointer} {Attention} {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.12621},
	abstract = {Graph neural networks (GNNs) have emerged as a powerful tool for learning software engineering tasks including code completion, bug finding, and program repair. They benefit from leveraging program structure like control flow graphs, but they are not well-suited to tasks like program execution that require far more sequential reasoning steps than number of GNN propagation steps. Recurrent neural networks (RNNs), on the other hand, are well-suited to long sequential chains of reasoning, but they do not naturally incorporate program structure and generally perform worse on the above tasks. Our aim is to achieve the best of both worlds, and we do so by introducing a novel GNN architecture, the Instruction Pointer Attention Graph Neural Networks (IPA-GNN), which achieves improved systematic generalization on the task of learning to execute programs using control flow graphs. The model arises by considering RNNs operating on program traces with branch decisions as latent variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN model or as a GNN variant more tailored to execution. To test the models, we propose evaluating systematic generalization on learning to execute using control flow graphs, which tests sequential reasoning and use of program structure. More practically, we evaluate these models on the task of learning to execute partial programs, as might arise if using the model as a heuristic function in program synthesis. Results show that the IPA-GNN outperforms a variety of RNN and GNN baselines on both tasks.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Bieber, David and Sutton, Charles and Larochelle, Hugo and Tarlow, Daniel},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12621 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/HKW2UBN6/2010.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/KTYGA8YG/Bieber et al. - 2020 - Learning to Execute Programs with Instruction Poin.pdf:application/pdf},
}

@inproceedings{yang_program_2021,
	title = {Program {Synthesis} {Guided} {Reinforcement} {Learning} for {Partially} {Observed} {Environments}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/f7e2b2b75b04175610e5a00c1e221ebb-Abstract.html},
	abstract = {A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. However, these approaches impose a high manual burden on the user since they must provide a guiding program for every new task. Partially observed environments further complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible configuration of the hidden regions of the environment. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncertainty. In our experiments, we show that our approach significantly outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent. Our results demonstrate that our approach can obtain the benefits of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.},
	urldate = {2023-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Yichen and Inala, Jeevana Priya and Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando and Rinard, Martin},
	year = {2021},
	pages = {29669--29683},
	file = {Full Text PDF:/Users/ron/Zotero/storage/GQ5DEEV7/Yang et al. - 2021 - Program Synthesis Guided Reinforcement Learning fo.pdf:application/pdf},
}

@misc{li_scallop_2023,
	title = {Scallop: {A} {Language} for {Neurosymbolic} {Programming}},
	shorttitle = {Scallop},
	url = {http://arxiv.org/abs/2304.04812},
	abstract = {We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Li, Ziyang and Huang, Jiani and Naik, Mayur},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04812 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/84AX2RE4/2304.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/JKMYTWZD/Li et al. - 2023 - Scallop A Language for Neurosymbolic Programming.pdf:application/pdf},
}

@article{qiu_programmatic_2022,
	title = {{PROGRAMMATIC} {REINFORCEMENT} {LEARNING} {WITHOUT} {ORACLES}},
	abstract = {Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difﬁculty of identifying how the model’s control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our ﬁrst contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space deﬁned by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efﬁcient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable.},
	language = {en},
	author = {Qiu, Wenjie and Zhu, He},
	year = {2022},
	file = {Qiu and Zhu - 2022 - PROGRAMMATIC REINFORCEMENT LEARNING WITHOUT ORACLE.pdf:/Users/ron/Zotero/storage/NM3SBVJJ/Qiu and Zhu - 2022 - PROGRAMMATIC REINFORCEMENT LEARNING WITHOUT ORACLE.pdf:application/pdf},
}

@article{seger_category_2010,
	title = {Category {Learning} in the {Brain}},
	volume = {33},
	issn = {0147-006X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3709834/},
	doi = {10.1146/annurev.neuro.051508.135546},
	abstract = {The ability to group items and events into functional categories is a fundamental characteristic of sophisticated thought. It is subserved by plasticity in many neural systems, including neocortical regions (sensory, prefrontal, parietal, and motor cortex), the medial temporal lobe, the basal ganglia, and midbrain dopaminergic systems. These systems interact during category learning. Corticostriatal loops may mediate recursive, bootstrapping interactions between fast reward-gated plasticity in the basal ganglia and slow reward-shaded plasticity in the cortex. This can provide a balance between acquisition of details of experiences and generalization across them. Interactions between the corticostriatal loops can integrate perceptual, response, and feedback-related aspects of the task and mediate the shift from novice to skilled performance. The basal ganglia and medial temporal lobe interact competitively or cooperatively, depending on the demands of the learning task.},
	urldate = {2023-05-08},
	journal = {Annual review of neuroscience},
	author = {Seger, Carol A. and Miller, Earl K.},
	year = {2010},
	pmid = {20572771},
	pmcid = {PMC3709834},
	pages = {203--219},
	file = {PubMed Central Full Text PDF:/Users/ron/Zotero/storage/KNL3WNFL/Seger and Miller - 2010 - Category Learning in the Brain.pdf:application/pdf},
}

@article{nikolic_practopoiesis_2015,
	title = {Practopoiesis: {Or} how life fosters a mind},
	volume = {373},
	issn = {0022-5193},
	shorttitle = {Practopoiesis},
	url = {https://www.sciencedirect.com/science/article/pii/S002251931500106X},
	doi = {10.1016/j.jtbi.2015.03.003},
	abstract = {The mind is a biological phenomenon. Thus, biological principles of organization should also be the principles underlying mental operations. Practopoiesis states that the key for achieving intelligence through adaptation is an arrangement in which mechanisms laying at a lower level of organization, by their operations and interaction with the environment, enable creation of mechanisms laying at a higher level of organization. When such an organizational advance of a system occurs, it is called a traverse. A case of traverse is when plasticity mechanisms (at a lower level of organization), by their operations, create a neural network anatomy (at a higher level of organization). Another case is the actual production of behavior by that network, whereby the mechanisms of neuronal activity operate to create motor actions. Practopoietic theory explains why the adaptability of a system increases with each increase in the number of traverses. With a larger number of traverses, a system can be relatively small and yet, produce a higher degree of adaptive/intelligent behavior than a system with a lower number of traverses. The present analyses indicate that the two well-known traverses – neural plasticity and neural activity – are not sufficient to explain human mental capabilities. At least one additional traverse is needed, which is named anapoiesis for its contribution in reconstructing knowledge e.g., from long-term memory into working memory. The conclusions bear implications for brain theory, the mind–body explanatory gap, and developments of artificial intelligence technologies.},
	language = {en},
	urldate = {2023-05-08},
	journal = {Journal of Theoretical Biology},
	author = {Nikolić, Danko},
	month = may,
	year = {2015},
	keywords = {Adaptive systems, Cybernetics, Intelligent behavior, Neurophysiology},
	pages = {40--61},
	file = {ScienceDirect Full Text PDF:/Users/ron/Zotero/storage/2LA8E5TI/Nikolić - 2015 - Practopoiesis Or how life fosters a mind.pdf:application/pdf;ScienceDirect Snapshot:/Users/ron/Zotero/storage/4BXZN64Q/S002251931500106X.html:text/html},
}

@article{zeithamova_brain_2019,
	title = {Brain {Mechanisms} of {Concept} {Learning}},
	volume = {39},
	copyright = {Copyright © 2019 the authors},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/39/42/8259},
	doi = {10.1523/JNEUROSCI.1166-19.2019},
	abstract = {Concept learning, the ability to extract commonalities and highlight distinctions across a set of related experiences to build organized knowledge, is a critical aspect of cognition. Previous reviews have focused on concept learning research as a means for dissociating multiple brain systems. The current review surveys recent work that uses novel analytical approaches, including the combination of computational modeling with neural measures, focused on testing theories of specific computations and representations that contribute to concept learning. We discuss in detail the roles of the hippocampus, ventromedial prefrontal, lateral prefrontal, and lateral parietal cortices, and how their engagement is modulated by the coherence of experiences and the current learning goals. We conclude that the interaction of multiple brain systems relating to learning, memory, attention, perception, and reward support a flexible concept-learning mechanism that adapts to a range of category structures and incorporates motivational states, making concept learning a fruitful research domain for understanding the neural dynamics underlying complex behaviors.},
	language = {en},
	number = {42},
	urldate = {2023-05-08},
	journal = {Journal of Neuroscience},
	author = {Zeithamova, Dagmar and Mack, Michael L. and Braunlich, Kurt and Davis, Tyler and Seger, Carol A. and Kesteren, Marlieke T. R. van and Wutz, Andreas},
	month = oct,
	year = {2019},
	pmid = {31619495},
	note = {Publisher: Society for Neuroscience
Section: Symposium and Mini-Symposium},
	keywords = {categorization, computational modeling, fMRI, hippocampus, parietal cortex, prefrontal cortex},
	pages = {8259--8266},
	file = {Full Text PDF:/Users/ron/Zotero/storage/BL956KEY/Zeithamova et al. - 2019 - Brain Mechanisms of Concept Learning.pdf:application/pdf},
}

@misc{bunel_leveraging_2018,
	title = {Leveraging {Grammar} and {Reinforcement} {Learning} for {Neural} {Program} {Synthesis}},
	url = {http://arxiv.org/abs/1805.04276},
	abstract = {Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Bunel, Rudy and Hausknecht, Matthew and Devlin, Jacob and Singh, Rishabh and Kohli, Pushmeet},
	month = may,
	year = {2018},
	note = {arXiv:1805.04276 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/528X78CL/1805.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/AB3QCL83/Bunel et al. - 2018 - Leveraging Grammar and Reinforcement Learning for .pdf:application/pdf},
}

@incollection{kim_2_2022,
	title = {2 {The} {Apperception} {Engine}},
	isbn = {978-3-11-070661-1},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110706611-002/html},
	abstract = {This paper describes an attempt to repurpose Kant’s a priori psychology as the architectural blueprint for a machine learning system. First, it describes the conditions that must be satisfied for the agent to achieve unity of experience: the intuitions must be connected, via binary relations, so as to satisfy various unity conditions. Second, it shows how the categories are derived within this model: the categories are pure unary predicates that are derived from the pure binary relations. Third, I describe how Kant’s cognitive architecture has been implemented in a computer system (the Apperception Engine) and show in detail what it is like for the system to construct a unified experience from a sequence of raw sensory input.},
	language = {en},
	urldate = {2023-05-08},
	booktitle = {Kant and {Artificial} {Intelligence}},
	publisher = {De Gruyter},
	author = {Evans, Richard},
	editor = {Kim, Hyeongjoo and Schönecker, Dieter},
	month = apr,
	year = {2022},
	doi = {10.1515/9783110706611-002},
	pages = {39--104},
	file = {Evans - 2022 - 2 The Apperception Engine.pdf:/Users/ron/Zotero/storage/2EH5EH6F/Evans - 2022 - 2 The Apperception Engine.pdf:application/pdf},
}

@inproceedings{kim_compound_2019,
	address = {Florence, Italy},
	title = {Compound {Probabilistic} {Context}-{Free} {Grammars} for {Grammar} {Induction}},
	url = {https://aclanthology.org/P19-1228},
	doi = {10.18653/v1/P19-1228},
	abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.},
	urldate = {2023-05-08},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
	month = jul,
	year = {2019},
	pages = {2369--2385},
	file = {Full Text PDF:/Users/ron/Zotero/storage/6TLEYY57/Kim et al. - 2019 - Compound Probabilistic Context-Free Grammars for G.pdf:application/pdf},
}

@inproceedings{mao_grammar-based_2021,
	title = {Grammar-{Based} {Grounded} {Lexicon} {Learning}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/4158f6d19559955bae372bb00f6204e4-Abstract.html},
	urldate = {2023-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mao, Jiayuan and Shi, Freda and Wu, Jiajun and Levy, Roger and Tenenbaum, Josh},
	year = {2021},
	pages = {7865--7878},
	file = {Full Text PDF:/Users/ron/Zotero/storage/CNTJJ5YJ/Mao et al. - 2021 - Grammar-Based Grounded Lexicon Learning.pdf:application/pdf},
}

@article{piccinini_mind_2010,
	title = {The {Mind} as {Neural} {Software}? {Understanding} {Functionalism}, {Computationalism}, and {Computational} {Functionalism}},
	volume = {81},
	issn = {1933-1592},
	shorttitle = {The {Mind} as {Neural} {Software}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1933-1592.2010.00356.x},
	doi = {10.1111/j.1933-1592.2010.00356.x},
	abstract = {Defending or attacking either functionalism or computationalism requires clarity on what they amount to and what evidence counts for or against them. My goal here is not to evaluate their plausibility. My goal is to formulate them and their relationship clearly enough that we can determine which type of evidence is relevant to them. I aim to dispel some sources of confusion that surround functionalism and computationalism, recruit recent philosophical work on mechanisms and computation to shed light on them, and clarify how functionalism and computationalism may or may not legitimately come together.},
	language = {en},
	number = {2},
	urldate = {2023-05-09},
	journal = {Philosophy and Phenomenological Research},
	author = {Piccinini, Gualtiero},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1933-1592.2010.00356.x},
	keywords = {Done},
	pages = {269--311},
	file = {Full Text PDF:/Users/ron/Zotero/storage/AYQA9JFM/Piccinini - 2010 - The Mind as Neural Software Understanding Functio.pdf:application/pdf;Snapshot:/Users/ron/Zotero/storage/BYH3NRX8/j.1933-1592.2010.00356.html:text/html},
}

@misc{piantadosi_meaning_2022,
	title = {Meaning without reference in large language models},
	url = {http://arxiv.org/abs/2208.02957},
	abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
	urldate = {2023-05-10},
	publisher = {arXiv},
	author = {Piantadosi, Steven T. and Hill, Felix},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02957 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/GVJLHDWH/2208.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/SEV26R2Z/Piantadosi and Hill - 2022 - Meaning without reference in large language models.pdf:application/pdf},
}

@article{piccinini_neural_2013,
	title = {Neural {Computation} and the {Computational} {Theory} of {Cognition}},
	volume = {37},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12012},
	doi = {10.1111/cogs.12012},
	abstract = {We begin by distinguishing computationalism from a number of other theses that are sometimes conflated with it. We also distinguish between several important kinds of computation: computation in a generic sense, digital computation, and analog computation. Then, we defend a weak version of computationalism—neural processes are computations in the generic sense. After that, we reject on empirical grounds the common assimilation of neural computation to either analog or digital computation, concluding that neural computation is sui generis. Analog computation requires continuous signals; digital computation requires strings of digits. But current neuroscientific evidence indicates that typical neural signals, such as spike trains, are graded like continuous signals but are constituted by discrete functional elements (spikes); thus, typical neural signals are neither continuous signals nor strings of digits. It follows that neural computation is sui generis. Finally, we highlight three important consequences of a proper understanding of neural computation for the theory of cognition. First, understanding neural computation requires a specially designed mathematical theory (or theories) rather than the mathematical theories of analog or digital computation. Second, several popular views about neural computation turn out to be incorrect. Third, computational theories of cognition that rely on non-neural notions of computation ought to be replaced or reinterpreted in terms of neural computation.},
	language = {en},
	number = {3},
	urldate = {2023-05-11},
	journal = {Cognitive Science},
	author = {Piccinini, Gualtiero and Bahar, Sonya},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12012},
	keywords = {Analog computation, Computational theory of cognition, Digital computation, Neural computation, Spike trains},
	pages = {453--488},
	file = {Full Text PDF:/Users/ron/Zotero/storage/FYE4FDVS/Piccinini and Bahar - 2013 - Neural Computation and the Computational Theory of.pdf:application/pdf;Snapshot:/Users/ron/Zotero/storage/E43CBLGH/cogs.html:text/html},
}

@article{caucheteux_evidence_2023,
	title = {Evidence of a predictive coding hierarchy in the human brain listening to speech},
	volume = {7},
	copyright = {2023 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01516-2},
	doi = {10.1038/s41562-022-01516-2},
	abstract = {Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.},
	language = {en},
	number = {3},
	urldate = {2023-05-12},
	journal = {Nature Human Behaviour},
	author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Rémi},
	month = mar,
	year = {2023},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Language, Computational neuroscience, Computer science},
	pages = {430--441},
	file = {Full Text PDF:/Users/ron/Zotero/storage/QW6KJIF8/Caucheteux et al. - 2023 - Evidence of a predictive coding hierarchy in the h.pdf:application/pdf},
}

@article{ding_cortical_2016,
	title = {Cortical tracking of hierarchical linguistic structures in connected speech},
	volume = {19},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4186},
	doi = {10.1038/nn.4186},
	abstract = {Language consists of a hierarchy of linguistic units: words, phrases and sentences. The authors explore whether and how these abstract linguistic units are represented in the brain during speech comprehension. They find that cortical rhythms track the timescales of these linguistic units, revealing a hierarchy of neural processing timescales underlying internal construction of hierarchical linguistic structure.},
	language = {en},
	number = {1},
	urldate = {2023-05-12},
	journal = {Nature Neuroscience},
	author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
	month = jan,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Language, Psychology, Sensory processing},
	pages = {158--164},
	file = {Accepted Version:/Users/ron/Zotero/storage/XH2ZUBX4/Ding et al. - 2016 - Cortical tracking of hierarchical linguistic struc.pdf:application/pdf},
}

@article{garg_are_nodate,
	title = {Are {Transformers} {All} {That} {Karel} {Needs}?},
	abstract = {Recent works have shown the promise of using neural networks for the task of program synthesis from input-output examples. The Karel dataset has been a benchmark for evaluating program synthesis approaches. Several techniques have been proposed to use neural guided program synthesis with Karel being used as a baseline. Most of these techniques use an LSTM based model for decoding and improve performance by proposing complex algorithmic additions, such as using inferred execution traces, latent execution of partial programs and debugging generated programs. We observe that by changing the base architecture to a transformer based one, speciﬁcally GPT2, we are able to apply simple execution guidance on top to achieve a generalization accurary of 89.64\%, which is within 2.36 percentage points of the current state-of-the-art on Karel which uses ensembling.},
	language = {en},
	author = {Garg, Abhay and Sriraman, Anand and Karande, Kunal Pagarey Shirish},
	file = {Garg et al. - Are Transformers All That Karel Needs.pdf:/Users/ron/Zotero/storage/KJDLK2IH/Garg et al. - Are Transformers All That Karel Needs.pdf:application/pdf},
}

@misc{gauthier_alien_2023,
	title = {Alien {Coding}},
	url = {http://arxiv.org/abs/2301.11479},
	abstract = {We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Gauthier, Thibault and Olšák, Miroslav and Urban, Josef},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11479 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Logic in Computer Science, Mathematics - Number Theory},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/PZJA6LSC/2301.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/4F47IV2E/Gauthier et al. - 2023 - Alien Coding.pdf:application/pdf},
}

@inproceedings{peng_rethinking_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Rethinking {Positional} {Encoding} in {Tree} {Transformer} for {Code} {Representation}},
	url = {https://aclanthology.org/2022.emnlp-main.210},
	abstract = {Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we propose a novel tree Transformer encoding node positions based on our new description method for tree structures.Technically, local and global soft bias shown in previous works is both introduced as positional encodings of our Transformer model.Our model finally outperforms strong baselines on code summarization and completion tasks across two languages, demonstrating our model's effectiveness.Besides, extensive experiments and ablation study shows that combining both local and global paradigms is still helpful in improving model performance. We release our code at https://github.com/AwdHanPeng/TreeTransformer.},
	urldate = {2023-05-12},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Han and Li, Ge and Zhao, Yunfei and Jin, Zhi},
	month = dec,
	year = {2022},
	pages = {3204--3214},
	file = {Full Text PDF:/Users/ron/Zotero/storage/BZWW6KKS/Peng et al. - 2022 - Rethinking Positional Encoding in Tree Transformer.pdf:application/pdf},
}

@misc{fijalkow_scaling_2021,
	title = {Scaling {Neural} {Program} {Synthesis} with {Distribution}-based {Search}},
	url = {http://arxiv.org/abs/2110.12485},
	abstract = {We consider the problem of automatically constructing computer programs from input-output examples. We investigate how to augment probabilistic and neural program synthesis methods with new search algorithms, proposing a framework called distribution-based search. Within this framework, we introduce two new search algorithms: Heap Search, an enumerative method, and SQRT Sampling, a probabilistic method. We prove certain optimality guarantees for both methods, show how they integrate with probabilistic and neural techniques, and demonstrate how they can operate at scale across parallel compute environments. Collectively these findings offer theoretical and applied studies of search algorithms for program synthesis that integrate with recent developments in machine-learned program synthesizers.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Fijalkow, Nathanaël and Lagarde, Guillaume and Matricon, Théo and Ellis, Kevin and Ohlmann, Pierre and Potta, Akarsh},
	month = oct,
	year = {2021},
	note = {arXiv:2110.12485 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/HZ7HYXZT/2110.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/RG9DAQXS/Fijalkow et al. - 2021 - Scaling Neural Program Synthesis with Distribution.pdf:application/pdf},
}

@misc{schmidhuber_algorithmic_2000,
	title = {Algorithmic {Theories} of {Everything}},
	url = {http://arxiv.org/abs/quant-ph/0011122},
	abstract = {The probability distribution P from which the history of our universe is sampled represents a theory of everything or TOE. We assume P is formally describable. Since most (uncountably many) distributions are not, this imposes a strong inductive bias. We show that P(x) is small for any universe x lacking a short description, and study the spectrum of TOEs spanned by two Ps, one reflecting the most compact constructive descriptions, the other the fastest way of computing everything. The former derives from generalizations of traditional computability, Solomonoff's algorithmic probability, Kolmogorov complexity, and objects more random than Chaitin's Omega, the latter from Levin's universal search and a natural resource-oriented postulate: the cumulative prior probability of all x incomputable within time t by this optimal algorithm should be 1/t. Between both Ps we find a universal cumulatively enumerable measure that dominates traditional enumerable measures; any such CEM must assign low probability to any universe lacking a short enumerating program. We derive P-specific consequences for evolving observers, inductive reasoning, quantum physics, philosophy, and the expected duration of our universe.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Schmidhuber, Juergen},
	month = dec,
	year = {2000},
	note = {arXiv:quant-ph/0011122},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computational Complexity, High Energy Physics - Theory, Mathematical Physics, Physics - Computational Physics, Quantum Physics},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/CESJHG3H/0011122.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/RVVZQZFT/Schmidhuber - 2000 - Algorithmic Theories of Everything.pdf:application/pdf},
}

@article{friston_am_2018,
	title = {Am {I} {Self}-{Conscious}? ({Or} {Does} {Self}-{Organization} {Entail} {Self}-{Consciousness}?)},
	volume = {9},
	issn = {1664-1078},
	shorttitle = {Am {I} {Self}-{Conscious}?},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579},
	abstract = {Is self-consciousness necessary for consciousness? The answer is yes. So there you have it—the answer is yes. This was my response to a question I was asked to address in a recent AEON piece (https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference). What follows is based upon the notes for that essay, with a special focus on self-organization, self-evidencing and self-modeling. I will try to substantiate my (polemic) answer from the perspective of a physicist. In brief, the argument goes as follows: if we want to talk about creatures, like ourselves, then we have to identify the characteristic behaviors they must exhibit. This is fairly easy to do by noting that living systems return to a set of attracting states time and time again. Mathematically, this implies the existence of a Lyapunov function that turns out to be model evidence (i.e., self-evidence) in Bayesian statistics or surprise (i.e., self-information) in information theory. This means that all biological processes can be construed as performing some form of inference, from evolution through to conscious processing. If this is the case, at what point do we invoke consciousness? The proposal on offer here is that the mind comes into being when self-evidencing has a temporal thickness or counterfactual depth, which grounds inferences about the consequences of my action. On this view, consciousness is nothing more than inference about my future; namely, the self-evidencing consequences of what I could do.},
	urldate = {2023-05-28},
	journal = {Frontiers in Psychology},
	author = {Friston, Karl},
	year = {2018},
	file = {Full Text PDF:/Users/ron/Zotero/storage/NXVYVMC4/Friston - 2018 - Am I Self-Conscious (Or Does Self-Organization En.pdf:application/pdf},
}

@article{hoel_overfitted_2021-1,
	title = {The overfitted brain: {Dreams} evolved to assist generalization},
	volume = {2},
	issn = {2666-3899},
	shorttitle = {The overfitted brain},
	url = {https://www.cell.com/patterns/abstract/S2666-3899(21)00064-7},
	doi = {10.1016/j.patter.2021.100244},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Understanding of the evolved biological function of sleep has advanced considerably in the past decade. However, no equivalent understanding of dreams has emerged. Contemporary neuroscientific theories often view dreams as epiphenomena, and many of the proposals for their biological function are contradicted by the phenomenology of dreams themselves. Now, the recent advent of deep neural networks (DNNs) has finally provided the novel conceptual framework within which to understand the evolved function of dreams. Notably, all DNNs face the issue of overfitting as they learn, which is when performance on one dataset increases but the network's performance fails to generalize (often measured by the divergence of performance on training versus testing datasets). This ubiquitous problem in DNNs is often solved by modelers via "noise injections" in the form of noisy or corrupted inputs. The goal of this paper is to argue that the brain faces a similar challenge of overfitting and that nightly dreams evolved to combat the brain's overfitting during its daily learning. That is, dreams are a biological mechanism for increasing generalizability via the creation of corrupted sensory inputs from stochastic activity across the hierarchy of neural structures. Sleep loss, specifically dream loss, leads to an overfitted brain that can still memorize and learn but fails to generalize appropriately. Herein this "overfitted brain hypothesis" is explicitly developed and then compared and contrasted with existing contemporary neuroscientific theories of dreams. Existing evidence for the hypothesis is surveyed within both neuroscience and deep learning, and a set of testable predictions is put forward that can be pursued both \textit{in vivo} and \textit{in silico}.{\textless}/p{\textgreater}},
	language = {English},
	number = {5},
	urldate = {2023-05-28},
	journal = {Patterns},
	author = {Hoel, Erik},
	month = may,
	year = {2021},
	pmid = {34036289},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/ron/Zotero/storage/JRLHXYHC/Hoel - 2021 - The overfitted brain Dreams evolved to assist gen.pdf:application/pdf},
}

@misc{noauthor_dreaming_nodate,
	title = {Dreaming and {Narration} {\textbar} the living handbook of narratology},
	url = {https://www-archiv.fdm.uni-hamburg.de/lhn/node/70.html},
	urldate = {2023-05-28},
	file = {Dreaming and Narration | the living handbook of narratology:/Users/ron/Zotero/storage/D5PTQTKY/70.html:text/html},
}

@incollection{huhn_dreaming_2014,
	title = {Dreaming and {Narration}},
	isbn = {978-3-11-031634-6},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110316469.138/html},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {Handbook of {Narratology}},
	publisher = {DE GRUYTER},
	author = {Walsh, Richard},
	editor = {Hühn, Peter and Meister, Jan Christoph and Pier, John and Schmid, Wolf},
	month = aug,
	year = {2014},
	doi = {10.1515/9783110316469.138},
	pages = {138--148},
	file = {Walsh - 2014 - Dreaming and Narration.pdf:/Users/ron/Zotero/storage/ESFTEHWJ/Walsh - 2014 - Dreaming and Narration.pdf:application/pdf},
}

@techreport{ciaunica_nested_2023,
	type = {preprint},
	title = {Nested {Selves}: {Self}-{Organisation} and {Shared} {Markov} {Blankets} in {Prenatal} {Development} in {Humans}},
	shorttitle = {Nested {Selves}},
	url = {https://osf.io/g8q5d},
	abstract = {The immune system is a central component of organismic function in humans. This paper addresses self-organisation of a biological system in relation to — and nested within — an other biological system in pregnancy. Pregnancy constitutes a fundamental state for human embodiment and a key step in the evolution and conservation of our species. While not all humans can be pregnant, our initial state of emerging and growing within another person’s body is universal. Hence, the pregnant state does not concern some individuals, but all individuals. Indeed, the hierarchical relationship in pregnancy reflects an even earlier autopoietic process in the embryo by which the number of individuals in a single blastoderm is dynamically determined by cell-cell interactions. The relationship, and the interactions between the two self-organising systems during pregnancy may play a pivotal role in understanding the nature of biological self-organisation per se in humans. Specifically, we consider the role of the immune system in biological self-organisation in addition to neural/brain systems that furnish us with a sense of self. We examine the complex case of pregnancy, whereby two immune systems need to negotiate exchange of resources and information in order to maintain viable self-regulation of nested systems. We conclude with a proposal for the mechanisms—that scaffold the complex relationship between two selforganising systems in pregnancy—through the lens of the Active Inference, with a focus on shared Markov blankets.},
	language = {en},
	urldate = {2023-05-29},
	institution = {PsyArXiv},
	author = {Ciaunica, Anna and Levin, Michael and Rosas, Fernando E. and Friston, Karl},
	month = may,
	year = {2023},
	doi = {10.31234/osf.io/g8q5d},
	file = {Ciaunica et al. - 2023 - Nested Selves Self-Organisation and Shared Markov.pdf:/Users/ron/Zotero/storage/JWARK5RE/Ciaunica et al. - 2023 - Nested Selves Self-Organisation and Shared Markov.pdf:application/pdf},
}

@misc{wong_word_2023,
	title = {From {Word} {Models} to {World} {Models}: {Translating} from {Natural} {Language} to the {Probabilistic} {Language} of {Thought}},
	shorttitle = {From {Word} {Models} to {World} {Models}},
	url = {http://arxiv.org/abs/2306.12672},
	abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.12672 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Symbolic Computation},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/GMSIGTYF/2306.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/R2PMHZIE/Wong et al. - 2023 - From Word Models to World Models Translating from.pdf:application/pdf},
}

@article{pavlick_symbols_2023,
	title = {Symbols and grounding in large language models},
	volume = {381},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041},
	doi = {10.1098/rsta.2022.0041},
	abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models’ performance on challenging language understanding tasks, this article argues that the answer depends on models’ underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs’ ability (or lack thereof) to offer insights on human language representation and understanding.
            This article is part of a discussion meeting issue ‘Cognitive artificial intelligence’.},
	language = {en},
	number = {2251},
	urldate = {2023-07-11},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Pavlick, Ellie},
	month = jul,
	year = {2023},
	pages = {20220041},
	file = {Full Text PDF:/Users/ron/Zotero/storage/Y3X73RUU/Pavlick - 2023 - Symbols and grounding in large language models.pdf:application/pdf},
}

@article{piccinini_situated_2022,
	title = {Situated {Neural} {Representations}: {Solving} the {Problems} of {Content}},
	volume = {16},
	issn = {1662-5218},
	shorttitle = {Situated {Neural} {Representations}},
	url = {https://www.frontiersin.org/articles/10.3389/fnbot.2022.846979},
	abstract = {Situated approaches to cognition maintain that cognition is embodied, embedded, enactive, and affective (and extended, but that is not relevant here). Situated approaches are often pitched as alternatives to computational and representational approaches, according to which cognition is computation over representations. I argue that, far from being opposites, situatedness and neural representation are more deeply intertwined than anyone suspected. To show this, I introduce a neurocomputational account of cognition that relies on neural representations. I argue not only that this account is compatible with (non-question-begging) situated approaches, but also that it requires embodiment, embeddedness, enaction, and affect at its very core. That is, constructing neural representations and their semantic content, and learning computational processes appropriate for their content, requires a tight dynamic interaction between nervous system, body, and environment. Most importantly, I argue that situatedness is needed to give a satisfactory account of neural representation: neurocognitive systems that are embodied, embedded, affective, dynamically interact with their environment, and use feedback from their interaction to shape their own representations and computations (1) can construct neural representations with original semantic content, (2) their neural vehicles and the way they are processed are automatically coordinated with their content, (3) such content is causally efficacious, (4) is determinate enough for the system's purposes, (5) represents the distal stimulus, and (6) can misrepresent. This proposal hints at what is needed to build artifacts with some of the basic cognitive capacities possessed by neurocognitive systems.},
	urldate = {2023-07-12},
	journal = {Frontiers in Neurorobotics},
	author = {Piccinini, Gualtiero},
	year = {2022},
	file = {Full Text PDF:/Users/ron/Zotero/storage/INP8WSHM/Piccinini - 2022 - Situated Neural Representations Solving the Probl.pdf:application/pdf},
}

@article{pavlick_symbols_2023-1,
	title = {Symbols and grounding in large language models},
	volume = {381},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041},
	doi = {10.1098/rsta.2022.0041},
	abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models’ performance on challenging language understanding tasks, this article argues that the answer depends on models’ underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs’ ability (or lack thereof) to offer insights on human language representation and understanding.
            This article is part of a discussion meeting issue ‘Cognitive artificial intelligence’.},
	language = {en},
	number = {2251},
	urldate = {2023-07-12},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Pavlick, Ellie},
	month = jul,
	year = {2023},
	pages = {20220041},
	file = {Full Text PDF:/Users/ron/Zotero/storage/5BKZPF4T/Pavlick - 2023 - Symbols and grounding in large language models.pdf:application/pdf},
}

@article{ramstead_generative_2022,
	title = {From {Generative} {Models} to {Generative} {Passages}: {A} {Computational} {Approach} to ({Neuro}) {Phenomenology}},
	volume = {13},
	issn = {1878-5158},
	shorttitle = {From {Generative} {Models} to {Generative} {Passages}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8932094/},
	doi = {10.1007/s13164-021-00604-y},
	abstract = {This paper presents a version of neurophenomenology based on generative modelling techniques developed in computational neuroscience and biology. Our approach can be described as computational phenomenology because it applies methods originally developed in computational modelling to provide a formal model of the descriptions of lived experience in the phenomenological tradition of philosophy (e.g., the work of Edmund Husserl, Maurice Merleau-Ponty, etc.). The first section presents a brief review of the overall project to naturalize phenomenology. The second section presents and evaluates philosophical objections to that project and situates our version of computational phenomenology with respect to these projects. The third section reviews the generative modelling framework. The final section presents our approach in detail. We conclude by discussing how our approach differs from previous attempts to use generative modelling to help understand consciousness. In summary, we describe a version of computational phenomenology which uses generative modelling to construct a computational model of the inferential or interpretive processes that best explain this or that kind of lived experience.},
	number = {4},
	urldate = {2023-07-12},
	journal = {Review of Philosophy and Psychology},
	author = {Ramstead, Maxwell J. D. and Seth, Anil K. and Hesp, Casper and Sandved-Smith, Lars and Mago, Jonas and Lifshitz, Michael and Pagnoni, Giuseppe and Smith, Ryan and Dumas, Guillaume and Lutz, Antoine and Friston, Karl and Constant, Axel},
	year = {2022},
	pmid = {35317021},
	pmcid = {PMC8932094},
	pages = {829--857},
	file = {PubMed Central Full Text PDF:/Users/ron/Zotero/storage/UT6PPX79/Ramstead et al. - 2022 - From Generative Models to Generative Passages A C.pdf:application/pdf},
}

@misc{beckmann_rejecting_2023,
	title = {Rejecting {Cognitivism}: {Computational} {Phenomenology} for {Deep} {Learning}},
	shorttitle = {Rejecting {Cognitivism}},
	url = {http://arxiv.org/abs/2302.09071},
	abstract = {We propose a non-representationalist framework for deep learning relying on a novel method: computational phenomenology, a dialogue between the first-person perspective (relying on phenomenology) and the mechanisms of computational models. We thereby reject the modern cognitivist interpretation of deep learning, according to which artificial neural networks encode representations of external entities. This interpretation mainly relies on neuro-representationalism, a position that combines a strong ontological commitment towards scientific theoretical entities and the idea that the brain operates on symbolic representations of these entities. We proceed as follows: after offering a review of cognitivism and neuro-representationalism in the field of deep learning, we first elaborate a phenomenological critique of these positions; we then sketch out computational phenomenology and distinguish it from existing alternatives; finally we apply this new method to deep learning models trained on specific tasks, in order to formulate a conceptual framework of deep-learning, that allows one to think of artificial neural networks' mechanisms in terms of lived experience.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Beckmann, Pierre and Köstner, Guillaume and Hipólito, Inês},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09071 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/H8IDC3G6/2302.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/FXIKEYDS/Beckmann et al. - 2023 - Rejecting Cognitivism Computational Phenomenology.pdf:application/pdf},
}

@misc{mirchandani_large_2023,
	title = {Large {Language} {Models} as {General} {Pattern} {Machines}},
	url = {http://arxiv.org/abs/2307.04721},
	abstract = {We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
	month = jul,
	year = {2023},
	note = {arXiv:2307.04721 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/2VYR5VRD/2307.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/UI9XHRDA/Mirchandani et al. - 2023 - Large Language Models as General Pattern Machines.pdf:application/pdf},
}

@article{zhang_connecting_2020-1,
	title = {Connecting concepts in the brain by mapping cortical representations of semantic relations},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-15804-w},
	doi = {10.1038/s41467-020-15804-w},
	abstract = {In the brain, the semantic system is thought to store concepts. However, little is known about how it connects different concepts and infers semantic relations. To address this question, we collected hours of functional magnetic resonance imaging data from human subjects listening to natural stories. We developed a predictive model of the voxel-wise response and further applied it to thousands of new words. Our results suggest that both semantic categories and relations are represented by spatially overlapping cortical patterns, instead of anatomically segregated regions. Semantic relations that reflect conceptual progression from concreteness to abstractness are represented by cortical patterns of activation in the default mode network and deactivation in the frontoparietal attention network. We conclude that the human brain uses distributed networks to encode not only concepts but also relationships between concepts. In particular, the default mode network plays a central role in semantic processing for abstraction of concepts.},
	language = {en},
	number = {1},
	urldate = {2023-07-16},
	journal = {Nature Communications},
	author = {Zhang, Yizhen and Han, Kuan and Worth, Robert and Liu, Zhongming},
	month = apr,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Engineering, Neuroscience},
	pages = {1877},
	file = {Full Text PDF:/Users/ron/Zotero/storage/U9IPPB4C/Zhang et al. - 2020 - Connecting concepts in the brain by mapping cortic.pdf:application/pdf},
}

@article{huth_natural_2016,
	title = {Natural speech reveals the semantic maps that tile human cerebral cortex},
	volume = {532},
	copyright = {2016 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature17637},
	doi = {10.1038/nature17637},
	abstract = {The meaning of language is represented in regions of the cerebral cortex collectively known as the ‘semantic system’. However, little of the semantic system has been mapped comprehensively, and the semantic selectivity of most regions is unknown. Here we systematically map semantic selectivity across the cortex using voxel-wise modelling of functional MRI (fMRI) data collected while subjects listened to hours of narrative stories. We show that the semantic system is organized into intricate patterns that seem to be consistent across individuals. We then use a novel generative model to create a detailed semantic atlas. Our results suggest that most areas within the semantic system represent information about specific semantic domains, or groups of related concepts, and our atlas shows which domains are represented in each area. This study demonstrates that data-driven methods—commonplace in studies of human neuroanatomy and functional connectivity—provide a powerful and efficient means for mapping functional representations in the brain.},
	language = {en},
	number = {7600},
	urldate = {2023-07-16},
	journal = {Nature},
	author = {Huth, Alexander G. and de Heer, Wendy A. and Griffiths, Thomas L. and Theunissen, Frédéric E. and Gallant, Jack L.},
	month = apr,
	year = {2016},
	note = {Number: 7600
Publisher: Nature Publishing Group},
	keywords = {Language, Neural encoding},
	pages = {453--458},
	file = {Accepted Version:/Users/ron/Zotero/storage/BWYC32JR/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf:application/pdf},
}

@article{zhang_hippocampal_2023-1,
	title = {Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience},
	volume = {26},
	copyright = {2022 The Author(s)},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-022-01212-4},
	doi = {10.1038/s41593-022-01212-4},
	abstract = {Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the CA1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of CA1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.},
	language = {en},
	number = {1},
	urldate = {2023-07-25},
	journal = {Nature Neuroscience},
	author = {Zhang, Huanqiu and Rich, P. Dylan and Lee, Albert K. and Sharpee, Tatyana O.},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Learning and memory, Neural encoding},
	pages = {131--139},
	file = {Full Text PDF:/Users/ron/Zotero/storage/IYQFNVWL/Zhang et al. - 2023 - Hippocampal spatial representations exhibit a hype.pdf:application/pdf},
}

@misc{yoshuabengioScalingService,
	author = {Yoshua Bengio},
	title = {{Scaling in the service of reasoning \& model-based ML}},
	howpublished = {\url{https://yoshuabengio.org/2023/03/21/scaling-in-the-service-of-reasoning-model-based-ml/}},
	year = {2022},
	note = {Accessed 21-08-2023},
}


@article{mazzaglia_free_2022,
	title = {The {Free} {Energy} {Principle} for {Perception} and {Action}: {A} {Deep} {Learning} {Perspective}},
	volume = {24},
	issn = {1099-4300},
	shorttitle = {The {Free} {Energy} {Principle} for {Perception} and {Action}},
	url = {http://arxiv.org/abs/2207.06415},
	doi = {10.3390/e24020301},
	abstract = {The free energy principle, and its corollary active inference, constitute a bio-inspired theory that assumes biological agents act to remain in a restricted set of preferred states of the world, i.e., they minimize their free energy. Under this principle, biological agents learn a generative model of the world and plan actions in the future that will maintain the agent in an homeostatic state that satisfies its preferences. This framework lends itself to being realized in silico, as it comprehends important aspects that make it computationally affordable, such as variational inference and amortized planning. In this work, we investigate the tool of deep learning to design and realize artificial agents based on active inference, presenting a deep-learning oriented presentation of the free energy principle, surveying works that are relevant in both machine learning and active inference areas, and discussing the design choices that are involved in the implementation process. This manuscript probes newer perspectives for the active inference framework, grounding its theoretical aspects into more pragmatic affairs, offering a practical guide to active inference newcomers and a starting point for deep learning practitioners that would like to investigate implementations of the free energy principle.},
	number = {2},
	urldate = {2023-09-28},
	journal = {Entropy},
	author = {Mazzaglia, Pietro and Verbelen, Tim and Çatal, Ozan and Dhoedt, Bart},
	month = feb,
	year = {2022},
	note = {arXiv:2207.06415 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	pages = {301},
	file = {arXiv.org Snapshot:/Users/ron/Zotero/storage/HH5BHAE8/2207.html:text/html;Full Text PDF:/Users/ron/Zotero/storage/UPXE9BT5/Mazzaglia et al. - 2022 - The Free Energy Principle for Perception and Actio.pdf:application/pdf},
}

@article{Lake_Ullman_Tenenbaum_Gershman_2017, title={Building machines that learn and think like people}, volume={40}, ISSN={0140-525X, 1469-1825}, DOI={10.1017/S0140525X16001837}, abstractNote={Recent progress in artiﬁcial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Speciﬁcally, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.}, journal={Behavioral and Brain Sciences}, author={Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.}, year={2017}, pages={e253}, language={en} }

@article{gentner2011computational,
  title={Computational models of analogy},
  author={Gentner, Dedre and Forbus, Kenneth D},
  journal={Wiley interdisciplinary reviews: cognitive science},
  volume={2},
  number={3},
  pages={266--276},
  year={2011},
  publisher={Wiley Online Library}
}

@article{kokinov2003computational,
  title={Computational models of analogy-making},
  author={Kokinov, Boicho and French, Robert M},
  journal={Encyclopedia of cognitive science},
  volume={1},
  pages={113--118},
  year={2003},
  publisher={Nature Publishing Group London}
}

@article{gentner2001metaphor,
  title={Metaphor is like analogy},
  author={Gentner, Dedre and Bowdle, Brian and Wolff, Phillip and Boronat, Consuelo and others},
  journal={The analogical mind: Perspectives from cognitive science},
  pages={199--253},
  year={2001}
}

@book{boicho2001analogical,
  title={The analogical mind: Perspectives from cognitive science},
  author={Boicho, Dedre Gentner Keith James Holyoak and Kokinov, N},
  year={2001},
  publisher={MIT press}
}

@article{hill2019learning,
  title={Learning to make analogies by contrasting abstract relational structure},
  author={Hill, Felix and Santoro, Adam and Barrett, David GT and Morcos, Ari S and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1902.00120},
  year={2019}
}

@article{gentner1983structure,
  title={Structure-mapping: A theoretical framework for analogy},
  author={Gentner, Dedre},
  journal={Cognitive science},
  volume={7},
  number={2},
  pages={155--170},
  year={1983},
  publisher={Elsevier}
}

@inproceedings{hummel1996lisa,
  title={LISA: A computational model of analogical inference and schema induction},
  author={Hummel, John E and Holyoak, Keith J},
  booktitle={Proceedings of the eighteenth annual conference of the cognitive science society},
  pages={352--357},
  year={1996}
}

@article{kokinov2001integrating,
  title={Integrating memory and reasoning in analogy-making: The AMBR model},
  author={Kokinov, Boicho and Petrov, Alexander},
  journal={The analogical mind: Perspectives from cognitive science},
  pages={59--124},
  year={2001},
  publisher={MIT Press Cambridge, MA}
}

@inproceedings{kokinov1994dual,
  title={The DUAL Cognitive Architecture: A Hybrid Multi-Agent Approach.},
  author={Kokinov, Boicho N},
  booktitle={ECAI},
  pages={203--207},
  year={1994}
}

@article{kotseruba202040,
  title={40 years of cognitive architectures: core cognitive abilities and practical applications},
  author={Kotseruba, Iuliia and Tsotsos, John K},
  journal={Artificial Intelligence Review},
  volume={53},
  number={1},
  pages={17--94},
  year={2020},
  publisher={Springer}
}

@article{blouw2016concepts,
  title={Concepts as semantic pointers: A framework and computational model},
  author={Blouw, Peter and Solodkin, Eugene and Thagard, Paul and Eliasmith, Chris},
  journal={Cognitive science},
  volume={40},
  number={5},
  pages={1128--1162},
  year={2016},
  publisher={Wiley Online Library}
}

@book{eliasmith2013build,
  title={How to build a brain: A neural architecture for biological cognition},
  author={Eliasmith, Chris},
  year={2013},
  publisher={Oxford University Press}
}

@article{crawford2016biologically,
  title={Biologically plausible, human-scale knowledge representation},
  author={Crawford, Eric and Gingerich, Matthew and Eliasmith, Chris},
  journal={Cognitive science},
  volume={40},
  number={4},
  pages={782--821},
  year={2016},
  publisher={Wiley Online Library}
}

@inproceedings{stewart2012spaun,
  title={Spaun: A perception-cognition-action model using spiking neurons},
  author={Stewart, Terrence and Choo, Feng-Xuan and Eliasmith, Chris},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}

 @misc{eliasmith_2013, 
  title={NengoSPA}, 
  url={https://www.nengo.ai/nengo-spa/}, 
  journal={Nengo}, 
  author={Eliasmith, Chris}, 
  year={2013}
} 

@article{stewart2012technical,
  title={A technical overview of the neural engineering framework},
  author={Stewart, Terrence C},
  journal={University of Waterloo},
  year={2012}
}

 @misc{NEF_principles, 
  title={CNRGlab @ UWaterloo: Research}, 
  url={http://compneuro.uwaterloo.ca/research/nef/overview-of-the-nef.html}, 
  journal={CNRGlab @ UWaterloo | Research}, 
  author={Eliasmith, Chris}, 
  year={2013}
} 

@article{liu2014seeing,
  title={Seeing Jesus in toast: neural and behavioral correlates of face pareidolia},
  author={Liu, Jiangang and Li, Jun and Feng, Lu and Li, Ling and Tian, Jie and Lee, Kang},
  journal={Cortex},
  volume={53},
  pages={60--77},
  year={2014},
  publisher={Elsevier}
}

@article{nagao1984framework,
  title={A framework of a mechanical translation between Japanese and English by analogy principle},
  author={Nagao, Makoto},
  journal={Artificial and human intelligence},
  pages={351--354},
  year={1984}
}

@article{kaitz1988reexamination,
  title={A reexamination of newborns' ability to imitate facial expressions.},
  author={Kaitz, Marsha and Meschulach-Sarfaty, Orna and Auerbach, Judith and Eidelman, Arthur},
  journal={Developmental Psychology},
  volume={24},
  number={1},
  pages={3},
  year={1988},
  publisher={American Psychological Association}
}