@book{10.5555/1593511,
  title = {Python 3 Reference Manual},
  author = {Van Rossum, Guido and Drake, Fred L.},
  year = {2009},
  publisher = {{CreateSpace}},
  address = {{Scotts Valley, CA}},
  isbn = {1-4414-1269-7}
}

@misc{acquavivaCommunicatingNaturalPrograms2022,
  title = {Communicating {{Natural Programs}} to {{Humans}} and {{Machines}}},
  author = {Acquaviva, Samuel and Pu, Yewen and Kryven, Marta and Sechopoulos, Theodoros and Wong, Catherine and Ecanow, Gabrielle E. and Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B.},
  year = {2022},
  month = oct,
  number = {arXiv:2106.07824},
  eprint = {2106.07824},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-08},
  abstract = {The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of {\textbackslash}emph\{language\}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the {\textbackslash}textit\{Language-complete ARC\}: a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88{\textbackslash}\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ron/Zotero/storage/27HK5RZ2/Acquaviva et al. - 2022 - Communicating Natural Programs to Humans and Machi.pdf;/Users/ron/Zotero/storage/9I8NF4E8/2106.html}
}

@article{akhlaghpourRNAbasedTheoryNatural2022,
  title = {An {{RNA-based}} Theory of Natural Universal Computation},
  author = {Akhlaghpour, Hessameddin},
  year = {2022},
  month = mar,
  journal = {Journal of Theoretical Biology},
  volume = {537},
  pages = {110984},
  issn = {0022-5193},
  urldate = {2023-05-08},
  abstract = {Life is confronted with computation problems in a variety of domains including animal behavior, single-cell behavior, and embryonic development. Yet we currently do not know of a naturally existing biological system that is capable of universal computation, i.e., Turing-equivalent in scope. Generic finite-dimensional dynamical systems (which encompass most models of neural networks, intracellular signaling cascades, and gene regulatory networks) fall short of universal computation, but are assumed to be capable of explaining cognition and development. I present a class of models that bridge two concepts from distant fields: combinatory logic (or, equivalently, lambda calculus) and RNA molecular biology. A set of basic RNA editing rules can make it possible to compute any computable function with identical algorithmic complexity to that of Turing machines. The models do not assume extraordinarily complex molecular machinery or any processes that radically differ from what we already know to occur in cells. Distinct independent enzymes can mediate each of the rules and RNA molecules solve the problem of parenthesis matching through their secondary structure. In the most plausible of these models all of the editing rules can be implemented with merely cleavage and ligation operations at fixed positions relative to predefined motifs. This demonstrates that universal computation is well within the reach of molecular biology. It is therefore reasonable to assume that life has evolved {\textendash} or possibly began with {\textendash} a universal computer that yet remains to be discovered. The variety of seemingly unrelated computational problems across many scales can potentially be solved using the same RNA-based computation system. Experimental validation of this theory may immensely impact our understanding of memory, cognition, development, disease, evolution, and the early stages of life.},
  langid = {english},
  keywords = {Church-Turing thesis,Combinatory logic,Junk DNA,Lambda calculus,Molecular engram,RNA,Secondary structure,Turing-completeness,Turing-equivalence,Universal computation},
  file = {/Users/ron/Zotero/storage/GIVX9YGL/Akhlaghpour - 2022 - An RNA-based theory of natural universal computati.pdf;/Users/ron/Zotero/storage/EF326MVB/S0022519321004045.html}
}

@inproceedings{alakaHierarchicalSemanticWave2023,
  title = {Hierarchical {{Semantic Wave Function Collapse}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on the {{Foundations}} of {{Digital Games}}},
  author = {Alaka, Shaad and Bidarra, Rafael},
  year = {2023},
  month = apr,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Lisbon Portugal}},
  urldate = {2023-09-06},
  abstract = {There are few proposals to improve the interactivity and control of wave function collapse (WFC) in a mixed-initiative setting. Moreover, most WFC algorithm variants operate on an simple, unstructured set of tiles. This limitation on the level of control provided to designers hampers their creative work in various ways. We propose Hierarchical Semantic WFC, a generalized approach to WFC that organizes its tile-set into a hierarchy akin to a taxonomy induced by the relation `consists-of'. In such a hierarchical structure, abstract tiles (i.e. non-leaf nodes) can represent the first sketchy intentions of a designer (e.g. forest, urban, desert,...) This allows a designer to interactively collapse a given area into abstract tiles, while subsequently, (and repeatedly, if desired) WFC can resolve each area into a variety of particular instances, by further collapsing it into (a valid combination of) its children tiles (whether leaves or not). We identify how this subtle tile-set change affects the whole WFC algorithm, describe a number of novel exploratory and interactive functions that this enables, and showcase these with a variety of examples generated with our prototype implementation. We conclude that these new mixed-initiative content generation methods can considerably reduce design iteration times and improve the assistance given to designers in expressing their creative intent.},
  isbn = {978-1-4503-9855-8},
  langid = {english},
  file = {/Users/ron/Zotero/storage/GKKA7TVF/Alaka and Bidarra - 2023 - Hierarchical Semantic Wave Function Collapse.pdf}
}

@misc{alfordNeuralguidedBidirectionalProgram2021,
  title = {Neural-Guided, {{Bidirectional Program Search}} for {{Abstraction}} and {{Reasoning}}},
  author = {Alford, Simon and Gandhi, Anshula and Rangamani, Akshay and Banburski, Andrzej and Wang, Tony and Dandekar, Sylee and Chin, John and Poggio, Tomaso and Chin, Peter},
  year = {2021},
  month = oct,
  number = {arXiv:2110.11536},
  eprint = {2110.11536},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-09},
  abstract = {One of the challenges facing artificial intelligence research today is designing systems capable of utilizing systematic reasoning to generalize to new tasks. The Abstraction and Reasoning Corpus (ARC) measures such a capability through a set of visual reasoning tasks. In this paper we report incremental progress on ARC and lay the foundations for two approaches to abstraction and reasoning not based in brute-force search. We first apply an existing program synthesis system called DreamCoder to create symbolic abstractions out of tasks solved so far, and show how it enables solving of progressively more challenging ARC tasks. Second, we design a reasoning algorithm motivated by the way humans approach ARC. Our algorithm constructs a search graph and reasons over this graph structure to discover task solutions. More specifically, we extend existing execution-guided program synthesis approaches with deductive reasoning based on function inverse semantics to enable a neural-guided bidirectional search algorithm. We demonstrate the effectiveness of the algorithm on three domains: ARC, 24-Game tasks, and a 'double-and-add' arithmetic puzzle.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/XTSXRHR6/Alford et al. - 2021 - Neural-guided, Bidirectional Program Search for Ab.pdf;/Users/ron/Zotero/storage/WZ7FVWZD/2110.html}
}

@article{allamanis_learning_2018,
  title = {Learning to Represent Programs with Graphs},
  author = {Allamanis, Miltiadis and Khademi, Mahmoud and Brockschmidt, Marc},
  year = {2018},
  abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/WU7Q6I6D/Allamanis et al. - 2018 - LEARNING TO REPRESENT PROGRAMS WITH GRAPHS.pdf}
}

@article{alroumiMentalCompressionSpatial2021,
  title = {Mental Compression of Spatial Sequences in Human Working Memory Using Numerical and Geometrical Primitives},
  author = {Al Roumi, Fosca and Marti, S{\'e}bastien and Wang, Liping and Amalric, Marie and Dehaene, Stanislas},
  year = {2021},
  month = aug,
  journal = {Neuron},
  volume = {109},
  number = {16},
  pages = {2627-2639.e4},
  issn = {0896-6273},
  urldate = {2022-10-29},
  abstract = {How does the human brain store sequences of spatial locations? We propose that each sequence is internally compressed using an abstract, language-like code that captures its numerical and geometrical regularities. We exposed participants to spatial sequences of fixed length but variable regularity while their brain activity was recorded using magneto-encephalography. Using multivariate decoders, each successive location could be decoded from brain signals, and upcoming locations were anticipated prior to their actual onset. Crucially, sequences with lower complexity, defined as the minimal description length provided by the formal language, led to lower error rates and to increased anticipations. Furthermore, neural codes specific to the numerical and geometrical primitives of the postulated language could be detected, both in isolation and within the sequences. These results suggest that the human brain detects sequence regularities at multiple nested levels and uses them to compress long sequences in working memory.},
  langid = {english},
  keywords = {Geometry,Language of Thought,Magnetoencephalography,Memory,Ordinal Knowledge,Primitive Operations,Sequence Processing,Sequence Structure,Syntax},
  file = {/Users/ron/Zotero/storage/QSVLITNP/Al Roumi et al. - 2021 - Mental compression of spatial sequences in human w.pdf;/Users/ron/Zotero/storage/LABPQJCM/S0896627321004244.html}
}

@inproceedings{AndoniKO10,
  title = {Polylogarithmic Approximation for Edit Distance and the Asymmetric Query Complexity},
  booktitle = {51th Annual {{IEEE}} Symposium on Foundations of Computer Science, {{FOCS}} 2010, October 23-26, 2010, Las Vegas, Nevada, {{USA}}},
  author = {Andoni, Alexandr and Krauthgamer, Robert and Onak, Krzysztof},
  year = {2010},
  pages = {377--386},
  publisher = {{IEEE Computer Society}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Sat, 30 Sep 2023 09:41:22 +0200}
}

@misc{arsiwallaMorphospaceConsciousness2018,
  title = {The {{Morphospace}} of {{Consciousness}}},
  author = {Arsiwalla, Xerxes D. and Sole, Ricard and {Moulin-Frier}, Clement and Herreros, Ivan and {Sanchez-Fibla}, Marti and Verschure, Paul},
  year = {2018},
  month = nov,
  number = {arXiv:1705.11190},
  eprint = {1705.11190},
  primaryclass = {cond-mat, physics:physics, q-bio},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {We construct a complexity-based morphospace to study systems-level properties of conscious \& intelligent systems. The axes of this space label 3 complexity types: autonomous, cognitive \& social. Given recent proposals to synthesize consciousness, a generic complexity-based conceptualization provides a useful framework for identifying defining features of conscious \& synthetic systems. Based on current clinical scales of consciousness that measure cognitive awareness and wakefulness, we take a perspective on how contemporary artificially intelligent machines \& synthetically engineered life forms measure on these scales. It turns out that awareness \& wakefulness can be associated to computational \& autonomous complexity respectively. Subsequently, building on insights from cognitive robotics, we examine the function that consciousness serves, \& argue the role of consciousness as an evolutionary game-theoretic strategy. This makes the case for a third type of complexity for describing consciousness: social complexity. Having identified these complexity types, allows for a representation of both, biological \& synthetic systems in a common morphospace. A consequence of this classification is a taxonomy of possible conscious machines. We identify four types of consciousness, based on embodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii) group consciousness (resulting from group interactions), \& (iv) simulated consciousness (embodied by virtual agents within a simulated reality). This taxonomy helps in the investigation of comparative signatures of consciousness across domains, in order to highlight design principles necessary to engineer conscious machines. This is particularly relevant in the light of recent developments at the crossroads of cognitive neuroscience, biomedical engineering, artificial intelligence \& biomimetics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Condensed Matter - Disordered Systems and Neural Networks,Physics - Biological Physics,Quantitative Biology - Neurons and Cognition},
  file = {/Users/ron/Zotero/storage/44B9NLZM/Arsiwalla et al. - 2018 - The Morphospace of Consciousness.pdf;/Users/ron/Zotero/storage/YPAUUA2Y/1705.html}
}

@article{ArtificialIntelligenceFoundations,
  title = {Artificial {{Intelligence}}: {{Foundations}} of {{Computational Agents}}.},
  journal = {Artificial Intelligence},
  langid = {english},
  file = {/Users/ron/Zotero/storage/6L2HXC7C/Artificial Intelligence Foundations of Computatio.pdf}
}

@inproceedings{Assouel_Rodriguez_Taslakian_Vazquez_Bengio_2022,
  title = {Object-Centric Compositional Imagination for Visual Abstract Reasoning},
  author = {Assouel, Rim and Rodriguez, Pau and Taslakian, Perouz and Vazquez, David and Bengio, Yoshua},
  year = {2022},
  month = mar,
  langid = {english},
  annotation = {Abstract Note: Like humans devoid of imagination, current machine learning systems lack the ability to adapt to new, unexpected situations by foreseeing them, which makes them unable to solve new tasks by analogical reasoning. In this work, we introduce a new compositional imagination framework that improves a model's ability to generalize. One of the key components of our framework is object-centric inductive biases that enables models to perceive the environment as a series of objects, properties, and transformations. By composing these key ingredients, it is possible to generate new unseen tasks that, when used to train the model, improve generalization. Experiments on a simplified version of the Abstraction and Reasoning Corpus (ARC) demonstrate the effectiveness of our framework.}
}

@inproceedings{assouelObjectcentricCompositionalImagination2022,
  title = {Object-Centric {{Compositional Imagination}} for {{Visual Abstract Reasoning}}},
  booktitle = {{{ICLR2022 Workshop}} on the {{Elements}} of {{Reasoning}}: {{Objects}}, {{Structure}} and {{Causality}}},
  author = {Assouel, Rim and Rodriguez, Pau and Taslakian, Perouz and Vazquez, David and Bengio, Yoshua},
  year = {2022},
  month = mar,
  urldate = {2024-01-14},
  abstract = {Like humans devoid of imagination, current machine learning systems lack the ability to adapt to new, unexpected situations by foreseeing them, which makes them unable to solve new tasks by analogical reasoning. In this work, we introduce a new compositional imagination framework that improves a model's ability to generalize. One of the key components of our framework is object-centric inductive biases that enables models to perceive the environment as a series of objects, properties, and transformations. By composing these key ingredients, it is possible to generate new unseen tasks that, when used to train the model, improve generalization. Experiments on a simplified version of the Abstraction and Reasoning Corpus (ARC) demonstrate the effectiveness of our framework.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/WXBMFZUB/Assouel et al. - 2022 - Object-centric Compositional Imagination for Visua.pdf}
}

@article{auyespekHyperbolicEmbeddingFinding,
  title = {Hyperbolic {{Embedding}} for {{Finding Syntax}} in {{BERT}}},
  author = {Auyespek, Temirlan and Mach, Thomas and Assylbekov, Zhenisbek},
  abstract = {Recent advances in natural language processing have improved our understanding of what kind of linguistic knowledge is encoded in modern word representations. For example, methods for testing the ability to extract syntax trees from a language model architecture were developed by Hewitt and Manning (2019){\textemdash}they project word vectors into Euclidean subspace in such a way that the corresponding squared Euclidean distance approximates the tree distance between words in the syntax tree. This work proposes a method for assessing whether embedding word representations in hyperbolic space can better reflect the graph structure of syntax trees. We show that the tree distance between words in a syntax tree can be approximated well by the hyperbolic distance between corresponding word vectors.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/MGCIZMYJ/Auyespek et al. - Hyperbolic Embedding for Finding Syntax in BERT.pdf}
}

@article{bach_seven_nodate,
  title = {Seven {{Principles}} of {{Synthetic Intelligence}}},
  author = {Bach, Joscha},
  abstract = {Understanding why the original project of Artificial Intelligence is widely regarded as a failure and has been abandoned even by most of contemporary AI research itself may prove crucial to achieving synthetic intelligence. Here, we take a brief look at some principles that we might consider to be lessons from the past five decades of AI. The author's own AI architecture {\textendash}MicroPsi {\textendash} attempts to contribute to that discussion.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CHI3946N/Bach - Seven Principles of Synthetic Intelligence.pdf}
}

@article{bachSevenPrinciplesSynthetic,
  title = {Seven {{Principles}} of {{Synthetic Intelligence}}},
  author = {Bach, Joscha},
  abstract = {Understanding why the original project of Artificial Intelligence is widely regarded as a failure and has been abandoned even by most of contemporary AI research itself may prove crucial to achieving synthetic intelligence. Here, we take a brief look at some principles that we might consider to be lessons from the past five decades of AI. The author's own AI architecture {\textendash}MicroPsi {\textendash} attempts to contribute to that discussion.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/9GHW79HA/Bach - Seven Principles of Synthetic Intelligence.pdf}
}

@article{ballDanceSemanticPhoenix2021,
  title = {The {{Dance}} of the {{Semantic Phoenix}}: {{Autopoietic Systems}} of {{Meaning}} in {{Finnegans Wake}}},
  shorttitle = {The {{Dance}} of the {{Semantic Phoenix}}},
  author = {Ball, Andrew J.},
  year = {2021},
  journal = {Philosophy and Literature},
  volume = {45},
  number = {1},
  pages = {172--184},
  issn = {1086-329X},
  urldate = {2023-03-14},
  abstract = {In this essay, I propose an interpretation of James Joyce's Finnegans Wake that is informed by cybernetics and systems theory to show that the novel functions as a model of autopoiesis. Drawing in particular from the work of Niklas Luhmann, I argue that the novel and readers' engagement with it demonstrates how we individually and socially produce meaning from the overwhelming complexity of the world. Contrary to those who would dismiss it as meaningless, I show that Finnegans Wake is a complex system that infinitely generates meaning through self-reference and networks of association.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/4VSXDQT8/Ball - 2021 - The Dance of the Semantic Phoenix Autopoietic Sys.pdf}
}

@misc{bardesMCJEPAJointEmbeddingPredictive2023,
  title = {{{MC-JEPA}}: {{A Joint-Embedding Predictive Architecture}} for {{Self-Supervised Learning}} of {{Motion}} and {{Content Features}}},
  shorttitle = {{{MC-JEPA}}},
  author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  year = {2023},
  month = jul,
  number = {arXiv:2307.12698},
  eprint = {2307.12698},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-25},
  abstract = {Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/WJQYJEI6/Bardes et al. - 2023 - MC-JEPA A Joint-Embedding Predictive Architecture.pdf;/Users/ron/Zotero/storage/A6SR6F9J/2307.html}
}

@article{batesModelingHumanIntuitions2019,
  title = {Modeling Human Intuitions about Liquid Flow with Particle-Based Simulation},
  author = {Bates, Christopher J. and Yildirim, Ilker and Tenenbaum, Joshua B. and Battaglia, Peter},
  year = {2019},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {7},
  eprint = {1809.01524},
  primaryclass = {cs, q-bio},
  pages = {e1007210},
  issn = {1553-7358},
  urldate = {2023-05-08},
  abstract = {Humans can easily describe, imagine, and, crucially, predict a wide variety of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking, dripping, draining, trickling, pooling, and pouring--despite tremendous variability in their material and dynamical properties. Here we propose and test a computational model of how people perceive and predict these liquid dynamics, based on coarse approximate simulations of fluids as collections of interacting particles. Our model is analogous to a "game engine in the head", drawing on techniques for interactive simulations (as in video games) that optimize for efficiency and natural appearance rather than physical accuracy. In two behavioral experiments, we found that the model accurately captured people's predictions about how liquids flow among complex solid obstacles, and was significantly better than two alternatives based on simple heuristics and deep neural networks. Our model was also able to explain how people's predictions varied as a function of the liquids' properties (e.g., viscosity and stickiness). Together, the model and empirical results extend the recent proposal that human physical scene understanding for the dynamics of rigid, solid objects can be supported by approximate probabilistic simulation, to the more complex and unexplored domain of fluid dynamics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  file = {/Users/ron/Zotero/storage/YKSZJXK6/Bates et al. - 2019 - Modeling human intuitions about liquid flow with p.pdf;/Users/ron/Zotero/storage/VCGTSSWJ/1809.html}
}

@misc{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  number = {arXiv:1806.01261},
  eprint = {1806.01261},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-03-14},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/4HCM8TBT/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/Users/ron/Zotero/storage/F89M7858/1806.html}
}

@misc{BayesianMechanicsPhysics,
  title = {On {{Bayesian}} Mechanics: A Physics of and by Beliefs {\textbar} {{Interface Focus}}},
  urldate = {2023-12-17},
  howpublished = {https://royalsocietypublishing.org/doi/10.1098/rsfs.2022.0029},
  file = {/Users/ron/Zotero/storage/CY5IUJL4/rsfs.2022.html}
}

@misc{beckmannRejectingCognitivismComputational2023,
  title = {Rejecting {{Cognitivism}}: {{Computational Phenomenology}} for {{Deep Learning}}},
  shorttitle = {Rejecting {{Cognitivism}}},
  author = {Beckmann, Pierre and K{\"o}stner, Guillaume and Hip{\'o}lito, In{\^e}s},
  year = {2023},
  month = feb,
  number = {arXiv:2302.09071},
  eprint = {2302.09071},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-12},
  abstract = {We propose a non-representationalist framework for deep learning relying on a novel method: computational phenomenology, a dialogue between the first-person perspective (relying on phenomenology) and the mechanisms of computational models. We thereby reject the modern cognitivist interpretation of deep learning, according to which artificial neural networks encode representations of external entities. This interpretation mainly relies on neuro-representationalism, a position that combines a strong ontological commitment towards scientific theoretical entities and the idea that the brain operates on symbolic representations of these entities. We proceed as follows: after offering a review of cognitivism and neuro-representationalism in the field of deep learning, we first elaborate a phenomenological critique of these positions; we then sketch out computational phenomenology and distinguish it from existing alternatives; finally we apply this new method to deep learning models trained on specific tasks, in order to formulate a conceptual framework of deep-learning, that allows one to think of artificial neural networks' mechanisms in terms of lived experience.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ron/Zotero/storage/FXIKEYDS/Beckmann et al. - 2023 - Rejecting Cognitivism Computational Phenomenology.pdf;/Users/ron/Zotero/storage/H8IDC3G6/2302.html}
}

@article{BeispielMichaelJoyce,
  title = {{Beispiel 1: Michael Joyce: Afternoon (1987)}},
  langid = {ngerman},
  file = {/Users/ron/Zotero/storage/MFD873XI/Beispiel 1 Michael Joyce Afternoon (1987).pdf}
}

@article{bengio2021deep,
  title = {Deep Learning for {{AI}}},
  author = {Bengio, Yoshua and Lecun, Yann and Hinton, Geoffrey},
  year = {2021},
  journal = {Communications of the ACM},
  volume = {64},
  number = {7},
  pages = {58--65},
  publisher = {{ACM New York, NY, USA}}
}

@article{bengioConsciousnessPrior2019,
  title = {The {{Consciousness Prior}}},
  author = {Bengio, Yoshua},
  year = {2019},
  month = dec,
  journal = {arXiv:1709.08568 [cs, stat]},
  eprint = {1709.08568},
  primaryclass = {cs, stat},
  urldate = {2022-04-11},
  abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/4WD9F3RB/Bengio - 2019 - The Consciousness Prior.pdf;/Users/ron/Zotero/storage/MQ3DLVT6/1709.html}
}

@inproceedings{bengioFlowNetworkBased2021,
  title = {Flow {{Network}} Based {{Generative Models}} for {{Non-Iterative Diverse Candidate Generation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  year = {2021},
  volume = {34},
  pages = {27381--27394},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-13},
  abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation.  Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},
  file = {/Users/ron/Zotero/storage/RXZEKNQV/Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf}
}

@article{bengioGFlowNetFoundations2023,
  title = {{{GFlowNet Foundations}}},
  author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {210},
  pages = {1--55},
  issn = {1533-7928},
  urldate = {2023-09-29},
  abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new local and efficient training objective called detailed balance for the analogy with MCMC. GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, continuous actions and modular energy functions.},
  file = {/Users/ron/Zotero/storage/TDS7ER3X/Bengio et al. - 2023 - GFlowNet Foundations.pdf}
}

@article{berkovich-ohanaHitchhikerGuideNeurophenomenology2020,
  title = {The {{Hitchhiker}}'s {{Guide}} to {{Neurophenomenology}} {\textendash} {{The Case}} of {{Studying Self Boundaries With Meditators}}},
  author = {{Berkovich-Ohana}, Aviva and {Dor-Ziderman}, Yair and Trautwein, Fynn-Mathis and Schweitzer, Yoav and Nave, Ohad and Fulder, Stephen and Ataria, Yochai},
  year = {2020},
  journal = {Frontiers in Psychology},
  volume = {11},
  issn = {1664-1078},
  urldate = {2023-11-17},
  abstract = {This paper is a practical guide to neurophenomenology. Varela's neurophenomenological research program (NRP) aspires to bridge the gap between, and integrate, first-person (1P) and third-person (3P) approaches to understanding the mind. It does so by suggesting a methodological framework allowing these two irreducible phenomenal domains to relate and reciprocally support the investigation of one another. While highly appealing theoretically, neurophenomenology invites researchers to a challenging methodological endeavor. Based on our experience with empirical neurophenomenological implementation, we offer practical clarifications and insights learnt along the way. In the first part of the paper, we outline the theoretical principles of the NRP and briefly present the field of 1P research. We speak to the importance of phenomenological training and outline the utility of cooperating with meditators as skilled participants. We suggest that 1P accounts of subjective experience can be placed on a complexity continuum ranging between thick and thin phenomenology, highlighting the tension and trade-off inherent to the neurophenomenological attempt to naturalize phenomenology. We then outline a typology of bridges, which create mutual constraints between 1P and 3P approaches, and argue for the utility of alternating between the bridges depending on the available experimental resources, domain of interest and level of sought articulation. In the second part of the paper, we demonstrate how the theory can be put into practice by describing a decade of neurophenomenological studies investigating the sense of self with increasing focus on its embodied, and minimal, aspects. These aspects are accessed via the dissolution of the sense-of-boundaries, shedding new light on the multi-dimensionality and flexibility of embodied selfhood. We emphasize the evolving neurophenomenological dialogue, showing how consecutive studies, placed differently on the thin-to-thick 1P continuum, advance the research project by using the bridging principles appropriate for each stage.},
  file = {/Users/ron/Zotero/storage/DKZ64H5Y/Berkovich-Ohana et al. - 2020 - The Hitchhiker’s Guide to Neurophenomenology – The.pdf}
}

@article{berwickPovertyStimulusRevisited2011,
  title = {Poverty of the {{Stimulus Revisited}}},
  author = {Berwick, Robert C. and Pietroski, Paul and Yankama, Beracah and Chomsky, Noam},
  year = {2011},
  journal = {Cognitive Science},
  volume = {35},
  number = {7},
  pages = {1207--1242},
  issn = {1551-6709},
  urldate = {2024-01-23},
  abstract = {A central goal of modern generative grammar has been to discover invariant properties of human languages that reflect ``the innate schematism of mind that is applied to the data of experience'' and that ``might reasonably be attributed to the organism itself as its contribution to the task of the acquisition of knowledge'' (Chomsky, 1971). Candidates for such invariances include the structure dependence of grammatical rules, and in particular, certain constraints on question formation. Various ``poverty of stimulus'' (POS) arguments suggest that these invariances reflect an innate human endowment, as opposed to common experience: Such experience warrants selection of the grammars acquired only if humans assume, a priori, that selectable grammars respect substantive constraints. Recently, several researchers have tried to rebut these POS arguments. In response, we illustrate why POS arguments remain an important source of support for appeal to a priori structure-dependent constraints on the grammars that humans naturally acquire.},
  copyright = {Copyright {\textcopyright} 2011 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Language acquisition,Linguistics,Statistics,Syntax},
  file = {/Users/ron/Zotero/storage/UDN4W8SJ/Berwick et al. - 2011 - Poverty of the Stimulus Revisited.pdf}
}

@book{bhargavaGrokkingAlgorithmsIllustrated2016,
  title = {Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People},
  shorttitle = {Grokking Algorithms},
  author = {Bhargava, Aditya Y.},
  year = {2016},
  publisher = {{Manning}},
  address = {{Shelter Island}},
  abstract = {"Grokking Algorithms is a fully illustrated, friendly guide that teaches you how to apply common algorithms to the practical problems you face every day as a programmer. You'll start with sorting and searching and, as you build up your skills in thinking algorithmically, you'll tackle more complex concerns such as data compression and artificial intelligence. Each carefully presented example includes helpful diagrams and fully annotated code samples in Python."--Publisher's desription},
  isbn = {978-1-61729-223-1},
  langid = {english},
  lccn = {MLCM 2018/41830 (Q)},
  keywords = {Computer algorithms,Computer programming,{Handbooks, manuals, etc}},
  file = {/Users/ron/Zotero/storage/GHBVZBN9/Bhargava - 2016 - Grokking algorithms an illustrated guide for prog.pdf}
}

@misc{bhattaraiTsetlinMachineEmbedding2023,
  title = {Tsetlin {{Machine Embedding}}: {{Representing Words Using Logical Expressions}}},
  shorttitle = {Tsetlin {{Machine Embedding}}},
  author = {Bhattarai, Bimal and Granmo, Ole-Christoffer and Jiao, Lei and Yadav, Rohan and Sharma, Jivitesh},
  year = {2023},
  month = jan,
  number = {arXiv:2301.00709},
  eprint = {2301.00709},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Embedding words in vector space is a fundamental first step in state-of-the-art natural language processing (NLP). Typical NLP solutions employ pre-defined vector representations to improve generalization by co-locating similar words in vector space. For instance, Word2Vec is a self-supervised predictive model that captures the context of words using a neural network. Similarly, GLoVe is a popular unsupervised model incorporating corpus-wide word co-occurrence statistics. Such word embedding has significantly boosted important NLP tasks, including sentiment analysis, document classification, and machine translation. However, the embeddings are dense floating-point vectors, making them expensive to compute and difficult to interpret. In this paper, we instead propose to represent the semantics of words with a few defining words that are related using propositional logic. To produce such logical embeddings, we introduce a Tsetlin Machine-based autoencoder that learns logical clauses self-supervised. The clauses consist of contextual words like "black," "cup," and "hot" to define other words like "coffee," thus being human-understandable. We evaluate our embedding approach on several intrinsic and extrinsic benchmarks, outperforming GLoVe on six classification tasks. Furthermore, we investigate the interpretability of our embedding using the logical representations acquired during training. We also visualize word clusters in vector space, demonstrating how our logical embedding co-locate similar words.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/PTLM8GLL/Bhattarai et al. - 2023 - Tsetlin Machine Embedding Representing Words Usin.pdf;/Users/ron/Zotero/storage/QGRU7ZHJ/2301.html}
}

@misc{bieber_learning_2020,
  title = {Learning to {{Execute Programs}} with {{Instruction Pointer Attention Graph Neural Networks}}},
  author = {Bieber, David and Sutton, Charles and Larochelle, Hugo and Tarlow, Daniel},
  year = {2020},
  month = oct,
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Graph neural networks (GNNs) have emerged as a powerful tool for learning software engineering tasks including code completion, bug finding, and program repair. They benefit from leveraging program structure like control flow graphs, but they are not well-suited to tasks like program execution that require far more sequential reasoning steps than number of GNN propagation steps. Recurrent neural networks (RNNs), on the other hand, are well-suited to long sequential chains of reasoning, but they do not naturally incorporate program structure and generally perform worse on the above tasks. Our aim is to achieve the best of both worlds, and we do so by introducing a novel GNN architecture, the Instruction Pointer Attention Graph Neural Networks (IPA-GNN), which achieves improved systematic generalization on the task of learning to execute programs using control flow graphs. The model arises by considering RNNs operating on program traces with branch decisions as latent variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN model or as a GNN variant more tailored to execution. To test the models, we propose evaluating systematic generalization on learning to execute using control flow graphs, which tests sequential reasoning and use of program structure. More practically, we evaluate these models on the task of learning to execute partial programs, as might arise if using the model as a heuristic function in program synthesis. Results show that the IPA-GNN outperforms a variety of RNN and GNN baselines on both tasks.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/VWDKV8D5/Bieber et al. - 2020 - Learning to Execute Programs with Instruction Poin.pdf;/Users/ron/Zotero/storage/LBFFPPB3/2010.html}
}

@misc{bieberLearningExecutePrograms2020,
  title = {Learning to {{Execute Programs}} with {{Instruction Pointer Attention Graph Neural Networks}}},
  author = {Bieber, David and Sutton, Charles and Larochelle, Hugo and Tarlow, Daniel},
  year = {2020},
  month = oct,
  number = {arXiv:2010.12621},
  eprint = {2010.12621},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Graph neural networks (GNNs) have emerged as a powerful tool for learning software engineering tasks including code completion, bug finding, and program repair. They benefit from leveraging program structure like control flow graphs, but they are not well-suited to tasks like program execution that require far more sequential reasoning steps than number of GNN propagation steps. Recurrent neural networks (RNNs), on the other hand, are well-suited to long sequential chains of reasoning, but they do not naturally incorporate program structure and generally perform worse on the above tasks. Our aim is to achieve the best of both worlds, and we do so by introducing a novel GNN architecture, the Instruction Pointer Attention Graph Neural Networks (IPA-GNN), which achieves improved systematic generalization on the task of learning to execute programs using control flow graphs. The model arises by considering RNNs operating on program traces with branch decisions as latent variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN model or as a GNN variant more tailored to execution. To test the models, we propose evaluating systematic generalization on learning to execute using control flow graphs, which tests sequential reasoning and use of program structure. More practically, we evaluate these models on the task of learning to execute partial programs, as might arise if using the model as a heuristic function in program synthesis. Results show that the IPA-GNN outperforms a variety of RNN and GNN baselines on both tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/KTYGA8YG/Bieber et al. - 2020 - Learning to Execute Programs with Instruction Poin.pdf;/Users/ron/Zotero/storage/HKW2UBN6/2010.html}
}

@article{birleCognitiveFunctionHolarchy2021,
  title = {Cognitive Function: Holarchy or Holacracy?},
  shorttitle = {Cognitive Function},
  author = {Birle, Codruta and Slavoaca, Dana and Balea, Maria and Livint Popa, Livia and Muresanu, Ioana and Stefanescu, Emanuel and Vacaras, Vitalie and Dina, Constantin and Strilciuc, Stefan and Popescu, Bogdan Ovidiu and Muresanu, Dafin F.},
  year = {2021},
  month = jan,
  journal = {Neurological Sciences},
  volume = {42},
  number = {1},
  pages = {89--99},
  issn = {1590-1874, 1590-3478},
  urldate = {2023-03-14},
  abstract = {Cognition is the most complex function of the brain. When exploring the inner workings of cognitive processes, it is crucial to understand the complexity of the brain's dynamics. This paper aims to describe the integrated framework of the cognitive function, seen as the result of organization and interactions between several systems and subsystems. We briefly describe several organizational concepts, spanning from the reductionist hierarchical approach, up to the more dynamic theory of open complex systems. The homeostatic regulation of the mechanisms responsible for cognitive processes is showcased as a dynamic interplay between several anticorrelated mechanisms, which can be found at every level of the brain's organization, from molecular and cellular level to large-scale networks (e.g., excitation-inhibition, long-term plasticity-long-term depression, synchronizationdesynchronization, segregation-integration, order-chaos). We support the hypothesis that cognitive function is the consequence of multiple network interactions, integrating intricate relationships between several systems, in addition to neural circuits.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/RBCJ66JW/Birle et al. - 2021 - Cognitive function holarchy or holacracy.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/ron/Zotero/storage/8G9BVA3G/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{blankWhatAreLarge2023,
  title = {What Are Large Language Models Supposed to Model?},
  author = {Blank, Idan A.},
  year = {2023},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {27},
  number = {11},
  pages = {987--989},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  urldate = {2023-12-12},
  abstract = {{$<$}p{$>$}Do large language models (LLMs) constitute a computational account of how humans process language? And if so, what is the role of (psycho)linguistic theory in understanding the relationship between artificial and biological minds? The answer depends on choosing among several, fundamentally distinct ways of interpreting these models as hypotheses about humans.{$<$}/p{$>$}},
  langid = {english},
  pmid = {37659920},
  file = {/Users/ron/Zotero/storage/3ZFSVAFB/Blank - 2023 - What are large language models supposed to model.pdf}
}

@article{blokpoel2018deep,
  title = {Deep Analogical Inference as the Origin of Hypotheses},
  author = {Blokpoel, Mark and Wareham, {\relax HT} and Haselager, {\relax WFG} and Toni, Ivan and {van Rooij}, {\relax IJEI}},
  year = {2018}
}

@article{blokpoelJournalProblemSolving2018,
  title = {Journal of {{Problem Solving}}},
  author = {Blokpoel, Mark and Wareham, Todd and Haselager, Pim and Toni, Ivan and {van Rooij}, Iris},
  year = {2018},
  volume = {11},
  pages = {24},
  langid = {english},
  file = {/Users/ron/Documents/Artificial Intelligence/Master/1. Year/2. Semester/Theoretical Foundations of Cognitive Agents/Literature/Block IV - Applications/Deep Analogical Inference as the Origin of Hypotheses.pdf}
}

@article{blouw2016concepts,
  title = {Concepts as Semantic Pointers: {{A}} Framework and Computational Model},
  author = {Blouw, Peter and Solodkin, Eugene and Thagard, Paul and Eliasmith, Chris},
  year = {2016},
  journal = {Cognitive science},
  volume = {40},
  number = {5},
  pages = {1128--1162},
  publisher = {{Wiley Online Library}}
}

@book{boicho2001analogical,
  title = {The Analogical Mind: {{Perspectives}} from Cognitive Science},
  author = {Boicho, Dedre Gentner Keith James Holyoak and Kokinov, N},
  year = {2001},
  publisher = {{MIT press}}
}

@article{bongardTherePlentyRoom2023,
  title = {There's {{Plenty}} of {{Room Right Here}}: {{Biological Systems}} as {{Evolved}}, {{Overloaded}}, {{Multi-Scale Machines}}},
  shorttitle = {There's {{Plenty}} of {{Room Right Here}}},
  author = {Bongard, Joshua and Levin, Michael},
  year = {2023},
  month = mar,
  journal = {Biomimetics},
  volume = {8},
  number = {1},
  pages = {110},
  issn = {2313-7673},
  urldate = {2023-11-17},
  abstract = {The applicability of computational models to the biological world is an active topic of debate. We argue that a useful path forward results from abandoning hard boundaries between categories and adopting an observer-dependent, pragmatic view. Such a view dissolves the contingent dichotomies driven by human cognitive biases (e.g., a tendency to oversimplify) and prior technological limitations in favor of a more continuous view, necessitated by the study of evolution, developmental biology, and intelligent machines. Form and function are tightly entwined in nature, and in some cases, in robotics as well. Thus, efforts to re-shape living systems for biomedical or bioengineering purposes require prediction and control of their function at multiple scales. This is challenging for many reasons, one of which is that living systems perform multiple functions in the same place at the same time. We refer to this as ``polycomputing''{\textemdash}the ability of the same substrate to simultaneously compute different things, and make those computational results available to different observers. This ability is an important way in which living things are a kind of computer, but not the familiar, linear, deterministic kind; rather, living things are computers in the broad sense of their computational materials, as reported in the rapidly growing physical computing literature. We argue that an observer-centered framework for the computations performed by evolved and designed systems will improve the understanding of mesoscale events, as it has already done at quantum and relativistic scales. To develop our understanding of how life performs polycomputing, and how it can be convinced to alter one or more of those functions, we can first create technologies that polycompute and learn how to alter their functions. Here, we review examples of biological and technological polycomputing, and develop the idea that the overloading of different functions on the same hardware is an important design principle that helps to understand and build both evolved and designed systems. Learning to hack existing polycomputing substrates, as well as to evolve and design new ones, will have massive impacts on regenerative medicine, robotics, and computer engineering.},
  pmcid = {PMC10046700},
  pmid = {36975340},
  file = {/Users/ron/Zotero/storage/878MQH5J/Bongard and Levin - 2023 - There’s Plenty of Room Right Here Biological Syst.pdf}
}

@article{bosselut2019comet,
  title = {Comet: {{Commonsense}} Transformers for Automatic Knowledge Graph Construction},
  author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  year = {2019},
  journal = {arXiv preprint arXiv:1906.05317},
  eprint = {1906.05317},
  archiveprefix = {arxiv}
}

@article{bottemannePredictiveMindIntroduction2022,
  title = {{[The predictive mind: An introduction to Bayesian Brain Theory]}},
  shorttitle = {{[The predictive mind}},
  author = {Bottemanne, H. and Longuet, Y. and Gauld, C.},
  year = {2022},
  month = aug,
  journal = {L'Encephale},
  volume = {48},
  number = {4},
  pages = {436--444},
  issn = {0013-7006},
  abstract = {The question of how the mind works is at the heart of cognitive science. It aims to understand and explain the complex processes underlying perception, decision-making and learning, three fundamental areas of cognition. Bayesian Brain Theory, a computational approach derived from the principles of Predictive Processing (PP), offers a mechanistic and mathematical formulation of these cognitive processes. This theory assumes that the brain encodes beliefs (probabilistic states) to generate predictions about sensory input, then uses prediction errors to update its beliefs. In this paper, we present an introduction to the fundamentals of Bayesian Brain Theory. We show how this innovative theory hybridizes concepts inherited from the philosophy of mind and experimental data from neuroscience, and how it translates complex cognitive processes such as perception, action, emotion, or belief, or even the psychiatric symptomatology.},
  langid = {fre},
  pmid = {35012898},
  keywords = {Bayes Theorem,Bayesian brain,Bayesianism,Bay{\'e}sianisme,Belief,Belief updating,Brain,Cerveau bay{\'e}sien,Codage pr{\'e}dictif,Cognition,Computational neuroscience,Computational psychiatry,Croyance,Emotions,Humans,Interoception,Mise {\`a} jour des croyances,Neurosciences,Neurosciences computationnelles,Predictive coding,Predictive processing,Traitement pr{\'e}dictif}
}

@misc{bouizegareneNarrativeActiveInference2020,
  title = {Narrative as Active Inference},
  author = {Bouizegarene, Nabil and Ramstead, Maxwell and Constant, Axel and Friston, Karl and Kirmayer, Laurence},
  year = {2020},
  month = jul,
  publisher = {{PsyArXiv}},
  urldate = {2023-03-14},
  abstract = {The ubiquity and importance of narratives in human adaptation has been recognized by many scholars. Research has identified several functions of narratives that are conducive to individuals' well-being and adaptation as well as to coordinated social practices and enculturation. In this paper, we characterize the social and cognitive functions of narratives in terms of the framework of active inference. Active inference depicts the fundamental tendency of living organisms to adapt by creating, updating, and maintaining inferences about their environment. We review the literature on the functions of narratives in identity, event segmentation, episodic memory, future projection, storytelling practices, and enculturation. We then re-cast these functions of narratives in terms of active inference, outlining a parsimonious model that can guide future developments in narrative theory, research, and clinical applications.},
  langid = {american},
  keywords = {Active inference,Cultural affordances,Cultural Psychology,Future projection,Memory,Narratives,Social and Behavioral Sciences,Social and Personality Psychology},
  file = {/Users/ron/Zotero/storage/JHA49DAW/Bouizegarene et al. - 2020 - Narrative as active inference.pdf}
}

@article{bowersTopDownSynthesisLibrary,
  title = {Top-{{Down Synthesis For Library Learning}}},
  author = {Bowers, Matthew and Olausson, Theo X and Wong, Catherine and Grand, Gabriel and Tenenbaum, Joshua B and Ellis, Kevin and {Solar-Lezama}, Armando},
  volume = {1},
  pages = {29},
  abstract = {This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus. We present an implementation of the approach in a tool called Stitch and evaluate it against the state-of-the-art deductive library learning algorithm from DreamCoder [Ellis et al. 2021]. Our evaluation shows that Stitch is 3-4 orders of magnitude faster and uses 2 orders of magnitude less memory while maintaining comparable or better library quality (as measured by compressivity). We also demonstrate Stitch's scalability on corpora containing hundreds of complex programs, which are intractable with prior deductive approaches, and show empirically that it is robust to terminating the search procedure early{\textemdash}further allowing it to scale to challenging datasets by means of early stopping. CCS Concepts: {\textbullet} Software and its engineering {\textrightarrow} Automatic programming.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/ENNCMZAI/Bowers et al. - Top-Down Synthesis For Library Learning.pdf}
}

@misc{bunelLeveragingGrammarReinforcement2018,
  title = {Leveraging {{Grammar}} and {{Reinforcement Learning}} for {{Neural Program Synthesis}}},
  author = {Bunel, Rudy and Hausknecht, Matthew and Devlin, Jacob and Singh, Rishabh and Kohli, Pushmeet},
  year = {2018},
  month = may,
  number = {arXiv:1805.04276},
  eprint = {1805.04276},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/AB3QCL83/Bunel et al. - 2018 - Leveraging Grammar and Reinforcement Learning for .pdf;/Users/ron/Zotero/storage/528X78CL/1805.html}
}

@article{burks1946peirce,
  title = {Peirce's Theory of Abduction},
  author = {Burks, Arthur W},
  year = {1946},
  journal = {Philosophy of science},
  volume = {13},
  number = {4},
  pages = {301--306},
  publisher = {{Williams and Wilkins Co.}}
}

@misc{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08708},
  eprint = {2308.08708},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  urldate = {2023-08-24},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/ron/Zotero/storage/2G2ID6KL/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf;/Users/ron/Zotero/storage/8Y5794ZW/2308.html}
}

@article{camperoLearningAMIGoAdversarially2021,
  title = {Learning with {{AMIGo}}: {{Adversarially Motivated Intrinsic Goals}}},
  shorttitle = {Learning with {{AMIGo}}},
  author = {Campero, Andres and Raileanu, Roberta and K{\"u}ttler, Heinrich and Tenenbaum, Joshua B. and Rockt{\"a}schel, Tim and Grefenstette, Edward},
  year = {2021},
  month = feb,
  journal = {arXiv:2006.12122 [cs, stat]},
  eprint = {2006.12122},
  primaryclass = {cs, stat},
  urldate = {2022-05-03},
  abstract = {A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating -- as form of meta-learning -- a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned "student" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective "constructively adversarial" objective, the teacher learns to propose increasingly challenging -- yet achievable -- goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/7MN9NYWE/Campero et al. - 2021 - Learning with AMIGo Adversarially Motivated Intri.pdf;/Users/ron/Zotero/storage/DEPP4BFU/2006.html}
}

@article{cartuyvelsDiscreteContinuousRepresentations2021,
  title = {Discrete and Continuous Representations and Processing in Deep Learning: {{Looking}} Forward},
  shorttitle = {Discrete and Continuous Representations and Processing in Deep Learning},
  author = {Cartuyvels, Ruben and Spinks, Graham and Moens, Marie-Francine},
  year = {2021},
  journal = {AI Open},
  volume = {2},
  eprint = {2201.01233},
  primaryclass = {cs},
  pages = {143--159},
  issn = {26666510},
  urldate = {2023-05-08},
  abstract = {Discrete and continuous representations of content (e.g., of language or images) have interesting properties to be explored for the understanding of or reasoning with this content by machines. This position paper puts forward our opinion on the role of discrete and continuous representations and their processing in the deep learning field. Current neural network models compute continuous-valued data. Information is compressed into dense, distributed embeddings. By stark contrast, humans use discrete symbols in their communication with language. Such symbols represent a compressed version of the world that derives its meaning from shared contextual information. Additionally, human reasoning involves symbol manipulation at a cognitive level, which facilitates abstract reasoning, the composition of knowledge and understanding, generalization and efficient learning. Motivated by these insights, in this paper we argue that combining discrete and continuous representations and their processing will be essential to build systems that exhibit a general form of intelligence. We suggest and discuss several avenues that could improve current neural networks with the inclusion of discrete elements to combine the advantages of both types of representations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/ron/Zotero/storage/V6SN9229/Cartuyvels et al. - 2021 - Discrete and continuous representations and proces.pdf;/Users/ron/Zotero/storage/7GXRL6ZA/2201.html}
}

@article{caucheteuxEvidencePredictiveCoding2023,
  title = {Evidence of a Predictive Coding Hierarchy in the Human Brain Listening to Speech},
  author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  year = {2023},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {7},
  number = {3},
  pages = {430--441},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  urldate = {2023-05-12},
  abstract = {Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Computer science,Language},
  file = {/Users/ron/Zotero/storage/QW6KJIF8/Caucheteux et al. - 2023 - Evidence of a predictive coding hierarchy in the h.pdf}
}

@misc{cernaAntiunificationGeneralizationSurvey2023,
  title = {Anti-Unification and {{Generalization}}: {{A Survey}}},
  shorttitle = {Anti-Unification and {{Generalization}}},
  author = {Cerna, David M. and Kutsia, Temur},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00277},
  eprint = {2302.00277},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {Anti-unification (AU), also known as generalization, is a fundamental operation used for inductive inference and is the dual operation to unification, an operation at the foundation of theorem proving. Interest in AU from the AI and related communities is growing, but without a systematic study of the concept, nor surveys of existing work, investigations7 often resort to developing application-specific methods that may be covered by existing approaches. We provide the first survey of AU research and its applications, together with a general framework for categorizing existing and future developments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science},
  file = {/Users/ron/Zotero/storage/Y8T3U7GV/Cerna and Kutsia - 2023 - Anti-unification and Generalization A Survey.pdf;/Users/ron/Zotero/storage/EELRQ6SN/2302.html}
}

@misc{cetinHyperbolicDeepReinforcement2022,
  title = {Hyperbolic {{Deep Reinforcement Learning}}},
  author = {Cetin, Edoardo and Chamberlain, Benjamin and Bronstein, Michael and Hunt, Jonathan J.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.01542},
  eprint = {2210.01542},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-07},
  abstract = {We propose a new class of deep reinforcement learning (RL) algorithms that model latent representations in hyperbolic space. Sequential decision-making requires reasoning about the possible future consequences of current behavior. Consequently, capturing the relationship between key evolving features for a given task is conducive to recovering effective policies. To this end, hyperbolic geometry provides deep RL models with a natural basis to precisely encode this inherently hierarchical information. However, applying existing methodologies from the hyperbolic deep learning literature leads to fatal optimization instabilities due to the non-stationarity and variance characterizing RL gradient estimators. Hence, we design a new general method that counteracts such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope future RL research will consider hyperbolic representations as a standard tool.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/FR2XBSAI/Cetin et al. - 2022 - Hyperbolic Deep Reinforcement Learning.pdf;/Users/ron/Zotero/storage/Z3WZ6DRB/2210.html}
}

@article{chalmersComputationalRepresentationalLanguageofThought2023,
  title = {The {{Computational}} and the {{Representational Language-of-Thought Hypotheses}}},
  author = {Chalmers, David J.},
  year = {2023},
  journal = {Behavioral and Brain Sciences},
  volume = {46},
  pages = {e269},
  urldate = {2024-01-23},
  file = {/Users/ron/Zotero/storage/FITY6MX8/Chalmers - 2023 - The Computational and the Representational Languag.pdf}
}

@inproceedings{chanLeniaExpandedUniverse2020,
  title = {Lenia and {{Expanded Universe}}},
  booktitle = {The 2020 {{Conference}} on {{Artificial Life}}},
  author = {Chan, Bert Wang-Chak},
  year = {2020},
  pages = {221--229},
  publisher = {{MIT Press}},
  address = {{Online}},
  urldate = {2024-01-18},
  abstract = {We report experimental extensions of Lenia, a continuous cellular automata family capable of producing lifelike selforganizing autonomous patterns. The rule of Lenia was generalized into higher dimensions, multiple kernels, and multiple channels. The final architecture approaches what can be seen as a recurrent convolutional neural network. Using semiautomatic search e.g. genetic algorithm, we discovered new phenomena like polyhedral symmetries, individuality, selfreplication, emission, growth by ingestion, and saw the emergence of ``virtual eukaryotes'' that possess internal division of labor and type differentiation. We discuss the results in the contexts of biology, artificial life, and artificial intelligence.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/98ZQPBCL/Chan - 2020 - Lenia and Expanded Universe.pdf}
}

@article{chaterProgramsCausalModels2013,
  title = {Programs as {{Causal Models}}: {{Speculations}} on {{Mental Programs}} and {{Mental Representation}}},
  shorttitle = {Programs as {{Causal Models}}},
  author = {Chater, Nick and Oaksford, Mike},
  year = {2013},
  journal = {Cognitive Science},
  volume = {37},
  number = {6},
  pages = {1171--1191},
  issn = {1551-6709},
  urldate = {2024-01-29},
  abstract = {Judea Pearl has argued that counterfactuals and causality are central to intelligence, whether natural or artificial, and has helped create a rich mathematical and computational framework for formally analyzing causality. Here, we draw out connections between these notions and various current issues in cognitive science, including the nature of mental ``programs'' and mental representation. We argue that programs (consisting of algorithms and data structures) have a causal (counterfactual-supporting) structure; these counterfactuals can reveal the nature of mental representations. Programs can also provide a causal model of the external world. Such models are, we suggest, ubiquitous in perception, cognition, and language processing.},
  copyright = {Copyright {\textcopyright} 2013 Cognitive Science Society, Inc.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/FHUQEI57/Chater and Oaksford - 2013 - Programs as Causal Models Speculations on Mental .pdf;/Users/ron/Zotero/storage/EEQ3WE73/cogs.html}
}

@inproceedings{cheYourGANSecretly2020,
  title = {Your {{GAN}} Is {{Secretly}} an {{Energy-based Model}} and {{You Should Use Discriminator Driven Latent Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Che, Tong and ZHANG, Ruixiang and {Sohl-Dickstein}, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
  year = {2020},
  volume = {33},
  pages = {12275--12287},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2024-01-04},
  file = {/Users/ron/Zotero/storage/BSNQ2LCW/Che et al. - 2020 - Your GAN is Secretly an Energy-based Model and You.pdf}
}

@misc{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  number = {arXiv:1911.01547},
  eprint = {1911.01547},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-22},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ron/Zotero/storage/GGGC5PA6/Chollet - 2019 - On the Measure of Intelligence.pdf;/Users/ron/Zotero/storage/KI8UK6H2/1911.html}
}

@article{chomsky1959certain,
  title = {On Certain Formal Properties of Grammars},
  author = {Chomsky, Noam},
  year = {1959},
  journal = {Information and control},
  volume = {2},
  number = {2},
  pages = {137--167},
  publisher = {{Elsevier}}
}

@article{christenCyberneticalConceptsCellular2019,
  title = {Cybernetical {{Concepts}} for {{Cellular Automaton}} and {{Artificial Neural Network Modelling}} and {{Implementation}}},
  author = {Christen, Patrik and Del Fabbro, Olivier},
  year = {2019},
  month = oct,
  journal = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
  eprint = {2001.02037},
  pages = {4124--4130},
  urldate = {2022-05-03},
  abstract = {As a discipline cybernetics has a long and rich history. In its first generation it not only had a worldwide span, in the area of computer modelling, for example, its proponents such as John von Neumann, Stanislaw Ulam, Warren McCulloch and Walter Pitts, also came up with models and methods such as cellular automata and artificial neural networks, which are still the foundation of most modern modelling approaches. At the same time, cybernetics also got the attention of philosophers, such as the Frenchman Gilbert Simondon, who made use of cybernetical concepts in order to establish a metaphysics and a natural philosophy of individuation, giving cybernetics thereby a philosophical interpretation, which he baptised allagmatic. In this paper, we emphasise this allagmatic theory by showing how Simondon's philosophical concepts can be used to formulate a generic computer model or metamodel for complex systems modelling and its implementation in program code, according to generic programming. We also present how the developed allagmatic metamodel is capable of building simple cellular automata and artificial neural networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Other Computer Science},
  file = {/Users/ron/Zotero/storage/5X4KAZA8/Christen and Del Fabbro - 2019 - Cybernetical Concepts for Cellular Automaton and A.pdf;/Users/ron/Zotero/storage/4MMMGGLL/2001.html}
}

@techreport{ciaunicaNestedSelvesSelfOrganisation2023,
  type = {Preprint},
  title = {Nested {{Selves}}: {{Self-Organisation}} and {{Shared Markov Blankets}} in {{Prenatal Development}} in {{Humans}}},
  shorttitle = {Nested {{Selves}}},
  author = {Ciaunica, Anna and Levin, Michael and Rosas, Fernando E. and Friston, Karl},
  year = {2023},
  month = may,
  institution = {{PsyArXiv}},
  urldate = {2023-05-29},
  abstract = {The immune system is a central component of organismic function in humans. This paper addresses self-organisation of a biological system in relation to {\textemdash} and nested within {\textemdash} an other biological system in pregnancy. Pregnancy constitutes a fundamental state for human embodiment and a key step in the evolution and conservation of our species. While not all humans can be pregnant, our initial state of emerging and growing within another person's body is universal. Hence, the pregnant state does not concern some individuals, but all individuals. Indeed, the hierarchical relationship in pregnancy reflects an even earlier autopoietic process in the embryo by which the number of individuals in a single blastoderm is dynamically determined by cell-cell interactions. The relationship, and the interactions between the two self-organising systems during pregnancy may play a pivotal role in understanding the nature of biological self-organisation per se in humans. Specifically, we consider the role of the immune system in biological self-organisation in addition to neural/brain systems that furnish us with a sense of self. We examine the complex case of pregnancy, whereby two immune systems need to negotiate exchange of resources and information in order to maintain viable self-regulation of nested systems. We conclude with a proposal for the mechanisms{\textemdash}that scaffold the complex relationship between two selforganising systems in pregnancy{\textemdash}through the lens of the Active Inference, with a focus on shared Markov blankets.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/JWARK5RE/Ciaunica et al. - 2023 - Nested Selves Self-Organisation and Shared Markov.pdf}
}

@misc{colasAutotelicAgentsIntrinsically2022,
  title = {Autotelic {{Agents}} with {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning}}: A {{Short Survey}}},
  shorttitle = {Autotelic {{Agents}} with {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning}}},
  author = {Colas, C{\'e}dric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  year = {2022},
  month = jul,
  number = {arXiv:2012.09830},
  eprint = {2012.09830},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by \$autotelic\$ \$agents\$: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: \$developmental\$ \$reinforcement\$ \$learning\$. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem -- the \$intrinsically\$ \$motivated\$ \$acquisition\$ \$of\$ \$open\$-\$ended\$ \$repertoires\$ \$of\$ \$skills\$. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/Y5J5KQZC/Colas et al. - 2022 - Autotelic Agents with Intrinsically Motivated Goal.pdf;/Users/ron/Zotero/storage/IP6HTU2U/2012.html}
}

@book{cormenIntroductionAlgorithms2009,
  title = {Introduction to Algorithms},
  author = {Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald Linn and Stein, Clifford},
  year = {2009},
  edition = {Third edition},
  publisher = {{MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  abstract = {I. Foundations. The role of algorithms in computing -- Getting started -- Growth of functions -- Divide-and-conquer -- Probabilistic analysis and randomized algorithms -- II. Sorting and order statistics. Heapsort -- Quicksort -- Sorting in linear time -- Medians and order statistics -- III. Data structures. Elementary data structures -- Hash tables -- Binary search trees -- Red-black trees -- Augmenting data structures -- IV. Advanced design and analysis techniques. Dynamic programming -- Greedy algorithms -- Amortized analysis -- V. Advanced data structures. B-trees -- Fibonacci heaps -- van Emde Boas trees -- Data structures for disjoint sets -- VI. Graph algorithms. Elementary graph algorithms -- Minimum spanning trees -- Single-source shortest paths -- All-pairs shortest paths -- Maximun flow -- VII. Selected topics. Multithreaded algorithms -- Matrix operations -- Linear programming -- Polynomials and the FFT -- Number-theoretic algorithms -- String matching -- Computational geometry -- NP-completeness -- Approximation algorithms -- VIII. Appendix: Mathematical background. Summations -- Sets, etc. -- Counting and probability -- Matrices},
  isbn = {978-0-262-03384-8 978-0-262-53305-8},
  langid = {english},
  file = {/Users/ron/Zotero/storage/WUQXBWKI/Cormen et al. - 2009 - Introduction to algorithms.pdf}
}

@article{crawford2016biologically,
  title = {Biologically Plausible, Human-Scale Knowledge Representation},
  author = {Crawford, Eric and Gingerich, Matthew and Eliasmith, Chris},
  year = {2016},
  journal = {Cognitive science},
  volume = {40},
  number = {4},
  pages = {782--821},
  publisher = {{Wiley Online Library}}
}

@misc{culpTestingGLOMAbility2022,
  title = {Testing {{GLOM}}'s Ability to Infer Wholes from Ambiguous Parts},
  author = {Culp, Laura and Sabour, Sara and Hinton, Geoffrey E.},
  year = {2022},
  month = nov,
  number = {arXiv:2211.16564},
  eprint = {2211.16564},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {The GLOM architecture proposed by Hinton [2021] is a recurrent neural network for parsing an image into a hierarchy of wholes and parts. When a part is ambiguous, GLOM assumes that the ambiguity can be resolved by allowing the part to make multi-modal predictions for the pose and identity of the whole to which it belongs and then using attention to similar predictions coming from other possibly ambiguous parts to settle on a common mode that is predicted by several different parts. In this study, we describe a highly simplified version of GLOM that allows us to assess the effectiveness of this way of dealing with ambiguity. Our results show that, with supervised training, GLOM is able to successfully form islands of very similar embedding vectors for all of the locations occupied by the same object and it is also robust to strong noise injections in the input and to out-of-distribution input transformations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/FIHPBJ5Z/Culp et al. - 2022 - Testing GLOM's ability to infer wholes from ambigu.pdf;/Users/ron/Zotero/storage/KPN5WGFW/2211.html}
}

@article{das2004mean,
  title = {Mean Squared Error of Empirical Predictor},
  author = {Das, Kalyan and Jiang, Jiming and Rao, {\relax JNK}},
  year = {2004}
}

@article{debruijnLambdaCalculusNotation1972,
  title = {Lambda Calculus Notation with Nameless Dummies, a Tool for Automatic Formula Manipulation, with Application to the {{Church-Rosser}} Theorem},
  author = {{de Bruijn}, N. G},
  year = {1972},
  month = jan,
  journal = {Indagationes Mathematicae (Proceedings)},
  volume = {75},
  number = {5},
  pages = {381--392},
  issn = {1385-7258},
  urldate = {2023-12-22},
  abstract = {In ordinary lambda calculus the occurrences of a bound variable are made recognizable by the use of one and the same (otherwise irrelevant) name at all occurrences. This convention is known to cause considerable trouble in cases of substitution. In the present paper a different notational system is developed, where occurrences of variables are indicated by integers giving the ``distance'' to the binding {$\lambda$} instead of a name attached to that {$\lambda$}. The system is claimed to be efficient for automatic formula manipulation as well as for metalingual discussion. As an example the most essential part of a proof of the Church-Rosser theorem is presented in this namefree calculus.},
  file = {/Users/ron/Zotero/storage/PS6QTBPX/de Bruijn - 1972 - Lambda calculus notation with nameless dummies, a .pdf}
}

@misc{defeliceCategoricalToolsNatural2022,
  title = {Categorical {{Tools}} for {{Natural Language Processing}}},
  author = {{de Felice}, Giovanni},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06636},
  eprint = {2212.06636},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  urldate = {2024-01-23},
  abstract = {This thesis develops the translation between category theory and computational linguistics as a foundation for natural language processing. The three chapters deal with syntax, semantics and pragmatics. First, string diagrams provide a unified model of syntactic structures in formal grammars. Second, functors compute semantics by turning diagrams into logical, tensor, neural or quantum computation. Third, the resulting functorial models can be composed to form games where equilibria are the solutions of language processing tasks. This framework is implemented as part of DisCoPy, the Python library for computing with string diagrams. We describe the correspondence between categorical, linguistic and computational structures, and demonstrate their applications in compositional natural language processing.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Mathematics - Category Theory},
  file = {/Users/ron/Zotero/storage/MB236AVE/de Felice - 2022 - Categorical Tools for Natural Language Processing.pdf}
}

@article{dehaeneSymbolsMentalPrograms2022,
  title = {Symbols and Mental Programs: A Hypothesis about Human Singularity},
  shorttitle = {Symbols and Mental Programs},
  author = {Dehaene, Stanislas and Al Roumi, Fosca and Lakretz, Yair and Planton, Samuel and {Sabl{\'e}-Meyer}, Mathias},
  year = {2022},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {26},
  number = {9},
  pages = {751--766},
  issn = {13646613},
  urldate = {2022-08-22},
  langid = {english},
  file = {/Users/ron/Zotero/storage/3QSTG9EM/Dehaene et al. - 2022 - Symbols and mental programs a hypothesis about hu.pdf}
}

@misc{deletangNeuralNetworksChomsky2023,
  title = {Neural {{Networks}} and the {{Chomsky Hierarchy}}},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and {Grau-Moya}, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
  year = {2023},
  month = feb,
  number = {arXiv:2207.02098},
  eprint = {2207.02098},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/828FYZ2L/Delétang et al. - 2023 - Neural Networks and the Chomsky Hierarchy.pdf;/Users/ron/Zotero/storage/8GNSTLFM/2207.html}
}

@misc{deleuBayesianStructureLearning2022,
  title = {Bayesian {{Structure Learning}} with {{Generative Flow Networks}}},
  author = {Deleu, Tristan and G{\'o}is, Ant{\'o}nio and Emezue, Chris and Rankawat, Mansi and {Lacoste-Julien}, Simon and Bauer, Stefan and Bengio, Yoshua},
  year = {2022},
  month = jun,
  number = {arXiv:2202.13903},
  eprint = {2202.13903},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-25},
  abstract = {In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/N9KGWTUX/Deleu et al. - 2022 - Bayesian Structure Learning with Generative Flow N.pdf;/Users/ron/Zotero/storage/UARUUKZQ/2202.html}
}

@misc{deleuGenerativeFlowNetworks2023,
  title = {Generative {{Flow Networks}}: A {{Markov Chain Perspective}}},
  shorttitle = {Generative {{Flow Networks}}},
  author = {Deleu, Tristan and Bengio, Yoshua},
  year = {2023},
  month = jul,
  number = {arXiv:2307.01422},
  eprint = {2307.01422},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-02},
  abstract = {While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allows us to identify the similarities between both frameworks, and most importantly to highlight their differences.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/RFGPQX83/Deleu and Bengio - 2023 - Generative Flow Networks a Markov Chain Perspecti.pdf}
}

@misc{deleuJointBayesianInference2023,
  title = {Joint {{Bayesian Inference}} of {{Graphical Structure}} and {{Parameters}} with a {{Single Generative Flow Network}}},
  author = {Deleu, Tristan and {Nishikawa-Toomey}, Mizu and Subramanian, Jithendaraa and Malkin, Nikolay and Charlin, Laurent and Bengio, Yoshua},
  year = {2023},
  month = oct,
  number = {arXiv:2305.19366},
  eprint = {2305.19366},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-27},
  abstract = {Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized by neural networks. We show that our method, called JSP-GFN, offers an accurate approximation of the joint posterior, while comparing favorably against existing methods on both simulated and real data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/U3FH5AJ5/Deleu et al. - 2023 - Joint Bayesian Inference of Graphical Structure an.pdf;/Users/ron/Zotero/storage/HIX53XBQ/2305.html}
}

@article{dennett2005cognitive,
  title = {Cognitive Wheels: {{The}} Frame Problem of {{AI}}},
  author = {Dennett, Daniel C},
  year = {2005},
  journal = {Language and Thought},
  volume = {3},
  pages = {217},
  publisher = {{The Open University}}
}

@article{denoudenHowPredictionErrors2012,
  title = {How {{Prediction Errors Shape Perception}}, {{Attention}}, and {{Motivation}}},
  author = {Den Ouden, Hanneke and Kok, Peter and De Lange, Floris},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {3},
  issn = {1664-1078},
  urldate = {2024-01-17},
  abstract = {Prediction errors (PE) are a central notion in theoretical models of reinforcement learning, perceptual inference, decision-making and cognition, and prediction error signals have been reported across a wide range of brain regions and experimental paradigms. Here, we will make an attempt to see the forest for the trees and consider the commonalities and differences of reported PE signals in light of recent suggestions that the computation of PE forms a fundamental mode of brain function. We discuss where different types of PE are encoded, how they are generated, and the different functional roles they fulfill. We suggest that while encoding of PE is a common computation across brain regions, the content and function of these error signals can be very different and are determined by the afferent and efferent connections within the neural circuitry in which they arise.},
  file = {/Users/ron/Zotero/storage/NDCITHZ7/Den Ouden et al. - 2012 - How Prediction Errors Shape Perception, Attention,.pdf}
}

@inproceedings{devlinRobustFillNeuralProgram2017,
  title = {{{RobustFill}}: {{Neural Program Learning}} under {{Noisy I}}/{{O}}},
  shorttitle = {{{RobustFill}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
  year = {2017},
  month = jul,
  pages = {990--998},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-12-21},
  abstract = {The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for `automatic program learning' have received significant attention: (1) `neural program synthesis', where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) `neural program induction', where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92{\textbackslash}\% accuracy on a real-world test set, compared to the 34{\textbackslash}\% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CJF32QXQ/Devlin et al. - 2017 - RobustFill Neural Program Learning under Noisy I.pdf}
}

@article{dingCorticalTrackingHierarchical2016,
  title = {Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech},
  author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
  year = {2016},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {19},
  number = {1},
  pages = {158--164},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  urldate = {2023-05-12},
  abstract = {Language consists of a hierarchy of linguistic units: words, phrases and sentences. The authors explore whether and how these abstract linguistic units are represented in the brain during speech comprehension. They find that cortical rhythms track the timescales of these linguistic units, revealing a hierarchy of neural processing timescales underlying internal construction of hierarchical linguistic structure.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Language,Psychology,Sensory processing},
  file = {/Users/ron/Zotero/storage/XH2ZUBX4/Ding et al. - 2016 - Cortical tracking of hierarchical linguistic struc.pdf}
}

@article{ditullioDynamicalSelforganizationEfficient2021,
  title = {Dynamical Self-Organization and Efficient Representation of Space by Grid Cells},
  author = {DiTullio, Ronald W. and Balasubramanian, Vijay},
  year = {2021},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {70},
  pages = {206--213},
  issn = {0959-4388},
  urldate = {2023-05-08},
  abstract = {To plan trajectories and navigate, animals must maintain a mental representation of the environment and their own position within it. This ``cognitive map'' is thought to be supported in part by the entorhinal cortex, where grid cells are active when an animal occupies the vertices of a scaling hierarchy of periodic lattices of locations in an enclosure. Here, we review computational developments which suggest that the grid cell network is: (a) efficient, providing required spatial resolution with a minimum number of neurons, (b) self-organizing, dynamically coordinating the structure and scale of the responses, and (c) adaptive, re-organizing in response to changes in landmarks and the structure of the boundaries of spaces. We consider these ideas in light of recent discoveries of similar structures in the mental representation of abstract spaces of shapes and smells, and in other brain areas, and highlight promising directions for future research.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/6QSY8JJT/DiTullio and Balasubramanian - 2021 - Dynamical self-organization and efficient represen.pdf;/Users/ron/Zotero/storage/KB8VD8C4/S0959438821001343.html}
}

@article{doNeuralCircuitsSymbolic2021,
  title = {Neural Circuits and Symbolic Processing},
  author = {Do, Quan and Hasselmo, Michael E.},
  year = {2021},
  month = dec,
  journal = {Neurobiology of Learning and Memory},
  volume = {186},
  pages = {107552},
  issn = {1074-7427},
  urldate = {2023-05-08},
  abstract = {The ability to use symbols is a defining feature of human intelligence. However, neuroscience has yet to explain the fundamental neural circuit mechanisms for flexibly representing and manipulating abstract concepts. This article will review the research on neural models for symbolic processing. The review first focuses on the question of how symbols could possibly be represented in neural circuits. The review then addresses how neural symbolic representations could be flexibly combined to meet a wide range of reasoning demands. Finally, the review assesses the research on program synthesis and proposes that the most flexible neural representation of symbolic processing would involve the capacity to rapidly synthesize neural operations analogous to lambda calculus to solve complex cognitive tasks.},
  langid = {english},
  keywords = {Bayesian inference,Category theory,Conjunctive coding,Dynamic binding,Program synthesis,Reinforcement learning},
  file = {/Users/ron/Zotero/storage/CBEQBKQH/Do and Hasselmo - 2021 - Neural circuits and symbolic processing.pdf;/Users/ron/Zotero/storage/HLCDM7P8/S107474272100174X.html}
}

@article{duImprovedContrastiveDivergence2021,
  title = {Improved {{Contrastive Divergence Training}} of {{Energy Based Models}}},
  author = {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  year = {2021},
  month = jun,
  journal = {arXiv:2012.01316 [cs]},
  eprint = {2012.01316},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases,such as image generation, OOD detection, and compositional generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/8UNGBQJ3/Du et al. - 2021 - Improved Contrastive Divergence Training of Energy.pdf;/Users/ron/Zotero/storage/WFSJ6VMD/2012.html}
}

@misc{duvalHitchhikerGuideGeometric2023,
  title = {A {{Hitchhiker}}'s {{Guide}} to {{Geometric GNNs}} for {{3D Atomic Systems}}},
  author = {Duval, Alexandre and Mathis, Simon V. and Joshi, Chaitanya K. and Schmidt, Victor and Miret, Santiago and Malliaros, Fragkiskos D. and Cohen, Taco and Lio, Pietro and Bengio, Yoshua and Bronstein, Michael},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07511},
  eprint = {2312.07511},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-17},
  abstract = {Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage -- such as physical symmetries and chemical properties -- to learn informative representations of these geometric graphs. In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems. We cover fundamental background material and introduce a pedagogical taxonomy of Geometric GNN architectures:(1) invariant networks, (2) equivariant networks in Cartesian basis, (3) equivariant networks in spherical basis, and (4) unconstrained networks. Additionally, we outline key datasets and application areas and suggest future research directions. The objective of this work is to present a structured perspective on the field, making it accessible to newcomers and aiding practitioners in gaining an intuition for its mathematical abstractions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/KIJHMT8U/Duval et al. - 2023 - A Hitchhiker's Guide to Geometric GNNs for 3D Atom.pdf;/Users/ron/Zotero/storage/E6UF7SSJ/2312.html}
}

@article{eisensteinNaturalLanguageProcessing,
  title = {Natural {{Language Processing}}},
  author = {Eisenstein, Jacob},
  langid = {english},
  file = {/Users/ron/Zotero/storage/S5DM5DIM/Eisenstein - Natural Language Processing.pdf}
}

@misc{eliasmith_2013,
  title = {{{NengoSPA}}},
  author = {Eliasmith, Chris},
  year = {2013}
}

@book{eliasmithHowBuildBrain2013,
  title = {How to {{Build}} a {{Brain}}: {{A Neural Architecture}} for {{Biological Cognition}}},
  shorttitle = {How to {{Build}} a {{Brain}}},
  author = {Eliasmith, Chris},
  year = {2013},
  month = jun,
  publisher = {{Oxford University Press}},
  urldate = {2024-01-18},
  isbn = {978-0-19-979454-6},
  langid = {english},
  file = {/Users/ron/Zotero/storage/8QLMMZZS/Eliasmith - 2013 - How to Build a Brain A Neural Architecture for Bi.pdf}
}

@article{Ellis_Wong_Nye_Sable-Meyer_Cary_Morales_Hewitt_Solar-Lezama_Tenenbaum,
  title = {{{DreamCoder}}: {{Building}} Interpretable Hierarchical Knowledge Representations with Wake-Sleep {{Bayesian}} Program Learning},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sable-Meyer}, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and {Solar-Lezama}, Armando and Tenenbaum, Joshua B},
  pages = {68},
  langid = {english}
}

@article{ellisAlgorithmsLearningInduce,
  title = {Algorithms for {{Learning}} to {{Induce Programs}}},
  author = {Ellis, Kevin},
  pages = {224},
  abstract = {The future of machine learning should have a knowledge representation that supports, at a minimum, several features: Expressivity, interpretability, the potential for reuse by both humans and machines, while also enabling sample-efficient generalization. Here we argue that programs{\textendash}i.e., source code{\textendash}are a knowledge representation which can contribute to the project of capturing these elements of intelligence. This research direction however requires new program synthesis algorithms which can induce programs solving a range of AI tasks. This program induction challenge confronts two primary obstacles: the space of all programs is infinite, so we need a strong inductive bias or prior to steer us toward the correct programs; and even if we have that prior, effectively searching through the vast combinatorial space of all programs is generally intractable. We introduce algorithms that learn to induce programs, with the goal of addressing these two primary obstacles. Focusing on case studies in vision, computational linguistics, and learning-to-learn, we develop an algorithmic toolkit for learning inductive biases over programs as well as learning to search for programs, drawing on probabilistic, neural, and symbolic methods. Together this toolkit suggests ways in which program induction can contribute to AI, and how we can use learning to improve program synthesis technologies.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/BXXXACWT/Ellis - Algorithms for Learning to Induce Programs.pdf}
}

@inproceedings{ellisDreamCoderBootstrappingInductive2021,
  title = {{{DreamCoder}}: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
  shorttitle = {{{DreamCoder}}},
  booktitle = {Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sabl{\'e}-Meyer}, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and {Solar-Lezama}, Armando and Tenenbaum, Joshua B.},
  year = {2021},
  month = jun,
  pages = {835--850},
  publisher = {{ACM}},
  address = {{Virtual Canada}},
  urldate = {2022-05-04},
  abstract = {We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of {\l}wake-sleep{\v z} approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.},
  isbn = {978-1-4503-8391-2},
  langid = {english},
  file = {/Users/ron/Zotero/storage/IADX3AGN/Ellis et al. - 2021 - DreamCoder bootstrapping inductive program synthe.pdf}
}

@article{ellisDreamCoderBuildingInterpretable,
  title = {{{DreamCoder}}: {{Building}} Interpretable Hierarchical Knowledge Representations with Wake-Sleep {{Bayesian}} Program Learning},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sable-Meyer}, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and {Solar-Lezama}, Armando and Tenenbaum, Joshua B},
  pages = {68},
  langid = {english},
  file = {/Users/ron/Zotero/storage/LSZ8KD7V/Ellis et al. - DreamCoder Building interpretable hierarchical kn.pdf}
}

@article{ellisDreamCoderGrowingGeneralizable2020,
  title = {{{DreamCoder}}: {{Growing}} Generalizable, Interpretable Knowledge with Wake-Sleep {{Bayesian}} Program Learning},
  shorttitle = {{{DreamCoder}}},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sable-Meyer}, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and {Solar-Lezama}, Armando and Tenenbaum, Joshua B.},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.08381 [cs]},
  eprint = {2006.08381},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/S8XHGWRJ/Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf}
}

@article{ellisDreamCoderGrowingGeneralizable2023,
  title = {{{DreamCoder}}: Growing Generalizable, Interpretable Knowledge with Wake{\textendash}Sleep {{Bayesian}} Program Learning},
  shorttitle = {{{DreamCoder}}},
  author = {Ellis, Kevin and Wong, Lionel and Nye, Maxwell and {Sabl{\'e}-Meyer}, Mathias and Cary, Luc and Anaya Pozo, Lore and Hewitt, Luke and {Solar-Lezama}, Armando and Tenenbaum, Joshua B.},
  year = {2023},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {381},
  number = {2251},
  pages = {20220050},
  publisher = {{Royal Society}},
  urldate = {2024-01-12},
  abstract = {Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages{\textemdash}systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating domain-specific programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A `wake{\textendash}sleep' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multilayered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience. This article is part of a discussion meeting issue `Cognitive artificial intelligence'.},
  keywords = {Bayesian program learning,expertise,program synthesis},
  file = {/Users/ron/Zotero/storage/QJSU3QF2/Ellis et al. - 2023 - DreamCoder growing generalizable, interpretable k.pdf}
}

@inproceedings{ellisLearningLibrariesSubroutines2018,
  title = {Learning {{Libraries}} of {{Subroutines}} for {{Neurally}}{\textendash} {{Guided Bayesian Program Induction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ellis, Kevin and Morales, Lucas and {Sabl{\'e}-Meyer}, Mathias and {Solar-Lezama}, Armando and Tenenbaum, Josh},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-12-12},
  abstract = {Successful approaches to program induction require a hand-engineered   domain-specific language (DSL), constraining the space of allowed   programs and imparting prior knowledge of the domain.  We contribute   a program induction algorithm that learns a DSL while   jointly training a neural network to efficiently search for programs   in the learned DSL.  We use our model to synthesize functions on lists,   edit text, and solve symbolic regression problems, showing how the   model learns a domain-specific library of program components for   expressing solutions to problems in the domain.},
  file = {/Users/ron/Zotero/storage/F9XGBE7H/Ellis et al. - 2018 - Learning Libraries of Subroutines for Neurally– Gu.pdf}
}

@article{ellmanReconsiderationMatterWaves,
  title = {A {{Reconsideration}} of {{Matter Waves}}},
  author = {Ellman, Roger},
  abstract = {Matter waves were discovered in the early 20th century from their wavelength, predicted by DeBroglie, Planck's constant divided by the particle's momentum, that is, {$\lambda$}mw = h/m{$\cdot$}v. But, the failure to obtain a reasonable theory for the matter wave frequency resulted somewhat in loss of further interest.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/ZISZJT7W/Ellman - A Reconsideration of Matter Waves.pdf}
}

@article{englandStatisticalPhysicsSelfreplication2013,
  title = {Statistical Physics of Self-Replication},
  author = {England, Jeremy L.},
  year = {2013},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {139},
  number = {12},
  pages = {121923},
  issn = {0021-9606, 1089-7690},
  urldate = {2024-01-18},
  abstract = {Self-replication is a capacity common to every species of living thing, and simple physical intuition dictates that such a process must invariably be fueled by the production of entropy. Here, we undertake to make this intuition rigorous and quantitative by deriving a lower bound for the amount of heat that is produced during a process of self-replication in a system coupled to a thermal bath. We find that the minimum value for the physically allowed rate of heat production is determined by the growth rate, internal entropy, and durability of the replicator, and we discuss the implications of this finding for bacterial cell division, as well as for the pre-biotic emergence of self-replicating nucleic acids.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/335GYDDU/England - 2013 - Statistical physics of self-replication.pdf}
}

@article{erdiJOHNNeumannCOMPUTER,
  title = {{{JOHN}} von {{Neumann}}: ({{The COMPUTER}} and the {{BRAIN}})},
  author = {Erdi, Peter},
  langid = {english},
  file = {/Users/ron/Zotero/storage/U8DC2JWD/Erdi - JOHN von Neumann (The COMPUTER and the BRAIN).pdf}
}

@incollection{evansApperceptionEngine2022,
  title = {2 {{The Apperception Engine}}},
  booktitle = {Kant and {{Artificial Intelligence}}},
  author = {Evans, Richard},
  editor = {Kim, Hyeongjoo and Sch{\"o}necker, Dieter},
  year = {2022},
  month = apr,
  pages = {39--104},
  publisher = {{De Gruyter}},
  urldate = {2023-05-08},
  abstract = {This paper describes an attempt to repurpose Kant's a priori psychology as the architectural blueprint for a machine learning system. First, it describes the conditions that must be satisfied for the agent to achieve unity of experience: the intuitions must be connected, via binary relations, so as to satisfy various unity conditions. Second, it shows how the categories are derived within this model: the categories are pure unary predicates that are derived from the pure binary relations. Third, I describe how Kant's cognitive architecture has been implemented in a computer system (the Apperception Engine) and show in detail what it is like for the system to construct a unified experience from a sequence of raw sensory input.},
  isbn = {978-3-11-070661-1},
  langid = {english},
  file = {/Users/ron/Zotero/storage/2EH5EH6F/Evans - 2022 - 2 The Apperception Engine.pdf}
}

@article{evansMakingSenseSensory2021,
  title = {Making Sense of Sensory Input},
  author = {Evans, Richard and {Hern{\'a}ndez-Orallo}, Jos{\'e} and Welbl, Johannes and Kohli, Pushmeet and Sergot, Marek},
  year = {2021},
  month = apr,
  journal = {Artificial Intelligence},
  volume = {293},
  pages = {103438},
  issn = {0004-3702},
  urldate = {2022-12-12},
  abstract = {This paper attempts to answer a central question in unsupervised learning: what does it mean to ``make sense'' of a sensory sequence? In our formalization, making sense involves constructing a symbolic causal theory that both explains the sensory sequence and also satisfies a set of unity conditions. The unity conditions insist that the constituents of the causal theory {\textendash} objects, properties, and laws {\textendash} must be integrated into a coherent whole. On our account, making sense of sensory input is a type of program synthesis, but it is unsupervised program synthesis. Our second contribution is a computer implementation, the Apperception Engine, that was designed to satisfy the above requirements. Our system is able to produce interpretable human-readable causal theories from very small amounts of data, because of the strong inductive bias provided by the unity conditions. A causal theory produced by our system is able to predict future sensor readings, as well as retrodict earlier readings, and impute (fill in the blanks of) missing sensory readings, in any combination. In fact, it is able to do all three tasks simultaneously. We tested the engine in a diverse variety of domains, including cellular automata, rhythms and simple nursery tunes, multi-modal binding problems, occlusion tasks, and sequence induction intelligence tests. In each domain, we test our engine's ability to predict future sensor values, retrodict earlier sensor values, and impute missing sensory data. The Apperception Engine performs well in all these domains, significantly out-performing neural net baselines. We note in particular that in the sequence induction intelligence tests, our system achieved human-level performance. This is notable because our system is not a bespoke system designed specifically to solve intelligence tests, but a general-purpose system that was designed to make sense of any sensory sequence.},
  langid = {english},
  keywords = {Learning dynamical models,Unsupervised program synthesis},
  file = {/Users/ron/Zotero/storage/D55RGCDU/Evans et al. - 2021 - Making sense of sensory input.pdf;/Users/ron/Zotero/storage/YPMIYGPZ/S0004370220301855.html}
}

@article{farrenAnalysisNetworksChess,
  title = {Analysis of {{Networks}} in {{Chess}}},
  author = {Farren, Derek and Templeton, Daniel and Wang, Meiji},
  langid = {english},
  file = {/Users/ron/Zotero/storage/HWT2YSBR/Farren et al. - Analysis of Networks in Chess.pdf}
}

@misc{fijalkowScalingNeuralProgram2021,
  title = {Scaling {{Neural Program Synthesis}} with {{Distribution-based Search}}},
  author = {Fijalkow, Nathana{\"e}l and Lagarde, Guillaume and Matricon, Th{\'e}o and Ellis, Kevin and Ohlmann, Pierre and Potta, Akarsh},
  year = {2021},
  month = oct,
  number = {arXiv:2110.12485},
  eprint = {2110.12485},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-15},
  abstract = {We consider the problem of automatically constructing computer programs from input-output examples. We investigate how to augment probabilistic and neural program synthesis methods with new search algorithms, proposing a framework called distribution-based search. Within this framework, we introduce two new search algorithms: Heap Search, an enumerative method, and SQRT Sampling, a probabilistic method. We prove certain optimality guarantees for both methods, show how they integrate with probabilistic and neural techniques, and demonstrate how they can operate at scale across parallel compute environments. Collectively these findings offer theoretical and applied studies of search algorithms for program synthesis that integrate with recent developments in machine-learned program synthesizers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/ron/Zotero/storage/RG9DAQXS/Fijalkow et al. - 2021 - Scaling Neural Program Synthesis with Distribution.pdf;/Users/ron/Zotero/storage/HZ7HYXZT/2110.html}
}

@article{Fodor_Pylyshyn_1988,
  title = {Connectionism and Cognitive Architecture: {{A}} Critical Analysis},
  author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
  year = {1988},
  month = mar,
  journal = {Cognition},
  volume = {28},
  number = {1},
  pages = {3--71},
  issn = {0010-0277},
  annotation = {Abstract Note: This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a `language of thought': i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the `systematicity' of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or `abstract neurological') structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation. R{\'e}sum{\'e} Cet article{\'e}tudie les diff{\'e}rences entre mod{\`e}les connectionistes et mod{\`e}les classiques de la structure cognitive. Nous pensons que, bien que les deux types de mod{\`e}les stipulent l'existence d'{\'e}tats mentaux repr{\'e}sentationnels, la diff{\'e}rence essentielle est que seuls les mod{\`e}les classiques requi{\`e}rent l'existence d'un niveau de repr{\'e}sentation symbolique{\textemdash}un ``langage de la pens{\'e}e''{\textemdash}, c'est-{\`a}-dire d'{\'e}tats repr{\'e}sentationnels poss{\'e}dant une structure syntaxique et s{\'e}mantique. Nous examinons ensuite diff{\'e}rents arguments qui militent en faveur de l'existence de repr{\'e}sentations mentales ayant ces propri{\'e}t{\'e}s. Certains de ces arguments reposent sur la ``syst{\'e}maticit{\'e}'' des repr{\'e}sentations mentales, c'est-{\`a}-dire sur le fait que les capacit{\'e}s cognitives exhibent toujours certaines sym{\'e}tries, de sorte que la capacit{\'e}d'entretenir certaines pens{\'e}es implique la capacit{\'e}d'entretenir d'autres pens{\'e}es apparent{\'e}es par leur contenu s{\'e}mantique. Nous pensons que ces arguments montrent de mani{\`e}re convainquante que l'architecture de l'esprit/du cerveau n'est pas connectioniste au niveau cognitif. Nous nous demandons ensuite s'il est possible d'interpr{\'e}ter le connectionisme comme une analyse des structures neuronales (ou des structures neurologiques ``abstraites'') dans lesquelles est r{\'e}alis{\'e}e l'architecture cognitive classique. Nous examinons plusieurs des arguments avanc{\'e}s habituellement en d{\'e}fense du connectionisme, et en concluons que ceux-ci n'ont de sens que dans cette interpr{\'e}tation.}
}

@book{fodor1983modularity,
  title = {The Modularity of Mind},
  author = {Fodor, Jerry A},
  year = {1983},
  publisher = {{MIT press}}
}

@misc{fongSevenSketchesCompositionality2018,
  title = {Seven {{Sketches}} in {{Compositionality}}: {{An Invitation}} to {{Applied Category Theory}}},
  shorttitle = {Seven {{Sketches}} in {{Compositionality}}},
  author = {Fong, Brendan and Spivak, David I.},
  year = {2018},
  month = oct,
  number = {arXiv:1803.05316},
  eprint = {1803.05316},
  primaryclass = {math},
  publisher = {{arXiv}},
  urldate = {2024-01-23},
  abstract = {This book is an invitation to discover advanced topics in category theory through concrete, real-world examples. It aims to give a tour: a gentle, quick introduction to guide later exploration. The tour takes place over seven sketches, each pairing an evocative application, such as databases, electric circuits, or dynamical systems, with the exploration of a categorical structure, such as adjoint functors, enriched categories, or toposes. No prior knowledge of category theory is assumed. A feedback form for typos, comments, questions, and suggestions is available here: https://docs.google.com/document/d/160G9OFcP5DWT8Stn7TxdVx83DJnnf7d5GML0\_FOD5Wg/edit},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {18-01,Mathematics - Category Theory},
  file = {/Users/ron/Zotero/storage/TRDK4GTQ/Fong and Spivak - 2018 - Seven Sketches in Compositionality An Invitation .pdf}
}

@article{fosterGenerativeDeepLearning,
  title = {Generative {{Deep Learning}}},
  author = {Foster, David},
  langid = {english},
  file = {/Users/ron/Zotero/storage/49MWU5SN/Foster - Generative Deep Learning.pdf}
}

@article{franklandConceptsCompositionalitySearch2020,
  title = {Concepts and {{Compositionality}}: {{In Search}} of the {{Brain}}'s {{Language}} of {{Thought}}},
  shorttitle = {Concepts and {{Compositionality}}},
  author = {Frankland, Steven M. and Greene, Joshua D.},
  year = {2020},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {71},
  number = {1},
  pages = {273--303},
  issn = {0066-4308, 1545-2085},
  urldate = {2024-01-25},
  abstract = {Imagine Genghis Khan, Aretha Franklin, and the Cleveland Cavaliers performing an opera on Maui. This silly sentence makes a serious point: As humans, we can flexibly generate and comprehend an unbounded number of complex ideas. Little is known, however, about how our brains accomplish this. Here we assemble clues from disparate areas of cognitive neuroscience, integrating recent research on language, memory, episodic simulation, and computational models of high-level cognition. Our review is framed by Fodor's classic language of thought hypothesis, according to which our minds employ an amodal, language-like system for combining and recombining simple concepts to form more complex thoughts. Here, we highlight emerging work on combinatorial processes in the brain and consider this work's relation to the language of thought. We review evidence for distinct, but complementary, contributions of map-like representations in subregions of the default mode network and sentence-like representations of conceptual relations in regions of the temporal and prefrontal cortex.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/SEPXWYSK/Frankland and Greene - 2020 - Concepts and Compositionality In Search of the Br.pdf}
}

@article{franks1989army,
  title = {Army Ants: A Collective Intelligence},
  author = {Franks, Nigel R},
  year = {1989},
  journal = {Am. Scientist},
  volume = {77},
  pages = {139--145}
}

@article{fristonActiveInferenceCommunication2015,
  title = {Active Inference, Communication and Hermeneutics},
  author = {Friston, Karl J. and Frith, Christopher D.},
  year = {2015},
  month = jul,
  journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
  volume = {68},
  pages = {129--143},
  issn = {0010-9452},
  urldate = {2023-09-29},
  abstract = {Hermeneutics refers to interpretation and translation of text (typically ancient scriptures) but also applies to verbal and non-verbal communication. In a psychological setting it nicely frames the problem of inferring the intended content of a communication. In this paper, we offer a solution to the problem of neural hermeneutics based upon active inference. In active inference, action fulfils predictions about how we will behave (e.g., predicting we will speak). Crucially, these predictions can be used to predict both self and others {\textendash} during speaking and listening respectively. Active inference mandates the suppression of prediction errors by updating an internal model that generates predictions {\textendash} both at fast timescales (through perceptual inference) and slower timescales (through perceptual learning). If two agents adopt the same model, then {\textendash} in principle {\textendash} they can predict each other and minimise their mutual prediction errors. Heuristically, this ensures they are singing from the same hymn sheet. This paper builds upon recent work on active inference and communication to illustrate perceptual learning using simulated birdsongs. Our focus here is the neural hermeneutics implicit in learning, where communication facilitates long-term changes in generative models that are trying to predict each other. In other words, communication induces perceptual learning and enables others to (literally) change our minds and vice versa.},
  pmcid = {PMC4502445},
  pmid = {25957007},
  file = {/Users/ron/Zotero/storage/EXM7Z7D2/Friston and Frith - 2015 - Active inference, communication and hermeneutics.pdf}
}

@article{fristonAmSelfConsciousDoes2018,
  title = {Am {{I Self-Conscious}}? ({{Or Does Self-Organization Entail Self-Consciousness}}?)},
  shorttitle = {Am {{I Self-Conscious}}?},
  author = {Friston, Karl},
  year = {2018},
  journal = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  urldate = {2023-05-28},
  abstract = {Is self-consciousness necessary for consciousness? The answer is yes. So there you have it{\textemdash}the answer is yes. This was my response to a question I was asked to address in a recent AEON piece (https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference). What follows is based upon the notes for that essay, with a special focus on self-organization, self-evidencing and self-modeling. I will try to substantiate my (polemic) answer from the perspective of a physicist. In brief, the argument goes as follows: if we want to talk about creatures, like ourselves, then we have to identify the characteristic behaviors they must exhibit. This is fairly easy to do by noting that living systems return to a set of attracting states time and time again. Mathematically, this implies the existence of a Lyapunov function that turns out to be model evidence (i.e., self-evidence) in Bayesian statistics or surprise (i.e., self-information) in information theory. This means that all biological processes can be construed as performing some form of inference, from evolution through to conscious processing. If this is the case, at what point do we invoke consciousness? The proposal on offer here is that the mind comes into being when self-evidencing has a temporal thickness or counterfactual depth, which grounds inferences about the consequences of my action. On this view, consciousness is nothing more than inference about my future; namely, the self-evidencing consequences of what I could do.},
  file = {/Users/ron/Zotero/storage/NXVYVMC4/Friston - 2018 - Am I Self-Conscious (Or Does Self-Organization En.pdf}
}

@article{fristonFreeEnergyPrinciple2023,
  title = {The Free Energy Principle Made Simpler but Not Too Simple},
  author = {Friston, Karl and Da Costa, Lancelot and Sajid, Noor and Heins, Conor and Ueltzh{\"o}ffer, Kai and Pavliotis, Grigorios A. and Parr, Thomas},
  year = {2023},
  month = jun,
  journal = {Physics Reports},
  series = {The Free Energy Principle Made Simpler but Not Too Simple},
  volume = {1024},
  pages = {1--29},
  issn = {0370-1573},
  urldate = {2023-09-28},
  abstract = {This paper provides a concise description of the free energy principle, starting from a formulation of random dynamical systems in terms of a Langevin equation and ending with a Bayesian mechanics that can be read as a physics of sentience. It rehearses the key steps using standard results from statistical physics. These steps entail (i) establishing a particular partition of states based upon conditional independencies that inherit from sparsely coupled dynamics, (ii) unpacking the implications of this partition in terms of Bayesian inference and (iii) describing the paths of particular states with a variational principle of least action. Teleologically, the free energy principle offers a normative account of self-organisation in terms of optimal Bayesian design and decision-making, in the sense of maximising marginal likelihood or Bayesian model evidence. In summary, starting from a description of the world in terms of random dynamical systems, we end up with a description of self-organisation as sentient behaviour that can be interpreted as self-evidencing; namely, self-assembly, autopoiesis or active inference.},
  keywords = {Bayesian,Markov blanket,Nonequilibrium,Self-organisation,Variational inference},
  file = {/Users/ron/Zotero/storage/2IE7ZZIY/Friston et al. - 2023 - The free energy principle made simpler but not too.pdf;/Users/ron/Zotero/storage/83MB89XW/S037015732300203X.html}
}

@article{fristonFreeenergyPrincipleUnified2010,
  title = {The Free-Energy Principle: A Unified Brain Theory?},
  shorttitle = {The Free-Energy Principle},
  author = {Friston, Karl},
  year = {2010},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {2},
  pages = {127--138},
  issn = {1471-003X, 1471-0048},
  urldate = {2022-05-12},
  abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories {\textemdash} optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/HH9RIKYL/Friston - 2010 - The free-energy principle a unified brain theory.pdf}
}

@article{fristonHistoryFutureBayesian2012,
  title = {The History of the Future of the {{Bayesian}} Brain},
  author = {Friston, Karl},
  year = {2012},
  month = aug,
  journal = {NeuroImage},
  series = {20 {{YEARS OF fMRI}}},
  volume = {62},
  number = {2},
  pages = {1230--1233},
  issn = {1053-8119},
  urldate = {2024-01-14},
  abstract = {The slight perversion of the original title of this piece (The Future of the Bayesian Brain) reflects my attempt to write prospectively about `Science and Stories' over the past 20years. I will meet this challenge by dealing with the future and then turning to its history. The future of the Bayesian brain (in neuroimaging) is clear: it is the application of dynamic causal modeling to understand how the brain conforms to the free energy principle. In this context, the Bayesian brain is a corollary of the free energy principle, which says that any self organizing system (like a brain or neuroimaging community) must maximize the evidence for its own existence, which means it must minimize its free energy using a model of its world. Dynamic causal modeling involves finding models of the brain that have the greatest evidence or the lowest free energy. In short, the future of imaging neuroscience is to refine models of the brain to minimize free energy, where the brain refines models of the world to minimize free energy. This endeavor itself minimizes free energy because our community is itself a self organizing system. I cannot imagine an alternative future that has the same beautiful self consistency as mine. Having dispensed with the future, we can now focus on the past, which is much more interesting:},
  keywords = {Bayesian brain,Effective connectivity,Free energy,Functional integration,Generative models,Inference,Optimization,Predictive coding},
  file = {/Users/ron/Zotero/storage/M7NXG9NE/S1053811911011657.html}
}

@article{fristonPathIntegralsParticular2023,
  title = {Path Integrals, Particular Kinds, and Strange Things},
  author = {Friston, Karl and Da Costa, Lancelot and Sakthivadivel, Dalton A. R. and Heins, Conor and Pavliotis, Grigorios A. and Ramstead, Maxwell and Parr, Thomas},
  year = {2023},
  month = dec,
  journal = {Physics of Life Reviews},
  volume = {47},
  pages = {35--62},
  issn = {1571-0645},
  urldate = {2023-12-12},
  abstract = {This paper describes a path integral formulation of the free energy principle. The ensuing account expresses the paths or trajectories that a particle takes as it evolves over time. The main results are a method or principle of least action that can be used to emulate the behaviour of particles in open exchange with their external milieu. Particles are defined by a particular partition, in which internal states are individuated from external states by active and sensory blanket states. The variational principle at hand allows one to interpret internal dynamics{\textemdash}of certain kinds of particles{\textemdash}as inferring external states that are hidden behind blanket states. We consider different kinds of particles, and to what extent they can be imbued with an elementary form of inference or sentience. Specifically, we consider the distinction between dissipative and conservative particles, inert and active particles and, finally, ordinary and strange particles. Strange particles can be described as inferring their own actions, endowing them with apparent autonomy or agency. In short{\textemdash}of the kinds of particles afforded by a particular partition{\textemdash}strange kinds may be apt for describing sentient behaviour.},
  keywords = {Active matter,Bayesian,Markov blanket,Path integral,Self-organisation,Variational inference},
  file = {/Users/ron/Zotero/storage/9VUYVX6G/Friston et al. - 2023 - Path integrals, particular kinds, and strange thin.pdf}
}

@article{fristonWorldModelLearning2021,
  title = {World Model Learning and Inference},
  author = {Friston, Karl and Moran, Rosalyn J. and Nagai, Yukie and Taniguchi, Tadahiro and Gomi, Hiroaki and Tenenbaum, Josh},
  year = {2021},
  month = dec,
  journal = {Neural Networks},
  volume = {144},
  pages = {573--590},
  issn = {0893-6080},
  urldate = {2023-05-08},
  abstract = {Understanding information processing in the brain{\textemdash}and creating general-purpose artificial intelligence{\textemdash}are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.},
  langid = {english},
  keywords = {Bayesian inference,Cognitive development,Free energy principle,Generative model,Predictive coding,Probabilistic inference},
  file = {/Users/ron/Zotero/storage/BHMH7H34/Friston et al. - 2021 - World model learning and inference.pdf;/Users/ron/Zotero/storage/HH7D6IZB/S0893608021003610.html}
}

@misc{FrontiersHitchhikerGuide,
  title = {Frontiers {\textbar} {{The Hitchhiker}}'s {{Guide}} to {{Neurophenomenology}} {\textendash} {{The Case}} of {{Studying Self Boundaries With Meditators}}},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01680/full}
}

@incollection{FrontMatter2015,
  title = {Front {{Matter}}},
  booktitle = {Doing {{Bayesian Data Analysis}}},
  year = {2015},
  pages = {i-ii},
  publisher = {{Elsevier}},
  urldate = {2024-01-27},
  isbn = {978-0-12-405888-0},
  langid = {english},
  file = {/Users/ron/Zotero/storage/24BXSS4E/2015 - Front Matter.pdf}
}

@article{furlongModellingNeuralProbabilistic2023,
  title = {Modelling Neural Probabilistic Computation Using Vector Symbolic Architectures},
  author = {Furlong, P. Michael and Eliasmith, Chris},
  year = {2023},
  month = dec,
  journal = {Cognitive Neurodynamics},
  issn = {1871-4080, 1871-4099},
  urldate = {2024-01-11},
  abstract = {Distributed vector representations are a key bridging point between connectionist and symbolic representations in cognition. It is unclear how uncertainty should be modelled in systems using such representations. In this paper we discuss how bundles of symbols in certain Vector Symbolic Architectures (VSAs) can be understood as defining an object that has a relationship to a probability distribution, and how statements in VSAs can be understood as being analogous to probabilistic statements. The aim of this paper is to show how (spiking) neural implementations of VSAs can be used to implement probabilistic operations that are useful in building cognitive models. We show how similarity operators between continuous values represented as Spatial Semantic Pointers (SSPs), an example of a technique known as fractional binding, induces a quasi-kernel function that can be used in density estimation. Further, we sketch novel designs for networks that compute entropy and mutual information of VSA-represented distributions and demonstrate their performance when implemented as networks of spiking neurons. We also discuss the relationship between our technique and quantum probability, another technique proposed for modelling uncertainty in cognition. While we restrict ourselves to operators proposed for Holographic Reduced Representations, and for representing real-valued data. We suggest that the methods presented in this paper should translate to any VSA where the dot product between fractionally bound symbols induces a valid kernel.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/7MSAAEDS/Furlong and Eliasmith - 2023 - Modelling neural probabilistic computation using v.pdf}
}

@book{Gaerdenfors,
  title = {Conceptual Spaces: {{The}} Geometry of Thought},
  author = {G{\"a}rdenfors, Peter},
  year = {2000},
  month = mar,
  publisher = {{The MIT Press}},
  abstract = {Within cognitive science, two approaches currently dominate the problem of modeling representations. The symbolic approach views cognition as computation involving symbolic manipulation. Connectionism, a special case of associationism, models associations using artificial neuron networks. Peter G{\"a}rdenfors offers his theory of conceptual representations as a bridge between the symbolic and connectionist approaches.Symbolic representation is particularly weak at modeling concept learning, which is paramount for understanding many cognitive phenomena. Concept learning is closely tied to the notion of similarity, which is also poorly served by the symbolic approach. G{\"a}rdenfors's theory of conceptual spaces presents a framework for representing information on the conceptual level. A conceptual space is built up from geometrical structures based on a number of quality dimensions. The main applications of the theory are on the constructive side of cognitive science: as a constructive model the theory can be applied to the development of artificial systems capable of solving cognitive tasks. G{\"a}rdenfors also shows how conceptual spaces can serve as an explanatory framework for a number of empirical theories, in particular those concerning concept formation, induction, and semantics. His aim is to present a coherent research program that can be used as a basis for more detailed investigations.},
  isbn = {978-0-262-27355-8}
}

@article{garcezNeurosymbolicAI3rd2020,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {d'Avila Garcez, Artur and Lamb, Luis C.},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.05876 [cs]},
  eprint = {2012.05876},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.4,I.2.6},
  file = {/Users/ron/Zotero/storage/XZHCS3J2/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf;/Users/ron/Zotero/storage/WYKZKFLG/2012.html}
}

@article{gardenforsPrimaryCognitiveCategories2020,
  title = {Primary {{Cognitive Categories Are Determined}} by {{Their Invariances}}},
  author = {G{\"a}rdenfors, Peter},
  year = {2020},
  journal = {Frontiers in Psychology},
  volume = {11},
  issn = {1664-1078},
  urldate = {2024-01-03},
  abstract = {The world as we perceive it is structured into objects, actions and places that form parts of events. In this article, my aim is to explain why these categories are cognitively primary. From an empiricist and evolutionary standpoint, it is argued that the reduction of the complexity of sensory signals is based on the brain's capacity to identify various types of invariances that are evolutionarily relevant for the activities of the organism. The first aim of the article is to explain why places, object and actions are primary cognitive categories in our constructions of the external world. It is shown that the invariances that determine these categories have their separate characteristics and that they are, by and large, independent of each other. This separation is supported by what is known about the neural mechanisms. The second aim is to show that the category of events can be analyzed as being constituted of the primary categories. The category of numbers is briefly discussed. Some implications for computational models of the categories are also presented.},
  file = {/Users/ron/Zotero/storage/YJ9ITYBW/Gärdenfors - 2020 - Primary Cognitive Categories Are Determined by The.pdf}
}

@book{gardnerMagicNumbersDr1985,
  title = {The Magic Numbers of {{Dr}}. {{Matrix}}},
  author = {Gardner, Martin},
  year = {1985},
  publisher = {{Prometheus Books}},
  address = {{Buffalo, N.Y}},
  isbn = {978-0-87975-281-1 978-0-87975-282-8},
  langid = {english},
  lccn = {QA95 .G275 1985},
  keywords = {Mathematical recreations,Symbolism of numbers},
  file = {/Users/ron/Zotero/storage/W95EEG7X/Gardner - 1985 - The magic numbers of Dr. Matrix.pdf}
}

@article{gargAreTransformersAll,
  title = {Are {{Transformers All That Karel Needs}}?},
  author = {Garg, Abhay and Sriraman, Anand and Karande, Kunal Pagarey Shirish},
  abstract = {Recent works have shown the promise of using neural networks for the task of program synthesis from input-output examples. The Karel dataset has been a benchmark for evaluating program synthesis approaches. Several techniques have been proposed to use neural guided program synthesis with Karel being used as a baseline. Most of these techniques use an LSTM based model for decoding and improve performance by proposing complex algorithmic additions, such as using inferred execution traces, latent execution of partial programs and debugging generated programs. We observe that by changing the base architecture to a transformer based one, specifically GPT2, we are able to apply simple execution guidance on top to achieve a generalization accurary of 89.64\%, which is within 2.36 percentage points of the current state-of-the-art on Karel which uses ensembling.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/KJDLK2IH/Garg et al. - Are Transformers All That Karel Needs.pdf}
}

@article{garnerPoliticallyCorrectBedtime,
  title = {Politically {{Correct Bedtime Stories}}},
  author = {Garner, James Finn},
  langid = {english},
  file = {/Users/ron/Zotero/storage/NINUMGQI/Garner - Politically Correct Bedtime Stories.pdf}
}

@misc{gauthierAlienCoding2023,
  title = {Alien {{Coding}}},
  author = {Gauthier, Thibault and Ol{\v s}{\'a}k, Miroslav and Urban, Josef},
  year = {2023},
  month = jan,
  number = {arXiv:2301.11479},
  eprint = {2301.11479},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  urldate = {2023-05-12},
  abstract = {We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Number Theory},
  file = {/Users/ron/Zotero/storage/4F47IV2E/Gauthier et al. - 2023 - Alien Coding.pdf;/Users/ron/Zotero/storage/PZJA6LSC/2301.html}
}

@article{gentner1983structure,
  title = {Structure-Mapping: {{A}} Theoretical Framework for Analogy},
  author = {Gentner, Dedre},
  year = {1983},
  journal = {Cognitive science},
  volume = {7},
  number = {2},
  pages = {155--170},
  publisher = {{Elsevier}}
}

@article{gentner2001metaphor,
  title = {Metaphor Is like Analogy},
  author = {Gentner, Dedre and Bowdle, Brian and Wolff, Phillip and Boronat, Consuelo and others},
  year = {2001},
  journal = {The analogical mind: Perspectives from cognitive science},
  pages = {199--253}
}

@article{gentner2011computational,
  title = {Computational Models of Analogy},
  author = {Gentner, Dedre and Forbus, Kenneth D},
  year = {2011},
  journal = {Wiley interdisciplinary reviews: cognitive science},
  volume = {2},
  number = {3},
  pages = {266--276},
  publisher = {{Wiley Online Library}}
}

@article{gernsbackElectricalExperimenter1919,
  title = {The {{Electrical}} Experimenter},
  author = {Gernsback, Hugo and {1884-1967}, ed},
  year = {1919},
  langid = {english},
  file = {/Users/ron/Zotero/storage/HJGLYC5Y/Gernsback and 1884-1967 - 1919 - The Electrical experimenter.pdf}
}

@misc{gkanatsiosEnergybasedModelsAre2024,
  title = {Energy-Based {{Models}} Are {{Zero-Shot Planners}} for {{Compositional Scene Rearrangement}}},
  author = {Gkanatsios, Nikolaos and Jain, Ayush and Xian, Zhou and Zhang, Yunchu and Atkeson, Christopher and Fragkiadaki, Katerina},
  year = {2024},
  month = jan,
  number = {arXiv:2304.14391},
  eprint = {2304.14391},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-26},
  abstract = {Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene-rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then re-locate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model can execute highly compositional instructions zero-shot in simulation and in the real world. It outperforms language-to-action reactive policies and Large Language Model planners by a large margin, especially for long instructions that involve compositions of multiple spatial concepts. Simulation and real-world robot execution videos, as well as our code and datasets are publicly available on our website: https://ebmplanner.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/ron/Zotero/storage/L9AW9SSJ/Gkanatsios et al. - 2024 - Energy-based Models are Zero-Shot Planners for Com.pdf;/Users/ron/Zotero/storage/T7AWAB7B/2304.html}
}

@article{glass2012inference,
  title = {Inference to the Best Explanation: Does It Track Truth?},
  author = {Glass, David H},
  year = {2012},
  journal = {Synthese. An International Journal for Epistemology, Methodology and Philosophy of Science},
  volume = {185},
  number = {3},
  pages = {411--427},
  publisher = {{Springer}}
}

@article{goyalInductiveBiasesDeep2022,
  title = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  author = {Goyal, Anirudh and Bengio, Yoshua},
  year = {2022},
  month = oct,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {478},
  number = {2266},
  pages = {20210068},
  publisher = {{Royal Society}},
  urldate = {2023-02-09},
  abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
  keywords = {causality,deep learning,reasoning,system 2,systematic and out-of-distribution generalization},
  file = {/Users/ron/Zotero/storage/N28TYQQK/Goyal and Bengio - 2022 - Inductive biases for deep learning of higher-level.pdf}
}

@inproceedings{grattarolaLearningGraphCellular2021,
  title = {Learning {{Graph Cellular Automata}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
  year = {2021},
  volume = {34},
  pages = {20983--20994},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-05-10},
  abstract = {Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.},
  file = {/Users/ron/Zotero/storage/Q53QTD55/Grattarola et al. - 2021 - Learning Graph Cellular Automata.pdf}
}

@article{grazianoAttentionSchemaTheory2017,
  title = {The {{Attention Schema Theory}}: {{A Foundation}} for {{Engineering Artificial Consciousness}}},
  shorttitle = {The {{Attention Schema Theory}}},
  author = {Graziano, Michael S. A.},
  year = {2017},
  journal = {Frontiers in Robotics and AI},
  volume = {4},
  issn = {2296-9144},
  urldate = {2022-05-05},
  abstract = {The purpose of the attention schema theory is to explain how an information-processing device, the brain, arrives at the claim that it possesses a non-physical, subjective awareness and assigns a high degree of certainty to that extraordinary claim. The theory does not address how the brain might actually possess a non-physical essence. It is not a theory that deals in the non-physical. It is about the computations that cause a machine to make a claim and to assign a high degree of certainty to the claim. The theory is offered as a possible starting point for building artificial consciousness. Given current technology, it should be possible to build a machine that contains a rich internal model of what consciousness is, attributes that property of consciousness to itself and to the people it interacts with, and uses that attribution to make predictions about human behavior. Such a machine would ``believe'' it is conscious and act like it is conscious, in the same sense that the human machine believes and acts.},
  file = {/Users/ron/Zotero/storage/HG4N7L3F/Graziano - 2017 - The Attention Schema Theory A Foundation for Engi.pdf}
}

@misc{griffithsBayesAgeIntelligent2023,
  title = {Bayes in the Age of Intelligent Machines},
  author = {Griffiths, Thomas L. and Zhu, Jian-Qiao and Grant, Erin and McCoy, R. Thomas},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10206},
  eprint = {2311.10206},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-31},
  abstract = {The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/FKM263MA/Griffiths et al. - 2023 - Bayes in the age of intelligent machines.pdf;/Users/ron/Zotero/storage/IPTV9HEV/2311.html}
}

@article{griffithsBayesianModelsCognition,
  title = {Bayesian Models of Cognition},
  author = {Griffiths, Thomas L and Kemp, Charles and Tenenbaum, Joshua B},
  journal = {BAYESIAN MODELS},
  langid = {english},
  file = {/Users/ron/Zotero/storage/TRMLQ8WZ/Griﬃths et al. - Bayesian models of cognition.pdf}
}

@incollection{griffithsTwoProposalsCausal2007,
  title = {Two {{Proposals}} for {{Causal Grammars}}},
  booktitle = {Causal {{Learning}}},
  author = {Griffiths, Thomas L. and Tenenbaum, Joshua B.},
  editor = {Gopnik, Alison and Schulz, Laura},
  year = {2007},
  month = apr,
  edition = {1},
  pages = {323--346},
  publisher = {{Oxford University PressNew York}},
  urldate = {2024-01-28},
  abstract = {Abstract             A causal theory can be thought of as a grammar that generates events, and that can be used to parse events to identify underlying causal structure. This chapter considers what the components of such a grammar might be {\textemdash} the analogues of syntactic categories and the rules that relate them in a linguistic grammar. It presents two proposals for causal grammars. The first asserts that the variables which describe events can be organized into causal categories, and allows relationships between those categories to be expressed. The second uses a probabilistic variant of first-order logic in order to describe the ontology and causal laws expressed in an intuitive theory. This chapter illustrates how both kinds of grammar can guide causal learning.},
  isbn = {978-0-19-517680-3 978-0-19-995851-1},
  langid = {english},
  file = {/Users/ron/Zotero/storage/9P8S32CN/Griffiths and Tenenbaum - 2007 - Two Proposals for Causal Grammars.pdf}
}

@book{gulwaniProgramSynthesis2017,
  title = {Program Synthesis},
  author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
  year = {2017},
  series = {Foundations and Trends in Programming Languages},
  number = {4.2017, 1-2},
  publisher = {{Now Publishers}},
  address = {{Hanover, MA Delft}},
  isbn = {978-1-68083-292-1},
  langid = {english},
  file = {/Users/ron/Zotero/storage/9UZ68Y9G/Gulwani et al. - 2017 - Program synthesis.pdf}
}

@inproceedings{guzdialGameEngineLearning2017,
  title = {Game {{Engine Learning}} from {{Video}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O.},
  year = {2017},
  month = aug,
  pages = {3707--3713},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  urldate = {2023-05-08},
  abstract = {Intelligent agents need to be able to make predictions about their environment. In this work we present a novel approach to learn a forward simulation model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than reality. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  file = {/Users/ron/Zotero/storage/DYTW5Y5N/Guzdial et al. - 2017 - Game Engine Learning from Video.pdf}
}

@article{guzdialGameLevelGeneration2016,
  title = {Game {{Level Generation}} from {{Gameplay Videos}}},
  author = {Guzdial, Matthew and Riedl, Mark},
  year = {2016},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume = {12},
  number = {1},
  pages = {44--50},
  issn = {2334-0924},
  urldate = {2023-05-08},
  abstract = {We present an unsupervised process to generate full video game levels from a model trained on gameplay video. The model represents probabilistic relationships between shapes properties, and relates the relationships to stylistic variance within a domain. We utilize the classic platformer game Super Mario Bros. to evaluate this process due to its highly-regarded level design. We evaluate the output in comparison to other data-driven level generation techniques via a user study and demonstrate its ability to produce novel output more stylistically similar to exemplar input.},
  copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  langid = {english},
  keywords = {game ai},
  file = {/Users/ron/Zotero/storage/QXFEJJU9/Guzdial and Riedl - 2016 - Game Level Generation from Gameplay Videos.pdf}
}

@article{h.abrahamMaterialsScienceIdeas2012,
  title = {`{{The Materials}} of {{Science}}, the {{Ideas}} of {{Science}}, and the {{Poetry}} of {{Science}}': {{Warren McCulloch}} and {{Jerry Lettvin}}},
  shorttitle = {`{{The Materials}} of {{Science}}, the {{Ideas}} of {{Science}}, and the {{Poetry}} of {{Science}}'},
  author = {H. Abraham, Tara},
  year = {2012},
  month = sep,
  journal = {Interdisciplinary Science Reviews},
  volume = {37},
  number = {3},
  pages = {269--286},
  issn = {0308-0188, 1743-2790},
  urldate = {2024-01-18},
  langid = {english},
  file = {/Users/ron/Zotero/storage/J37PFQXD/H. Abraham - 2012 - ‘The Materials of Science, the Ideas of Science, a.pdf}
}

@article{haCollectiveIntelligenceDeep2022,
  title = {Collective Intelligence for Deep Learning: {{A}} Survey of Recent Developments},
  shorttitle = {Collective Intelligence for Deep Learning},
  author = {Ha, David and Tang, Yujin},
  year = {2022},
  month = aug,
  journal = {Collective Intelligence},
  volume = {1},
  number = {1},
  pages = {26339137221114874},
  publisher = {{SAGE Publications}},
  issn = {2633-9137},
  urldate = {2023-03-14},
  abstract = {In the past decade, we have witnessed the rise of deep learning to dominate the field of artificial intelligence. Advances in artificial neural networks alongside corresponding advances in hardware accelerators with large memory capacity, together with the availability of large datasets enabled practitioners to train and deploy sophisticated neural network models that achieve state-of-the-art performance on tasks across several fields spanning computer vision, natural language processing, and reinforcement learning. However, as these neural networks become bigger, more complex, and more widely used, fundamental problems with current deep learning models become more apparent. State-of-the-art deep learning models are known to suffer from issues that range from poor robustness, inability to adapt to novel task settings, to requiring rigid and inflexible configuration assumptions. Collective behavior, commonly observed in nature, tends to produce systems that are robust, adaptable, and have less rigid assumptions about the environment configuration. Collective intelligence, as a field, studies the group intelligence that emerges from the interactions of many individuals. Within this field, ideas such as self-organization, emergent behavior, swarm optimization, and cellular automata were developed to model and explain complex systems. It is therefore natural to see these ideas incorporated into newer deep learning methods. In this review, we will provide a historical context of neural network research?s involvement with complex systems, and highlight several active areas in modern deep learning research that incorporate the principles of collective intelligence to advance its capabilities. We hope this review can serve as a bridge between the complex systems and deep learning communities.},
  file = {/Users/ron/Zotero/storage/X898SSIT/Ha and Tang - 2022 - Collective intelligence for deep learning A surve.pdf}
}

@article{hammackWhyEngineersNeed,
  title = {Why {{Engineers Need}} to {{Grow}} a {{Long Tail}}},
  author = {Hammack, Bill},
  journal = {Michael Faraday},
  langid = {english},
  file = {/Users/ron/Zotero/storage/C6NFE8UR/Hammack - Why Engineers Need to Grow a Long Tail.pdf}
}

@book{han2022data,
  title = {Data Mining: Concepts and Techniques},
  author = {Han, Jiawei and Pei, Jian and Tong, Hanghang},
  year = {2022},
  publisher = {{Morgan kaufmann}}
}

@article{hanAbductiveSubconceptLearning2023,
  title = {Abductive Subconcept Learning},
  author = {Han, Zhongyi and Cai, Le-Wen and Dai, Wang-Zhou and Huang, Yu-Xuan and Wei, Benzheng and Wang, Wei and Yin, Yilong},
  year = {2023},
  month = feb,
  journal = {Science China Information Sciences},
  volume = {66},
  number = {2},
  pages = {122103},
  issn = {1674-733X, 1869-1919},
  urldate = {2023-05-08},
  abstract = {Bridging neural network learning and symbolic reasoning is crucial for strong AI. Few pioneering studies have made some progress on logical reasoning tasks that require partitioned inputs of instances (e.g., sequential data), from which a final concept is formed based on the complex (perhaps logical) relationships between them. However, they cannot apply to low-level cognitive tasks that require unpartitioned inputs (e.g., raw images), such as object recognition and text classification. In this paper, we propose abductive subconcept learning (ASL) to bridge neural network learning and symbolic reasoning on unsegmented image classification tasks. ASL uses deep learning and abductive logical reasoning to jointly learn subconcept perception and secondary reasoning. Specifically, it first employs meta-interpretive learning (MIL) to induce first-order logical hypotheses capturing the relationships between the high-level subconcepts that account for the target concept. Then, it uses the groundings of the logical hypotheses as labels to train a deep learning model for identifying the subconcepts from unpartitioned data. ASL jointly trains the deep learning model and learns the MIL theory by minimizing the inconsistency between their grounded outputs. Experimental results show that ASL successfully integrates machine learning and logical reasoning with accurate and interpretable results in several object recognition tasks.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/WLLGSR5S/Han et al. - 2023 - Abductive subconcept learning.pdf}
}

@article{haRecurrentWorldModels,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  langid = {english},
  file = {/Users/ron/Zotero/storage/V96N6KVU/Ha and Schmidhuber - Recurrent World Models Facilitate Policy Evolution.pdf}
}

@article{hartwigsenHowDoesHemispheric2021,
  title = {How Does Hemispheric Specialization Contribute to Human-Defining Cognition?},
  author = {Hartwigsen, Gesa and Bengio, Yoshua and Bzdok, Danilo},
  year = {2021},
  month = jul,
  journal = {Neuron},
  volume = {109},
  number = {13},
  pages = {2075--2090},
  issn = {0896-6273},
  urldate = {2022-04-11},
  abstract = {Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman's System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.},
  langid = {english},
  keywords = {artificial general intelligence,computational design principles,deep learning,global workspace theory,human intelligence,language},
  file = {/Users/ron/Zotero/storage/IDD8EDKX/Hartwigsen et al. - 2021 - How does hemispheric specialization contribute to .pdf;/Users/ron/Zotero/storage/FMMQTTF6/S0896627321002907.html}
}

@article{haselager1997,
  title = {Cognitive Science and Folk Psychology: {{The}} Right Frame of Mind.},
  author = {Haselager, W. F. G.},
  year = {1997},
  publisher = {{Sage Publications, Inc.}}
}

@article{hassonDirectFitNature2020,
  title = {Direct {{Fit}} to {{Nature}}: {{An Evolutionary Perspective}} on {{Biological}} and {{Artificial Neural Networks}}},
  shorttitle = {Direct {{Fit}} to {{Nature}}},
  author = {Hasson, Uri and Nastase, Samuel A. and Goldstein, Ariel},
  year = {2020},
  month = feb,
  journal = {Neuron},
  volume = {105},
  number = {3},
  pages = {416--434},
  issn = {08966273},
  urldate = {2022-04-13},
  langid = {english},
  file = {/Users/ron/Zotero/storage/2MASAIZ3/Hasson et al. - 2020 - Direct Fit to Nature An Evolutionary Perspective .pdf}
}

@article{haWorldModels2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  eprint = {1803.10122},
  primaryclass = {cs, stat},
  urldate = {2023-03-14},
  abstract = {We consider the benefits of dream mechanisms {\textendash} that is, the ability to simulate new experiences based on past ones {\textendash} in a machine learning context. Specifically, we are interested in learning for artificial agents that act in the world, and operationalize ``dreaming'' as a mechanism by which such an agent can use its own model of the learning environment to generate new hypotheses and training data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/GKC9JENL/Ha and Schmidhuber - 2018 - World Models.pdf}
}

@incollection{hendersonExperiencingMeaningsGeometry2007,
  title = {Experiencing {{Meanings}} in {{Geometry}}},
  booktitle = {Mathematics and {{Aesthetic}}},
  author = {Henderson, David W. and Taimina, Daina},
  editor = {Sinclair, Nathalie and Pimm, David and Higginson, William},
  year = {2007},
  pages = {58--83},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  urldate = {2024-01-18},
  isbn = {978-0-387-30526-4 978-0-387-38145-9},
  langid = {english},
  file = {/Users/ron/Zotero/storage/HYVLIRBL/Henderson and Taimina - 2007 - Experiencing Meanings in Geometry.pdf}
}

@misc{heTreesTransformersTheoretical2021,
  title = {Trees in Transformers: A Theoretical Analysis of the {{Transformer}}'s Ability to Represent Trees},
  shorttitle = {Trees in Transformers},
  author = {He, Qi and Sedoc, Jo{\~a}o and Rodu, Jordan},
  year = {2021},
  month = dec,
  number = {arXiv:2112.11913},
  eprint = {2112.11913},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Transformer networks are the de facto standard architecture in natural language processing. To date, there are no theoretical analyses of the Transformer's ability to capture tree structures. We focus on the ability of Transformer networks to learn tree structures that are important for tree transduction problems. We first analyze the theoretical capability of the standard Transformer architecture to learn tree structures given enumeration of all possible tree backbones, which we define as trees without labels. We then prove that two linear layers with ReLU activation function can recover any tree backbone from any two nonzero, linearly independent starting backbones. This implies that a Transformer can learn tree structures well in theory. We conduct experiments with synthetic data and find that the standard Transformer achieves similar accuracy compared to a Transformer where tree position information is explicitly encoded, albeit with slower convergence. This confirms empirically that Transformers can learn tree structures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/M8QIKVIP/He et al. - 2021 - Trees in transformers a theoretical analysis of t.pdf;/Users/ron/Zotero/storage/PLW2GDFZ/2112.html}
}

@misc{hewittLearningLearnGenerative2020,
  title = {Learning to Learn Generative Programs with {{Memoised Wake-Sleep}}},
  author = {Hewitt, Luke B. and Le, Tuan Anh and Tenenbaum, Joshua B.},
  year = {2020},
  month = jul,
  number = {arXiv:2007.03132},
  eprint = {2007.03132},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-09},
  abstract = {We study a class of neuro-symbolic generative models in which neural networks are used both for inference and as priors over symbolic, data-generating programs. As generative models, these programs capture compositional structures in a naturally explainable form. To tackle the challenge of performing program induction as an 'inner-loop' to learning, we propose the Memoised Wake-Sleep (MWS) algorithm, which extends Wake Sleep by explicitly storing and reusing the best programs discovered by the inference network throughout training. We use MWS to learn accurate, explainable models in three challenging domains: stroke-based character modelling, cellular automata, and few-shot learning in a novel dataset of real-world string concepts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/LUEKB2Y4/Hewitt et al. - 2020 - Learning to learn generative programs with Memoise.pdf;/Users/ron/Zotero/storage/Y9GL7QID/2007.html}
}

@article{HierarchicalGFlownetCrystal2023,
  title = {Hierarchical {{GFlownet}} for {{Crystal Structure Generation}}},
  year = {2023},
  month = oct,
  urldate = {2024-01-27},
  abstract = {Discovering new solid-state materials necessitates the ability to rapidly explore the vast space of crystal structures and locate stable regions. Generating stable materials with desired properties and composition is a challenging task because of (a) the exponentially large number of possibilities when the elements from the periodic table are considered along with vast variations in their 3D arrangement and corresponding lattice parameters and (b) the rarity of the stable structures. Furthermore, materials discovery requires not only optimized solution structures but also diversity in the configuration of generated material structures. Existing methods have difficulty when exploring large material spaces and generating significantly diverse samples with desired properties and requirements. We propose Crystal Hierarchical Generative Flow Network (CHGlownet), a new generative model that employs a hierarchical exploration strategy with Generative Flow Network to efficiently explore the material space while generating the crystal structure with desired properties. Our model decomposes the large material space into a hierarchy of subspaces of space groups, lattice parameters, and atoms. We significantly outperform the iterative generative methods such as Generative Flow Network (GFlowNet) and Physics Guided Crystal Generative Model (PGCGM) in crystal structure generative tasks in validity, diversity, and generating stable structures with optimized properties and requirements.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/N9SVY6Z4/2023 - Hierarchical GFlownet for Crystal Structure Genera.pdf}
}

@misc{hillLearningMakeAnalogies2019,
  title = {Learning to {{Make Analogies}} by {{Contrasting Abstract Relational Structure}}},
  author = {Hill, Felix and Santoro, Adam and Barrett, David G. T. and Morcos, Ari S. and Lillicrap, Timothy},
  year = {2019},
  month = jan,
  number = {arXiv:1902.00120},
  eprint = {1902.00120},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-06-27},
  abstract = {Analogical reasoning has been a principal focus of various waves of AI research. Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience. Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model. The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features. Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ron/Zotero/storage/FNEW3ZAV/Hill et al. - 2019 - Learning to Make Analogies by Contrasting Abstract.pdf;/Users/ron/Zotero/storage/VWVSN6EH/1902.html}
}

@article{hinton1995wake,
  title = {The" Wake-Sleep" Algorithm for Unsupervised Neural Networks},
  author = {Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
  year = {1995},
  journal = {Science (New York, N.Y.)},
  volume = {268},
  number = {5214},
  pages = {1158--1161},
  publisher = {{American Association for the Advancement of Science}}
}

@article{hinton2002training,
  title = {Training Products of Experts by Minimizing Contrastive Divergence},
  author = {Hinton, Geoffrey E},
  year = {2002},
  journal = {Neural computation},
  volume = {14},
  number = {8},
  pages = {1771--1800},
  publisher = {{MIT Press}}
}

@misc{hintonHowRepresentPartwhole2021,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  year = {2021},
  month = feb,
  number = {arXiv:2102.12627},
  eprint = {2102.12627},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.6,I.4.8},
  file = {/Users/ron/Zotero/storage/PXJDMPZG/Hinton - 2021 - How to represent part-whole hierarchies in a neura.pdf;/Users/ron/Zotero/storage/KS4BB9LM/2102.html}
}

@article{hoelOverfittedBrainDreams2021,
  title = {The Overfitted Brain: {{Dreams}} Evolved to Assist Generalization},
  shorttitle = {The Overfitted Brain},
  author = {Hoel, Erik},
  year = {2021},
  month = may,
  journal = {Patterns},
  volume = {2},
  number = {5},
  pages = {100244},
  issn = {2666-3899},
  urldate = {2023-03-15},
  abstract = {Understanding of the evolved biological function of sleep has advanced considerably in the past decade. However, no equivalent understanding of dreams has emerged. Contemporary neuroscientific theories often view dreams as epiphenomena, and many of the proposals for their biological function are contradicted by the phenomenology of dreams themselves. Now, the recent advent of deep neural networks (DNNs) has finally provided the novel conceptual framework within which to understand the evolved function of dreams. Notably, all DNNs face the issue of overfitting as they learn, which is when performance on one dataset increases but the network's performance fails to generalize (often measured by the divergence of performance on training versus testing datasets). This ubiquitous problem in DNNs is often solved by modelers via ``noise injections'' in the form of noisy or corrupted inputs. The goal of this paper is to argue that the brain faces a similar challenge of overfitting and that nightly dreams evolved to combat the brain's overfitting during its daily learning. That is, dreams are a biological mechanism for increasing generalizability via the creation of corrupted sensory inputs from stochastic activity across the hierarchy of neural structures. Sleep loss, specifically dream loss, leads to an overfitted brain that can still memorize and learn but fails to generalize appropriately. Herein this ''overfitted brain hypothesis'' is explicitly developed and then compared and contrasted with existing contemporary neuroscientific theories of dreams. Existing evidence for the hypothesis is surveyed within both neuroscience and deep learning, and a set of testable predictions is put forward that can be pursued both in~vivo and in silico., Dreaming remains a mystery to neuroscience. While various hypotheses of why brains evolved nightly dreaming have been put forward, many of these are contradicted by the sparse, hallucinatory, and narrative nature of dreams, a nature that seems to lack any particular function. Recently, research on artificial neural networks has shown that during learning, such networks face a ubiquitous problem: that of overfitting to a particular dataset, which leads to failures in generalization and therefore performance on novel datasets. Notably, the techniques that researchers employ to rescue overfitted artificial neural networks generally involve sampling from an out-of-distribution or randomized dataset. The overfitted brain hypothesis is that the brains of organisms similarly face the challenge of fitting too well to their daily distribution of stimuli, causing overfitting and poor generalization. By hallucinating out-of-distribution sensory stimulation every night, the brain is able to rescue the generalizability of its perceptual and cognitive abilities and increase task performance., Why do we dream? While it is known that dreams must be important for learning, it is unknown precisely how or why this is. This paper explores the many hypotheses around why organisms dream, eventually proposing that the evolved purpose of dreams can be identified from research on artificial neural networks. Specifically, the overfitted brain hypothesis claims that in our daily lives the brain learns its tasks too well, and dreams are necessary to stop this ``overfitting.''},
  pmcid = {PMC8134940},
  pmid = {34036289},
  file = {/Users/ron/Zotero/storage/8J92234K/Hoel - 2021 - The overfitted brain Dreams evolved to assist gen.pdf}
}

@book{hofstadter_gdel_1979,
  title = {G{\"o}del, Escher, Bach: An Eternal Golden Braid},
  author = {Hofstadter, Douglas R.},
  year = {1979},
  publisher = {{Basic Books Inc.}},
  added-at = {2009-02-19T14:19:12.000+0100},
  interhash = {311d5d81ee9dfd0fccabd4beafdc671c},
  intrahash = {8e4bfa972541e430f9520006e053ef9b},
  keywords = {logic mathematics},
  timestamp = {2009-02-19T14:19:14.000+0100}
}

@book{hofstadter2013surfaces,
  title = {Surfaces and Essences: {{Analogy}} as the Fuel and Fire of Thinking},
  author = {Hofstadter, Douglas and Sander, Emmanuel},
  year = {2013},
  publisher = {{Basic Books}}
}

@inproceedings{Hu_Malkin_Jain_Everett_Graikos_Bengio_2023, 
  title={GFlowNet-EM for Learning Compositional Latent Variable Models}, 
  ISSN={2640-3498}, 
  abstractNote={Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on images using discrete variational autoencoders (VAEs) without conditional independence enforced in the encoder.}, 
  booktitle={Proceedings of the 40th International Conference on Machine Learning}, 
  publisher={PMLR}, 
  author={Hu, Edward J. and Malkin, Nikolay and Jain, Moksh and Everett, Katie E. and Graikos, Alexandros and Bengio, Yoshua}, 
  year={2023}, 
  month=jul, 
  pages={13528–13549}, 
  language={en}
}

@inproceedings{hummel1996lisa,
  title = {{{LISA}}: {{A}} Computational Model of Analogical Inference and Schema Induction},
  booktitle = {Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society},
  author = {Hummel, John E and Holyoak, Keith J},
  year = {1996},
  pages = {352--357}
}

@article{hurleySharedCircuitsModel2008,
  title = {The Shared Circuits Model ({{SCM}}): How Control, Mirroring, and Simulation Can Enable Imitation, Deliberation, and Mindreading},
  shorttitle = {The Shared Circuits Model ({{SCM}})},
  author = {Hurley, Susan},
  year = {2008},
  month = feb,
  journal = {The Behavioral and Brain Sciences},
  volume = {31},
  number = {1},
  pages = {1-22; discussion 22-58},
  issn = {1469-1825},
  abstract = {Imitation, deliberation, and mindreading are characteristically human sociocognitive skills. Research on imitation and its role in social cognition is flourishing across various disciplines. Imitation is surveyed in this target article under headings of behavior, subpersonal mechanisms, and functions of imitation. A model is then advanced within which many of the developments surveyed can be located and explained. The shared circuits model (SCM) explains how imitation, deliberation, and mindreading can be enabled by subpersonal mechanisms of control, mirroring, and simulation. It is cast at a middle, functional level of description, that is, between the level of neural implementation and the level of conscious perceptions and intentional actions. The SCM connects shared informational dynamics for perception and action with shared informational dynamics for self and other, while also showing how the action/perception, self/other, and actual/possible distinctions can be overlaid on these shared informational dynamics. It avoids the common conception of perception and action as separate and peripheral to central cognition. Rather, it contributes to the situated cognition movement by showing how mechanisms for perceiving action can be built on those for active perception.;{$>$};{$>$}The SCM is developed heuristically, in five layers that can be combined in various ways to frame specific ontogenetic or phylogenetic hypotheses. The starting point is dynamic online motor control, whereby an organism is closely attuned to its embedding environment through sensorimotor feedback. Onto this are layered functions of prediction and simulation of feedback, mirroring, simulation of mirroring, monitored inhibition of motor output, and monitored simulation of input. Finally, monitored simulation of input specifying possible actions plus inhibited mirroring of such possible actions can generate information about the possible as opposed to actual instrumental actions of others, and the possible causes and effects of such possible actions, thereby enabling strategic social deliberation. Multiple instances of such shared circuits structures could be linked into a network permitting decomposition and recombination of elements, enabling flexible control, imitative learning, understanding of other agents, and instrumental and strategic deliberation. While more advanced forms of social cognition, which require tracking multiple others and their multiple possible actions, may depend on interpretative theorizing or language, the SCM shows how layered mechanisms of control, mirroring, and simulation can enable distinctively human cognitive capacities for imitation, deliberation, and mindreading.},
  langid = {english},
  pmid = {18394222},
  keywords = {Brain,Choice Behavior,Cognition,Consciousness,Feedback,Humans,Imitative Behavior,Nerve Net,Neurons,Social Perception},
  file = {/Users/ron/Zotero/storage/CZ938SZY/Hurley - 2008 - The shared circuits model (SCM) how control, mirr.pdf}
}

@article{huthNaturalSpeechReveals2016,
  title = {Natural Speech Reveals the Semantic Maps That Tile Human Cerebral Cortex},
  author = {Huth, Alexander G. and {de Heer}, Wendy A. and Griffiths, Thomas L. and Theunissen, Fr{\'e}d{\'e}ric E. and Gallant, Jack L.},
  year = {2016},
  month = apr,
  journal = {Nature},
  volume = {532},
  number = {7600},
  pages = {453--458},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  urldate = {2023-07-16},
  abstract = {The meaning of language is represented in regions of the cerebral cortex collectively known as the `semantic system'. However, little of the semantic system has been mapped comprehensively, and the semantic selectivity of most regions is unknown. Here we systematically map semantic selectivity across the cortex using voxel-wise modelling of functional MRI (fMRI) data collected while subjects listened to hours of narrative stories. We show that the semantic system is organized into intricate patterns that seem to be consistent across individuals. We then use a novel generative model to create a detailed semantic atlas. Our results suggest that most areas within the semantic system represent information about specific semantic domains, or groups of related concepts, and our atlas shows which domains are represented in each area. This study demonstrates that data-driven methods{\textemdash}commonplace in studies of human neuroanatomy and functional connectivity{\textemdash}provide a powerful and efficient means for mapping functional representations in the brain.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Language,Neural encoding},
  file = {/Users/ron/Zotero/storage/BWYC32JR/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf}
}

@misc{ibarz_generalist_2022,
  title = {A {{Generalist Neural Algorithmic Learner}}},
  author = {Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csord{\'a}s, R{\'o}bert and Dudzik, Andrew and Bo{\v s}njak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and Deac, Andreea and Bevilacqua, Beatrice and Ganin, Yaroslav and Blundell, Charles and Veli{\v c}kovi{\'c}, Petar},
  year = {2022},
  month = dec,
  publisher = {{arXiv}},
  urldate = {2022-12-12},
  abstract = {The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner {\textendash} a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20\% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/6LEFUTWY/Ibarz et al. - 2022 - A Generalist Neural Algorithmic Learner.pdf;/Users/ron/Zotero/storage/IASZ9HIB/2209.html}
}

@misc{ImplementingPoincareEmbeddings,
  title = {Implementing {{Poincar{\'e} Embeddings}} {\textbar} {{RARE Technologies}}},
  urldate = {2023-05-08},
  howpublished = {https://rare-technologies.com/implementing-poincare-embeddings/}
}

@article{jaaskelainenIntroductionCognitiveNeuroscience,
  title = {Introduction to {{Cognitive Neuroscience}}},
  author = {J{\"a}{\"a}skel{\"a}inen, Iiro P},
  langid = {english},
  file = {/Users/ron/Zotero/storage/X7XCSW6A/Jääskeläinen - Introduction to Cognitive Neuroscience.pdf}
}

@book{jablonkaEvolutionFourDimensions2014,
  title = {Evolution in Four Dimensions: Genetic, Epigenetic, Behavioral, and Symbolic Variation in the History of Life},
  shorttitle = {Evolution in Four Dimensions},
  author = {Jablonka, Eva and Lamb, Marion J. and Zeligowski, Anna},
  year = {2014},
  series = {Life and Mind: Philosophical Issues in Biology and Psychology},
  edition = {Revised edition},
  publisher = {{A Bradford Book, The MIT Press}},
  address = {{Cambridge, Massachusetts ; London, England}},
  isbn = {978-0-262-52584-8},
  langid = {english},
  lccn = {QH366.2 .J322 2014},
  keywords = {Biology,Evolution (Biology),Philosophy},
  file = {/Users/ron/Zotero/storage/P7SSUET6/Jablonka et al. - 2014 - Evolution in four dimensions genetic, epigenetic,.pdf}
}

@article{jacksonReverseengineeringCorticalArchitecture2021,
  title = {Reverse-Engineering the Cortical Architecture for Controlled Semantic Cognition},
  author = {Jackson, Rebecca L. and Rogers, Timothy T. and Lambon Ralph, Matthew A.},
  year = {2021},
  month = jun,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {6},
  pages = {774--786},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  urldate = {2023-05-08},
  abstract = {We employ a reverse-engineering approach to illuminate the neurocomputational building blocks that combine to support controlled semantic cognition: the storage and context-appropriate use of conceptual knowledge. By systematically varying the structure of a computational model and assessing the functional consequences, we identified the architectural properties that best promote some core functions of the semantic system. Semantic cognition presents a challenging test case, as the brain must achieve two seemingly contradictory functions: abstracting context-invariant conceptual representations across time and modalities, while producing specific context-sensitive behaviours appropriate for the immediate task. These functions were best achieved in models possessing a single, deep multimodal hub with sparse connections from modality-specific regions, and control systems acting on peripheral rather than deep network layers. The reverse-engineered model provides a unifying account of core findings in the cognitive neuroscience of controlled semantic cognition, including evidence from anatomy, neuropsychology and functional brain imaging.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Cognitive control,Language},
  file = {/Users/ron/Zotero/storage/3IE3PPPU/Jackson et al. - 2021 - Reverse-engineering the cortical architecture for .pdf}
}

@misc{jainMultiObjectiveGFlowNets2022,
  title = {Multi-{{Objective GFlowNets}}},
  author = {Jain, Moksh and Raparthy, Sharath Chandra and {Hernandez-Garcia}, Alex and {Rector-Brooks}, Jarrid and Bengio, Yoshua and Miret, Santiago and Bengio, Emmanuel},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12765},
  eprint = {2210.12765},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-11-29},
  abstract = {In many applications of machine learning, like drug discovery and material design, the goal is to generate candidates that simultaneously maximize a set of objectives. As these objectives are often conflicting, there is no single candidate that simultaneously maximizes all objectives, but rather a set of Pareto-optimal candidates where one objective cannot be improved without worsening another. Moreover, in practice, these objectives are often under-specified, making the diversity of candidates a key consideration. The existing multi-objective optimization methods focus predominantly on covering the Pareto front, failing to capture diversity in the space of candidates. Motivated by the success of GFlowNets for generation of diverse candidates in a single objective setting, in this paper we consider Multi-Objective GFlowNets (MOGFNs). MOGFNs consist of a novel Conditional GFlowNet which models a family of single-objective sub-problems derived by decomposing the multi-objective optimization problem. Our work is the first to empirically demonstrate conditional GFlowNets. Through a series of experiments on synthetic and benchmark tasks, we empirically demonstrate that MOGFNs outperform existing methods in terms of Hypervolume, R2-distance and candidate diversity. We also demonstrate the effectiveness of MOGFNs over existing methods in active learning settings. Finally, we supplement our empirical results with a careful analysis of each component of MOGFNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/WNT8E293/Jain et al. - 2022 - Multi-Objective GFlowNets.pdf;/Users/ron/Zotero/storage/MGJGN8SW/2210.html}
}

@misc{jannerWhenTrustYour2021,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  year = {2021},
  month = nov,
  number = {arXiv:1906.08253},
  eprint = {1906.08253},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-27},
  abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/4PJTCWYI/Janner et al. - 2021 - When to Trust Your Model Model-Based Policy Optim.pdf;/Users/ron/Zotero/storage/692D66RK/1906.html}
}

@book{JCopeland2004-JCOTET,
  title = {The Essential Turing},
  author = {Copeland, B. J.},
  year = {2004},
  publisher = {{Oxford University Press UK}}
}

@misc{johnstonConstructionRealityAI2023,
  title = {The {{Construction}} of {{Reality}} in an {{AI}}: {{A Review}}},
  shorttitle = {The {{Construction}} of {{Reality}} in an {{AI}}},
  author = {Johnston, Jeffrey W.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.05448},
  eprint = {2302.05448},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-15},
  abstract = {AI constructivism as inspired by Jean Piaget, described and surveyed by Frank Guerin, and representatively implemented by Gary Drescher seeks to create algorithms and knowledge structures that enable agents to acquire, maintain, and apply a deep understanding of the environment through sensorimotor interactions. This paper aims to increase awareness of constructivist AI implementations to encourage greater progress toward enabling lifelong learning by machines. It builds on Guerin's 2008 "Learning Like a Baby: A Survey of AI approaches." After briefly recapitulating that survey, it summarizes subsequent progress by the Guerin referents, numerous works not covered by Guerin (or found in other surveys), and relevant efforts in related areas. The focus is on knowledge representations and learning algorithms that have been used in practice viewed through lenses of Piaget's schemas, adaptation processes, and staged development. The paper concludes with a preview of a simple framework for constructive AI being developed by the author that parses concepts from sensory input and stores them in a semantic memory network linked to episodic data. Extensive references are provided.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ron/Zotero/storage/GGGGX6CK/Johnston - 2023 - The Construction of Reality in an AI A Review.pdf;/Users/ron/Zotero/storage/PT5LTE3E/2302.html}
}

@article{joshi2020transformers,
  title = {Transformers Are Graph Neural Networks},
  author = {Joshi, Chaitanya},
  year = {2020},
  journal = {The Gradient},
  volume = {7}
}

@article{jospinHandsonBayesianNeural2022,
  title = {Hands-on {{Bayesian Neural Networks}} -- a {{Tutorial}} for {{Deep Learning Users}}},
  author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  year = {2022},
  month = may,
  journal = {IEEE Computational Intelligence Magazine},
  volume = {17},
  number = {2},
  eprint = {2007.06823},
  primaryclass = {cs, stat},
  pages = {29--48},
  issn = {1556-603X, 1556-6048},
  urldate = {2024-01-27},
  abstract = {Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian Neural Networks, i.e., Stochastic Artificial Neural Networks trained using Bayesian methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62-02 (Primary),Computer Science - Machine Learning,G.3,I.2.6,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/I4F79VEM/Jospin et al. - 2022 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf}
}

@article{Juechems_Summerfield_2019, 
  title={Where Does Value Come From?}, 
  volume={23}, 
  ISSN={13646613}, 
  DOI={10.1016/j.tics.2019.07.012}, 
  number={10}, 
  journal={Trends in Cognitive Sciences}, author={Juechems, Keno and Summerfield, Christopher}, 
  year={2019}, 
  month=oct, 
  pages={836–850}, 
  language={en} 
}


@incollection{k.beraSyntheticBiologyArtificial2020,
  title = {Synthetic {{Biology}}, {{Artificial Intelligence}}, and {{Quantum Computing}}},
  booktitle = {Synthetic {{Biology}} - {{New Interdisciplinary Science}}},
  author = {K. Bera, Rajendra},
  editor = {L. Nagpal, Madan and Boldura, Oana-Maria and Balt{\u a}, Cornel and Enany, Shymaa},
  year = {2020},
  month = feb,
  publisher = {{IntechOpen}},
  urldate = {2024-01-18},
  abstract = {We envisage a world where genetic engineering, artificial intelligence (AI), and quantum computing (QC) will coalesce to bring about a forced speciation of the Homo sapiens. A forced speciation will drastically reduce the emergence time for a new species to a few years compared to Nature's hundreds of millennia. In this chapter, we explain the basic concepts that would allow a forced speciation of the Homo sapiens to occur and its consequences on life on Earth thereafter. Accelerating speciation mediated by Homo sapiens via domestication, gene splicing, and gene drive mechanisms is now scientifically well understood. Synthetic biology can advance speciation far more rapidly using a combination of clustered regularly interspaced short palindromic repeats (CRISPR) technology, advanced computing technologies, and knowledge creation using AI. The day is perhaps not far off when Homo sapiens itself will initiate its own speciation once it advances synthetic biology to a level where it can safely modify the brain to temper emotion and enhance rational thinking as a means of competing against AI-embedded machines guided by quantum algorithms.},
  isbn = {978-1-78984-089-6 978-1-78984-090-2},
  langid = {english},
  file = {/Users/ron/Zotero/storage/ZBTFW4EA/K. Bera - 2020 - Synthetic Biology, Artificial Intelligence, and Qu.pdf}
}

@book{Kahneman11,
  title = {Thinking, Fast and Slow},
  author = {Kahneman, Daniel},
  year = {2011},
  publisher = {{Farrar, Straus and Giroux}},
  address = {{New York}},
  abstract = {Daniel Kahneman, the renowned psychologist and winner of the Nobel Prize in Economics, takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. The impact of overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the profound effect of cognitive biases on everything from playing the stock market to planning our next vacation - each of these can be understood only by knowing how the two systems shape our judgments and decisions. Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives - and how we can use different techniques to guard against the mental glitches that often get us into trouble.},
  added-at = {2019-07-07T18:15:29.000+0200},
  gender = {sm},
  interhash = {a1400a299a00de009ec8eda73e6289af},
  intrahash = {c6ab2fa616e40cc280ac0c905eac02a0},
  isbn = {978-0-374-27563-1},
  owner = {flint},
  related = {Kahneman12},
  relatedtype = {translationas},
  keywords = {-shelf 02061 105 8book cognitive decision science},
  timestamp = {2019-07-07T18:15:29.000+0200}
}

@misc{kaiserModelBasedReinforcementLearning2020,
  title = {Model-{{Based Reinforcement Learning}} for {{Atari}}},
  author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2020},
  month = feb,
  number = {arXiv:1903.00374},
  eprint = {1903.00374},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/GKPUFYK5/Kaiser et al. - 2020 - Model-Based Reinforcement Learning for Atari.pdf;/Users/ron/Zotero/storage/FMVP5GR3/1903.html}
}

@article{kaitz1988reexamination,
  title = {A Reexamination of Newborns' Ability to Imitate Facial Expressions.},
  author = {Kaitz, Marsha and {Meschulach-Sarfaty}, Orna and Auerbach, Judith and Eidelman, Arthur},
  year = {1988},
  journal = {Developmental Psychology},
  volume = {24},
  number = {1},
  pages = {3},
  publisher = {{American Psychological Association}}
}

@misc{kantRecentAdvancesNeural2018,
  title = {Recent {{Advances}} in {{Neural Program Synthesis}}},
  author = {Kant, Neel},
  year = {2018},
  month = feb,
  number = {arXiv:1802.02353},
  eprint = {1802.02353},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-12},
  abstract = {In recent years, deep learning has made tremendous progress in a number of fields that were previously out of reach for artificial intelligence. The successes in these problems has led researchers to consider the possibilities for intelligent systems to tackle a problem that humans have only recently themselves considered: program synthesis. This challenge is unlike others such as object recognition and speech translation, since its abstract nature and demand for rigor make it difficult even for human minds to attempt. While it is still far from being solved or even competitive with most existing methods, neural program synthesis is a rapidly growing discipline which holds great promise if completely realized. In this paper, we start with exploring the problem statement and challenges of program synthesis. Then, we examine the fascinating evolution of program induction models, along with how they have succeeded, failed and been reimagined since. Finally, we conclude with a contrastive look at program synthesis and future research recommendations for the field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/ron/Zotero/storage/4C9X543I/Kant - 2018 - Recent Advances in Neural Program Synthesis.pdf;/Users/ron/Zotero/storage/W8KP7KJ8/1802.html}
}

@article{keLearningInduceCausal2022,
  title = {Learning to {{Induce Causal Structure}}},
  author = {Ke, Nan Rosemary and Chiappa, Silvia and Wang, Jane and Bornschein, Jorg and Weber, Theophane and Goyal, Anirudh and Botvinic, Matthew and Mozer, Michael and Rezende, Danilo Jimenez},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.04875 [cs, stat]},
  eprint = {2204.04875},
  primaryclass = {cs, stat},
  urldate = {2022-05-03},
  abstract = {The fundamental challenge in causal induction is to infer the underlying graph structure given observational and/or interventional data. Most existing causal induction algorithms operate by generating candidate graphs and then evaluating them using either score-based methods (including continuous optimization) or independence tests. In this work, instead of proposing scoring function or independence tests, we treat the inference process as a black box and design a neural network architecture that learns the mapping from both observational and interventional data to graph structures via supervised training on synthetic graphs. We show that the proposed model generalizes not only to new synthetic graphs but also to naturalistic graphs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/C4ECNTTI/Ke et al. - 2022 - Learning to Induce Causal Structure.pdf;/Users/ron/Zotero/storage/8CXBU29F/2204.html}
}

@misc{keles2022computational,
  title = {On the Computational Complexity of Self-Attention},
  author = {Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay},
  year = {2022},
  eprint = {2209.04881},
  primaryclass = {cs.LG},
  archiveprefix = {arxiv}
}

@article{khanHyperbolicRepresentationsSource,
  title = {Hyperbolic {{Representations}} of {{Source Code}}},
  author = {Khan, Raiyan and Nguyen, Thanh V and Srinivasan, Sengamedu H},
  abstract = {Learning effective representations of data is an important task in machine learning. Existing methods typically compute representations or embeddings in Euclidean space, which has shortcomings in representing hierarchical structures of the underlying data. Alternatively, hyperbolic geometry offers a representation scheme that is suited for robust, high-fidelity representations of tree-structured data. In this paper, we explore hyperbolic graph convolutional models for learning hyperbolic representations of source code, which exhibit natural hierarchies. We leverage the abstract syntax tree (AST) of source code and learn its graph-based representation to predict the function name from its body. We compare Lorentz and Poincar{\'e} Disk models of hyperbolic geometry with Euclidean geometry. We also propose several readout schemes to compute the graph-level representations and apply them to the method name prediction task. Using a Lorentz hyperbolic model, we establish a new state-of-the-art result on the ogbg-code2 benchmark for the task.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/UILJKTIW/Khan et al. - Hyperbolic Representations of Source Code.pdf}
}

@article{khanTransformersVisionSurvey2022,
  title = {Transformers in {{Vision}}: {{A Survey}}},
  shorttitle = {Transformers in {{Vision}}},
  author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2022},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  pages = {200:1--200:41},
  issn = {0360-0300},
  urldate = {2024-01-23},
  abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  keywords = {bidirectional encoders,convolutional networks,deep neural networks,literature survey,Self-attention,self-supervision,transformers},
  file = {/Users/ron/Zotero/storage/ANBP2VKR/Khan et al. - 2022 - Transformers in Vision A Survey.pdf}
}

@misc{kicimanCausalReasoningLarge2023,
  title = {Causal {{Reasoning}} and {{Large Language Models}}: {{Opening}} a {{New Frontier}} for {{Causality}}},
  shorttitle = {Causal {{Reasoning}} and {{Large Language Models}}},
  author = {K{\i}c{\i}man, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
  year = {2023},
  month = may,
  number = {arXiv:2305.00050},
  eprint = {2305.00050},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-02},
  abstract = {The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain), and actual causality (86\% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Methodology},
  file = {/Users/ron/Zotero/storage/5HBU4IZ3/Kıcıman et al. - 2023 - Causal Reasoning and Large Language Models Openin.pdf;/Users/ron/Zotero/storage/QGKRNKU4/2305.html}
}

@inproceedings{kimCompoundProbabilisticContextFree2019,
  title = {Compound {{Probabilistic Context-Free Grammars}} for {{Grammar Induction}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
  year = {2019},
  month = jul,
  pages = {2369--2385},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  urldate = {2023-05-08},
  abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.},
  file = {/Users/ron/Zotero/storage/6TLEYY57/Kim et al. - 2019 - Compound Probabilistic Context-Free Grammars for G.pdf}
}

@misc{kissnerAddingIntuitivePhysics2019,
  title = {Adding {{Intuitive Physics}} to {{Neural-Symbolic Capsules Using Interaction Networks}}},
  author = {Kissner, Michael and Mayer, Helmut},
  year = {2019},
  month = jun,
  number = {arXiv:1905.09891},
  eprint = {1905.09891},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Many current methods to learn intuitive physics are based on interaction networks and similar approaches. However, they rely on information that has proven difficult to estimate directly from image data in the past. We aim to narrow this gap by inferring all the semantic information needed from raw pixel data in the form of a scene-graph. Our approach is based on neural-symbolic capsules, which identify which objects in the scene are static, dynamic, elastic or rigid, possible joints between them, as well as their collision information. By integrating all this with interaction networks, we demonstrate how our method is able to learn intuitive physics directly from image sequences and apply its knowledge to new scenes and objects, resulting in an inverse-simulation pipeline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ron/Zotero/storage/J4DUZLBR/Kissner and Mayer - 2019 - Adding Intuitive Physics to Neural-Symbolic Capsul.pdf;/Users/ron/Zotero/storage/FR73HXTA/1905.html}
}

@book{kissnerNeuralSymbolicArchitectureInverse2019,
  title = {A {{Neural-Symbolic Architecture}} for {{Inverse Graphics Improved}} by {{Lifelong Meta-Learning}}},
  author = {Kissner, Michael and Mayer, Helmut},
  year = {2019},
  volume = {11824},
  eprint = {1905.08910},
  primaryclass = {cs},
  urldate = {2023-05-08},
  abstract = {We follow the idea of formulating vision as inverse graphics and propose a new type of element for this task, a neural-symbolic capsule. It is capable of de-rendering a scene into semantic information feed-forward, as well as rendering it feed-backward. An initial set of capsules for graphical primitives is obtained from a generative grammar and connected into a full capsule network. Lifelong meta-learning continuously improves this network's detection capabilities by adding capsules for new and more complex objects it detects in a scene using few-shot learning. Preliminary results demonstrate the potential of our novel approach.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ron/Zotero/storage/NB744TDT/Kissner and Mayer - 2019 - A Neural-Symbolic Architecture for Inverse Graphic.pdf}
}

@misc{kissnerNeuralSymbolicFrameworkMental2020,
  title = {A {{Neural-Symbolic Framework}} for {{Mental Simulation}}},
  author = {Kissner, Michael},
  year = {2020},
  month = aug,
  number = {arXiv:2008.02356},
  eprint = {2008.02356},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {We present a neural-symbolic framework for observing the environment and continuously learning visual semantics and intuitive physics to reproduce them in an interactive simulation. The framework consists of five parts, a neural-symbolic hybrid network based on capsules for inverse graphics, an episodic memory to store observations, an interaction network for intuitive physics, a meta-learning agent that continuously improves the framework and a querying language that acts as the framework's interface for simulation. By means of lifelong meta-learning, the capsule network is expanded and trained continuously, in order to better adapt to its environment with each iteration. This enables it to learn new semantics using a few-shot approach and with minimal input from an oracle over its lifetime. From what it learned through observation, the part for intuitive physics infers all the required physical properties of the objects in a scene, enabling predictions. Finally, a custom query language ties all parts together, which allows to perform various mental simulation tasks, such as navigation, sorting and simulation of a game environment, with which we illustrate the potential of our novel approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ron/Zotero/storage/3Y4CVFED/Kissner - 2020 - A Neural-Symbolic Framework for Mental Simulation.pdf;/Users/ron/Zotero/storage/ICBZUJ43/2008.html}
}

@article{knillBayesianBrainRole2004,
  title = {The {{Bayesian}} Brain: The Role of Uncertainty in Neural Coding and Computation},
  shorttitle = {The {{Bayesian}} Brain},
  author = {Knill, David C. and Pouget, Alexandre},
  year = {2004},
  month = dec,
  journal = {Trends in Neurosciences},
  volume = {27},
  number = {12},
  pages = {712--719},
  publisher = {{Elsevier}},
  issn = {0166-2236, 1878-108X},
  urldate = {2024-01-14},
  langid = {english},
  pmid = {15541511}
}

@inproceedings{kokinov1994dual,
  title = {The {{DUAL}} Cognitive Architecture: {{A}} Hybrid Multi-Agent Approach.},
  booktitle = {{{ECAI}}},
  author = {Kokinov, Boicho N},
  year = {1994},
  pages = {203--207}
}

@article{kokinov2001integrating,
  title = {Integrating Memory and Reasoning in Analogy-Making: {{The AMBR}} Model},
  author = {Kokinov, Boicho and Petrov, Alexander},
  year = {2001},
  journal = {The analogical mind: Perspectives from cognitive science},
  pages = {59--124},
  publisher = {{MIT Press Cambridge, MA}}
}

@article{kokinov2003computational,
  title = {Computational Models of Analogy-Making},
  author = {Kokinov, Boicho and French, Robert M},
  year = {2003},
  journal = {Encyclopedia of cognitive science},
  volume = {1},
  pages = {113--118},
  publisher = {{Nature Publishing Group London}}
}

@book{kolb2001introduction,
  title = {An Introduction to Brain and Behavior},
  author = {Kolb, Bryan and Whishaw, Ian Q and Teskey, G Campbell and Whishaw, Ian Q and Teskey, G Campbell},
  year = {2001},
  publisher = {{Worth New York}}
}

@article{korugaUltimateComputingBiomolecular1988,
  title = {Ultimate Computing: {{Biomolecular}} Consciousness and Nanotechnology},
  shorttitle = {Ultimate Computing},
  author = {Koruga, Djuro},
  year = {1988},
  month = jan,
  journal = {Biosystems},
  volume = {22},
  number = {1},
  pages = {83--84},
  issn = {03032647},
  urldate = {2024-01-18},
  langid = {english},
  file = {/Users/ron/Zotero/storage/EJCI5WLN/Koruga - 1988 - Ultimate computing Biomolecular consciousness and.pdf}
}

@misc{kosiorekStackedCapsuleAutoencoders2019,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  year = {2019},
  month = dec,
  number = {arXiv:1906.06818},
  eprint = {1906.06818},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55\%) and MNIST (98.7\%). The code is available at https://github.com/google-research/google-research/tree/master/stacked\_capsule\_autoencoders},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/WM3DUKLT/Kosiorek et al. - 2019 - Stacked Capsule Autoencoders.pdf;/Users/ron/Zotero/storage/R94Q6YLI/1906.html}
}

@article{kotseruba202040,
  title = {40 Years of Cognitive Architectures: Core Cognitive Abilities and Practical Applications},
  author = {Kotseruba, Iuliia and Tsotsos, John K},
  year = {2020},
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {1},
  pages = {17--94},
  publisher = {{Springer}}
}

@article{kourieIncrementalAlgorithmConstruct2009,
  title = {An Incremental Algorithm to Construct a Lattice of Set Intersections},
  author = {Kourie, Derrick G. and Obiedkov, Sergei and Watson, Bruce W. and Van Der Merwe, Dean},
  year = {2009},
  month = jan,
  journal = {Science of Computer Programming},
  volume = {74},
  number = {3},
  pages = {128--142},
  issn = {01676423},
  urldate = {2024-01-18},
  abstract = {An incremental algorithm to construct a lattice from a collection of sets is derived, refined, analyzed, and related to a similar previously published algorithm for constructing concept lattices. The lattice constructed by the algorithm is the one obtained by closing the collection of sets with respect to set intersection. The analysis explains the empirical efficiency of the related concept lattice construction algorithm that had been observed in previous studies. The derivation highlights the effectiveness of a correctness-byconstruction approach to algorithm development.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/3VTSG8FV/Kourie et al. - 2009 - An incremental algorithm to construct a lattice of.pdf}
}

@article{krakauerInformationTheoryIndividuality2020,
  title = {The Information Theory of Individuality},
  author = {Krakauer, David and Bertschinger, Nils and Olbrich, Eckehard and Flack, Jessica C. and Ay, Nihat},
  year = {2020},
  month = jun,
  journal = {Theory in Biosciences},
  volume = {139},
  number = {2},
  pages = {209--223},
  issn = {1611-7530},
  urldate = {2023-10-03},
  abstract = {Despite the near universal assumption of individuality in biology, there is little agreement about what individuals are and few rigorous quantitative methods for their identification. Here, we propose that individuals are aggregates that preserve a measure of temporal integrity, i.e., ``propagate'' information from their past into their futures. We formalize this idea using information theory and graphical models. This mathematical formulation yields three principled and distinct forms of individuality{\textemdash}an organismal, a colonial, and a driven form{\textemdash}each of which varies in the degree of environmental dependence and inherited information. This approach can be thought of as a Gestalt approach to evolution where selection makes figure-ground (agent{\textendash}environment) distinctions using suitable information-theoretic lenses. A benefit of the approach is that it expands the scope of allowable individuals to include adaptive aggregations in systems that are multi-scale, highly distributed, and do not necessarily have physical boundaries such as cell walls or clonal somatic tissue. Such individuals might be visible to selection but hard to detect by observers without suitable measurement principles. The information theory of individuality allows for the identification of individuals at all levels of organization from molecular to cultural and provides a basis for testing assumptions about the natural scales of a system and argues for the importance of uncertainty reduction through coarse-graining in adaptive systems.},
  langid = {english},
  keywords = {Adaptation,Control,Evolution,Gestalt,Information decomposition,Mutual information,Shannon information,Shared information,Synergy},
  file = {/Users/ron/Zotero/storage/PGE99MS5/Krakauer et al. - 2020 - The information theory of individuality.pdf}
}

@misc{kumarUsingNaturalLanguage2022,
  title = {Using {{Natural Language}} and {{Program Abstractions}} to {{Instill Human Inductive Biases}} in {{Machines}}},
  author = {Kumar, Sreejan and Correa, Carlos G. and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y. and Hawkins, Robert D. and Daw, Nathaniel D. and Cohen, Jonathan D. and Narasimhan, Karthik and Griffiths, Thomas L.},
  year = {2022},
  month = oct,
  number = {arXiv:2205.11558},
  eprint = {2205.11558},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-09},
  abstract = {Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ron/Zotero/storage/EEFFSPWE/Kumar et al. - 2022 - Using Natural Language and Program Abstractions to.pdf;/Users/ron/Zotero/storage/VACQBILB/2205.html}
}

@inproceedings{kwisthout2013most,
  title = {Most Inforbable Explanations: {{Finding}} Explanations in {{Bayesian}} Networks That Are Both Probable and Informative},
  booktitle = {European Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty},
  author = {Kwisthout, Johan},
  year = {2013},
  pages = {328--339},
  publisher = {{Springer}}
}

@article{lakeBuildingMachinesThat2017,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2017},
  journal = {Behavioral and Brain Sciences},
  volume = {40},
  pages = {e253},
  issn = {0140-525X, 1469-1825},
  urldate = {2023-10-03},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CU7YFC4U/Lake et al. - 2017 - Building machines that learn and think like people.pdf}
}

@article{lakeHumanlikeSystematicGeneralization2023,
  title = {Human-like Systematic Generalization through a Meta-Learning Neural Network},
  author = {Lake, Brenden M. and Baroni, Marco},
  year = {2023},
  month = nov,
  journal = {Nature},
  volume = {623},
  number = {7985},
  pages = {115--121},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  urldate = {2024-01-26},
  abstract = {The power of human language and thought arises from systematic compositionality{\textemdash}the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn's challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an~instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {/Users/ron/Zotero/storage/5TNBUXXF/Lake and Baroni - 2023 - Human-like systematic generalization through a met.pdf}
}

@article{langanCognitiveTheoreticModelUniverse,
  title = {The {{Cognitive-Theoretic Model}} of the {{Universe}}:},
  author = {Langan, Christopher Michael},
  abstract = {Inasmuch as science is observational or perceptual in nature, the goal of providing a scientific model and mechanism for the evolution of complex systems ultimately requires a supporting theory of reality of which perception itself is the model (or theory-to-universe mapping). Where information is the abstract currency of perception, such a theory must incorporate the theory of information while extending the information concept to incorporate reflexive self-processing in order to achieve an intrinsic (self-contained) description of reality. This extension is associated with a limiting formulation of model theory identifying mental and physical reality, resulting in a reflexively self-generating, self-modeling theory of reality identical to its universe on the syntactic level. By the nature of its derivation, this theory, the Cognitive Theoretic Model of the Universe or CTMU, can be regarded as a supertautological reality-theoretic extension of logic. Uniting the theory of reality with an advanced form of computational language theory, the CTMU describes reality as a Self-Configuring Self-Processing Language or SCSPL, a reflexive intrinsic language characterized not only by self-reference and recursive self-definition, but full self-configuration and selfexecution (reflexive read-write functionality). SCSPL reality embodies a dual-aspect monism consisting of infocognition, self-transducing information residing in self-recognizing SCSPL elements called syntactic operators. The CTMU identifies itself with the structure of these operators and thus with the distributive syntax of its self-modeling SCSPL universe, including the reflexive grammar by which the universe refines itself from unbound telesis or UBT, a primordial realm of infocognitive potential free of informational constraint. Under the guidance of a limiting (intrinsic) form of anthropic principle called the Telic Principle, SCSPL evolves by telic recursion, jointly configuring syntax and state while maximizing a generalized selfselection parameter and adjusting on the fly to freely-changing internal conditions. SCSPL relates space, time and object by means of conspansive duality and conspansion, an SCSPL-grammatical process featuring an alternation between dual phases of existence associated with design and actualization and related to the familiar wave-particle duality of quantum mechanics. By distributing the design phase of reality over the actualization phase, conspansive spacetime also provides a distributed mechanism for Intelligent Design, adjoining to the restrictive principle of natural selection a basic means of generating information and complexity. Addressing physical evolution on not only the biological but cosmic level, the CTMU addresses the most evident deficiencies and paradoxes associated with conventional discrete and continuum models of reality, including temporal directionality and accelerating cosmic expansion, while preserving virtually all of the major benefits of current scientific and mathematical paradigms.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/I2NYFADG/Langan - The Cognitive-Theoretic Model of the Universe.pdf}
}

@misc{lavinSimulationIntelligenceNew2022,
  title = {Simulation {{Intelligence}}: {{Towards}} a {{New Generation}} of {{Scientific Methods}}},
  shorttitle = {Simulation {{Intelligence}}},
  author = {Lavin, Alexander and Krakauer, David and Zenil, Hector and Gottschlich, Justin and Mattson, Tim and Brehmer, Johann and Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Prunkl, Carina and Paige, Brooks and Isayev, Olexandr and Peterson, Erik and McMahon, Peter L. and Macke, Jakob and Cranmer, Kyle and Zhang, Jiaxin and Wainwright, Haruko and Hanuka, Adi and Veloso, Manuela and Assefa, Samuel and Zheng, Stephan and Pfeffer, Avi},
  year = {2022},
  month = nov,
  number = {arXiv:2112.03235},
  eprint = {2112.03235},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-12},
  abstract = {The original "Seven Motifs" set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the "Nine Motifs of Simulation Intelligence", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,{Computer Science - Computational Engineering, Finance, and Science},Computer Science - Machine Learning,Computer Science - Mathematical Software},
  file = {/Users/ron/Zotero/storage/HATHXLJL/Lavin et al. - 2022 - Simulation Intelligence Towards a New Generation .pdf;/Users/ron/Zotero/storage/475E3HHS/2112.html}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  year={2022}
}

@article{lecunTutorialEnergyBasedLearning,
  title = {A {{Tutorial}} on {{Energy-Based Learning}}},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
  abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/NVP8F4JI/LeCun et al. - A Tutorial on Energy-Based Learning.pdf}
}

@article{Levenshtein,
  title = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  author = {Levenshtein, V. I.},
  year = {1966},
  month = feb,
  journal = {Soviet Physics Doklady},
  volume = {10},
  pages = {707},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{levinBioelectricNetworksCognitive2023,
  title = {Bioelectric Networks: The Cognitive Glue Enabling Evolutionary Scaling from Physiology to Mind},
  shorttitle = {Bioelectric Networks},
  author = {Levin, Michael},
  year = {2023},
  month = may,
  journal = {Animal Cognition},
  issn = {1435-9456},
  urldate = {2023-10-22},
  abstract = {Each of us made the remarkable journey from mere matter to mind: starting life as a quiescent oocyte (``just chemistry and physics''), and slowly, gradually, becoming an adult human with complex metacognitive processes, hopes, and dreams. In addition, even though we feel ourselves to be a unified, single Self, distinct from the emergent dynamics of termite mounds and other swarms, the reality is that all intelligence is collective intelligence: each of us consists of a huge number of cells working together to generate a coherent cognitive being with goals, preferences, and memories that belong to the whole and not to its parts. Basal cognition is the quest to understand how Mind scales{\textemdash}how large numbers of competent subunits can work together to become intelligences that expand the scale of their possible goals. Crucially, the remarkable trick of turning homeostatic, cell-level physiological competencies into large-scale behavioral intelligences is not limited to the electrical dynamics of the brain. Evolution was using bioelectric signaling long before neurons and muscles appeared, to solve the problem of creating and repairing complex bodies. In this Perspective, I review the deep symmetry between the intelligence of developmental morphogenesis and that of classical behavior. I describe the highly conserved mechanisms that enable the collective intelligence of cells to implement regulative embryogenesis, regeneration, and cancer suppression. I sketch the story of an evolutionary pivot that repurposed the algorithms and cellular machinery that enable navigation of morphospace into the behavioral navigation of the 3D world which we so readily recognize as intelligence. Understanding the bioelectric dynamics that underlie construction of complex bodies and brains provides an essential path to understanding the natural evolution, and bioengineered design, of diverse intelligences within and beyond the phylogenetic history of Earth.},
  langid = {english},
  keywords = {Basal cognition,Behavior,Cells,Development,Evolution,Morphogenesis,Regeneration,Voltage},
  file = {/Users/ron/Zotero/storage/B5U524P5/Levin - 2023 - Bioelectric networks the cognitive glue enabling .pdf}
}

@article{levinComputationalBoundarySelf2019,
  title = {The {{Computational Boundary}} of a ``{{Self}}'': {{Developmental Bioelectricity Drives Multicellularity}} and {{Scale-Free Cognition}}},
  shorttitle = {The {{Computational Boundary}} of a ``{{Self}}''},
  author = {Levin, Michael},
  year = {2019},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {2688},
  issn = {1664-1078},
  urldate = {2023-04-04},
  abstract = {All epistemic agents physically consist of parts that must somehow comprise an integrated cognitive self. Biological individuals consist of subunits (organs, cells, and molecular networks) that are themselves complex and competent in their own native contexts. How do coherent biological Individuals result from the activity of smaller sub-agents? To understand the evolution and function of metazoan creatures' bodies and minds, it is essential to conceptually explore the origin of multicellularity and the scaling of the basal cognition of individual cells into a coherent larger organism. In this article, I synthesize ideas in cognitive science, evolutionary biology, and developmental physiology toward a hypothesis about the origin of Individuality: ``Scale-Free Cognition.'' I propose a fundamental definition of an Individual based on the ability to pursue goals at an appropriate level of scale and organization and suggest a formalism for defining and comparing the cognitive capacities of highly diverse types of agents. Any Self is demarcated by a computational surface {\textendash} the spatio-temporal boundary of events that it can measure, model, and try to affect. This surface sets a functional boundary a cognitive ``light cone'' which defines the scale and limits of its cognition. I hypothesize that higher level goal-directed activity and agency, resulting in larger cognitive boundaries, evolve from the primal homeostatic drive of living things to reduce stress {\textendash} the difference between current conditions and life-optimal conditions. The mechanisms of developmental bioelectricity - the ability of all cells to form electrical networks that process information suggest a plausible set of gradual evolutionary steps that naturally lead from physiological homeostasis in single cells to memory, prediction, and ultimately complex cognitive agents, via scale-up of the basic drive of infotaxis. Recent data on the molecular mechanisms of pre-neural bioelectricity suggest a model of how increasingly sophisticated cognitive functions emerge smoothly from cell-cell communication used to guide embryogenesis and regeneration. This set of hypotheses provides a novel perspective on numerous phenomena, such as cancer, and makes several unique, testable predictions for interdisciplinary research that have implications not only for evolutionary developmental biology but also for biomedicine and perhaps artificial intelligence and exobiology.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/JQWNT5SE/Levin - 2019 - The Computational Boundary of a “Self” Developmen.pdf}
}

@article{levinTechnologicalApproachMind2022,
  title = {Technological {{Approach}} to {{Mind Everywhere}}: {{An Experimentally-Grounded Framework}} for {{Understanding Diverse Bodies}} and {{Minds}}},
  shorttitle = {Technological {{Approach}} to {{Mind Everywhere}}},
  author = {Levin, Michael},
  year = {2022},
  journal = {Frontiers in Systems Neuroscience},
  volume = {16},
  issn = {1662-5137},
  urldate = {2023-09-27},
  abstract = {Synthetic biology and bioengineering provide the opportunity to create novel embodied cognitive systems (otherwise known as minds) in a very wide variety of chimeric architectures combining evolved and designed material and software. These advances are disrupting familiar concepts in the philosophy of mind, and require new ways of thinking about and comparing truly diverse intelligences, whose composition and origin are not like any of the available natural model species. In this Perspective, I introduce TAME{\textemdash}Technological Approach to Mind Everywhere{\textemdash}a framework for understanding and manipulating cognition in unconventional substrates. TAME formalizes a non-binary (continuous), empirically-based approach to strongly embodied agency. TAME provides a natural way to think about animal sentience as an instance of collective intelligence of cell groups, arising from dynamics that manifest in similar ways in numerous other substrates. When applied to regenerating/developmental systems, TAME suggests a perspective on morphogenesis as an example of basal cognition. The deep symmetry between problem-solving in anatomical, physiological, transcriptional, and 3D (traditional behavioral) spaces drives specific hypotheses by which cognitive capacities can increase during evolution. An important medium exploited by evolution for joining active subunits into greater agents is developmental bioelectricity, implemented by pre-neural use of ion channels and gap junctions to scale up cell-level feedback loops into anatomical homeostasis. This architecture of multi-scale competency of biological systems has important implications for plasticity of bodies and minds, greatly potentiating evolvability. Considering classical and recent data from the perspectives of computational science, evolutionary biology, and basal cognition, reveals a rich research program with many implications for cognitive science, evolutionary biology, regenerative medicine, and artificial intelligence.},
  file = {/Users/ron/Zotero/storage/94BXLIYP/Levin - 2022 - Technological Approach to Mind Everywhere An Expe.pdf}
}

@article{lewisHowMemoryReplay2018,
  title = {How {{Memory Replay}} in {{Sleep Boosts Creative Problem-Solving}}},
  author = {Lewis, Penelope A. and Knoblich, G{\"u}nther and Poe, Gina},
  year = {2018},
  month = jun,
  journal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {6},
  pages = {491--503},
  issn = {13646613},
  urldate = {2022-05-04},
  langid = {english},
  file = {/Users/ron/Zotero/storage/GA6G7JVB/Lewis et al. - 2018 - How Memory Replay in Sleep Boosts Creative Problem.pdf}
}

@article{lewisHowMemoryReplay2018a,
  title = {How {{Memory Replay}} in {{Sleep Boosts Creative Problem-Solving}}},
  author = {Lewis, Penelope A. and Knoblich, G{\"u}nther and Poe, Gina},
  year = {2018},
  month = jun,
  journal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {6},
  pages = {491--503},
  issn = {13646613},
  urldate = {2024-01-18},
  langid = {english},
  file = {/Users/ron/Zotero/storage/ALSNP3Q2/Lewis et al. - 2018 - How Memory Replay in Sleep Boosts Creative Problem.pdf}
}

@inproceedings{liangCodePoliciesLanguage2023,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = {2023},
  month = may,
  pages = {9493--9500},
  urldate = {2024-01-12},
  abstract = {Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (`faster') depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  file = {/Users/ron/Zotero/storage/6UI9R2WD/Liang et al. - 2023 - Code as Policies Language Model Programs for Embo.pdf}
}

@misc{liScallopLanguageNeurosymbolic2023,
  title = {Scallop: {{A Language}} for {{Neurosymbolic Programming}}},
  shorttitle = {Scallop},
  author = {Li, Ziyang and Huang, Jiani and Naik, Mayur},
  year = {2023},
  month = apr,
  number = {arXiv:2304.04812},
  eprint = {2304.04812},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/ron/Zotero/storage/JKMYTWZD/Li et al. - 2023 - Scallop A Language for Neurosymbolic Programming.pdf;/Users/ron/Zotero/storage/84AX2RE4/2304.html}
}

@misc{liShortSurveySystematic2022,
  title = {A {{Short Survey}} of {{Systematic Generalization}}},
  author = {Li, Yuanpeng},
  year = {2022},
  month = nov,
  number = {arXiv:2211.11956},
  eprint = {2211.11956},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-25},
  abstract = {This survey includes systematic generalization and a history of how machine learning addresses it. We aim to summarize and organize the related information of both conventional and recent improvements. We first look at the definition of systematic generalization, then introduce Classicist and Connectionist. We then discuss different types of Connectionists and how they approach the generalization. Two crucial problems of variable binding and causality are discussed. We look into systematic generalization in language, vision, and VQA fields. Recent improvements from different aspects are discussed. Systematic generalization has a long history in artificial intelligence. We could cover only a small portion of many contributions. We hope this paper provides a background and is beneficial for discoveries in future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/RLCAT4K9/Li - 2022 - A Short Survey of Systematic Generalization.pdf;/Users/ron/Zotero/storage/IU2R8GY9/2211.html}
}

@article{liu2014seeing,
  title = {Seeing {{Jesus}} in Toast: Neural and Behavioral Correlates of Face Pareidolia},
  author = {Liu, Jiangang and Li, Jun and Feng, Lu and Li, Ling and Tian, Jie and Lee, Kang},
  year = {2014},
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {53},
  pages = {60--77},
  publisher = {{Elsevier}}
}

@misc{liuConesConceptNeurons2023,
  title = {Cones: {{Concept Neurons}} in {{Diffusion Models}} for {{Customized Generation}}},
  shorttitle = {Cones},
  author = {Liu, Zhiheng and Feng, Ruili and Zhu, Kai and Zhang, Yifei and Zheng, Kecheng and Liu, Yu and Zhao, Deli and Zhou, Jingren and Cao, Yang},
  year = {2023},
  month = mar,
  number = {arXiv:2303.05125},
  eprint = {2303.05125},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-14},
  abstract = {Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. A few steps of further fine-tuning can enhance the multi-concept capability, which may be the first to manage to generate up to four different subjects in a single image. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 values of the parameters, which reduces storage consumption by 90{\textbackslash}\% compared with previous subject-driven generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ron/Zotero/storage/4QJBZYT4/Liu et al. - 2023 - Cones Concept Neurons in Diffusion Models for Cus.pdf;/Users/ron/Zotero/storage/92Y3EHLA/2303.html}
}

@inproceedings{liuDiscreteValuedNeuralCommunication2021,
  title = {Discrete-{{Valued Neural Communication}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Dianbo and Lamb, Alex M and Kawaguchi, Kenji and ALIAS PARTH GOYAL, Anirudh Goyal and Sun, Chen and Mozer, Michael C and Bengio, Yoshua},
  year = {2021},
  volume = {34},
  pages = {2109--2121},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-11},
  abstract = {Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that discrete-valued neural communication (DVNC) substantially improves systematic generalization in a variety of architectures{\textemdash}transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method useful in practice.},
  file = {/Users/ron/Zotero/storage/5CTB3K88/Liu et al. - 2021 - Discrete-Valued Neural Communication.pdf}
}

@article{liuSentimentAnalysisOpinion,
  title = {Sentiment {{Analysis}} and {{Opinion Mining}}},
  author = {Liu, Bing},
  langid = {english},
  file = {/Users/ron/Zotero/storage/2VGK7NDI/Liu - Sentiment Analysis and Opinion Mining.pdf}
}

@article{LogicalCalculusIdeas,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  langid = {english},
  file = {/Users/ron/Zotero/storage/RMBDP6HV/A logical calculus of the ideas immanent in nervou.pdf}
}

@article{logieCognitivePsychology,
  title = {Cognitive {{Psychology}}},
  author = {Logie, Professor Robert},
  langid = {english},
  file = {/Users/ron/Zotero/storage/VZ798DPB/Logie - Cognitive Psychology.pdf}
}

@article{luHyperbolicFunctionEmbedding2019,
  title = {Hyperbolic {{Function Embedding}}: {{Learning Hierarchical Representation}} for {{Functions}} of {{Source Code}} in {{Hyperbolic Space}}},
  shorttitle = {Hyperbolic {{Function Embedding}}},
  author = {Lu, Mingming and Liu, Yan and Li, Haifeng and Tan, Dingwu and He, Xiaoxian and Bi, Wenjie and Li, Wendbo},
  year = {2019},
  month = feb,
  journal = {Symmetry},
  volume = {11},
  number = {2},
  pages = {254},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-8994},
  urldate = {2023-05-08},
  abstract = {Recently, source code mining has received increasing attention due to the rapid increase of open-sourced code repositories and the tremendous values implied in this large dataset, which can help us understand the organization of functions or classes in different software and analyze the impact of these organized patterns on the software behaviors. Hence, learning an effective representation model for the functions of source code, from a modern view, is a crucial problem. Considering the inherent hierarchy of functions, we propose a novel hyperbolic function embedding (HFE) method, which can learn a distributed and hierarchical representation for each function via the Poincar{\'e} ball model. To achieve this, a function call graph (FCG) is first constructed to model the call relationship among functions. To verify the underlying geometry of FCG, the Ricci curvature model is used. Finally, an HFE model is built to learn the representations that can capture the latent hierarchy of functions in the hyperbolic space, instead of the Euclidean space, which are usually used in those state-of-the-art methods. Moreover, HFE is more compact in terms of lower dimensionality than the existing graph embedding methods. Thus, HFE is more effective in terms of computation and storage. To experimentally evaluate the performance of HFE, two application scenarios, namely, function classification and link prediction, have been applied. HFE achieves up to 7.6\% performance improvement compared to the chosen state-of-the-art methods, namely, Node2vec and Struc2vec.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {function embedding representation,function-call graph,hyperbolic space,source code mining},
  file = {/Users/ron/Zotero/storage/TJ86UALI/Lu et al. - 2019 - Hyperbolic Function Embedding Learning Hierarchic.pdf}
}

@misc{luoDiffusionProbabilisticModels2021,
  title = {Diffusion {{Probabilistic Models}} for {{3D Point Cloud Generation}}},
  author = {Luo, Shitong and Hu, Wei},
  year = {2021},
  month = jun,
  number = {arXiv:2103.01458},
  eprint = {2103.01458},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-06-27},
  abstract = {We present a probabilistic model for point cloud generation, which is fundamental for various 3D vision tasks such as shape completion, upsampling, synthesis and data augmentation. Inspired by the diffusion process in non-equilibrium thermodynamics, we view points in point clouds as particles in a thermodynamic system in contact with a heat bath, which diffuse from the original distribution to a noise distribution. Point cloud generation thus amounts to learning the reverse diffusion process that transforms the noise distribution to the distribution of a desired shape. Specifically, we propose to model the reverse diffusion process for point clouds as a Markov chain conditioned on certain shape latent. We derive the variational bound in closed form for training and provide implementations of the model. Experimental results demonstrate that our model achieves competitive performance in point cloud generation and auto-encoding. The code is available at {\textbackslash}url\{https://github.com/luost26/diffusion-point-cloud\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ron/Zotero/storage/788P3YQR/Luo and Hu - 2021 - Diffusion Probabilistic Models for 3D Point Cloud .pdf;/Users/ron/Zotero/storage/4ZH4XWJE/2103.html}
}

@article{mackayInformationTheoryInference,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J C},
  langid = {english},
  file = {/Users/ron/Zotero/storage/SCBP62VW/MacKay - Information Theory, Inference, and Learning Algori.pdf}
}

@misc{mahowaldDissociatingLanguageThought2023,
  title = {Dissociating Language and Thought in Large Language Models: A Cognitive Perspective},
  shorttitle = {Dissociating Language and Thought in Large Language Models},
  author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06627},
  eprint = {2301.06627},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ron/Zotero/storage/35D2I936/Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf;/Users/ron/Zotero/storage/K36JWDWA/2301.html}
}

@article{mairsonLinearLambdaCalculus,
  title = {Linear Lambda Calculus and {{PTIME-completeness}}},
  author = {Mairson, Harry G},
  journal = {Journal of Functional Programming},
  abstract = {We give transparent proofs of the PTIME-completeness of two decision problems for terms in the {$\lambda$}-calculus. The first is a reproof of the theorem that type inference for the simplytyped {$\lambda$}-calculus is PTIME-complete. Our proof is interesting because it uses no more than the standard combinators Church knew of some 70 years ago, in which the terms are linear affine{\textemdash}each bound variable occurs at most once.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/SELGQARQ/Mairson - Linear lambda calculus and PTIME-completeness.pdf}
}

@misc{malkinGFlowNetsVariationalInference2023,
  title = {{{GFlowNets}} and Variational Inference},
  author = {Malkin, Nikolay and Lahlou, Salem and Deleu, Tristan and Ji, Xu and Hu, Edward and Everett, Katie and Zhang, Dinghuai and Bengio, Yoshua},
  year = {2023},
  month = mar,
  number = {arXiv:2210.00580},
  eprint = {2210.00580},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {This paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/MPI6RAL4/Malkin et al. - 2023 - GFlowNets and variational inference.pdf;/Users/ron/Zotero/storage/3SYHRTCD/2210.html}
}

@misc{malkinTrajectoryBalanceImproved2022,
  title = {Trajectory Balance: {{Improved}} Credit Assignment in {{GFlowNets}}},
  shorttitle = {Trajectory Balance},
  author = {Malkin, Nikolay and Jain, Moksh and Bengio, Emmanuel and Sun, Chen and Bengio, Yoshua},
  year = {2022},
  month = oct,
  number = {arXiv:2201.13259},
  eprint = {2201.13259},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-11-07},
  abstract = {Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets, flow matching and detailed balance, which are analogous to temporal difference learning, to be prone to inefficient credit propagation across long action sequences. We thus propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/7MSINDRW/Malkin et al. - 2022 - Trajectory balance Improved credit assignment in .pdf;/Users/ron/Zotero/storage/IY683DI3/2201.html}
}

@article{maltRepresentingWorldLanguage,
  title = {Representing the {{World}} in {{Language}} and {{Thought}}},
  author = {Malt, Barbara C.},
  journal = {Topics in Cognitive Science},
  volume = {n/a},
  number = {n/a},
  issn = {1756-8765},
  urldate = {2024-01-13},
  abstract = {Internal representations guide our navigation of the world, while language allows us to share some of what is encoded internally with others. I have been interested in the content of thought, the nature of word meanings and what they reveal about thought, and how thoughts are expressed in words. My work has combined evidence from laboratory experimentation with observation of word use in natural settings, including from people who speak different languages. Some of the ideas guiding the work are these: understanding entities in the world non-linguistically engages different representations and processes than talking about them; patterns of word use in a language reflect cultural and linguistic history, not only conceptual representations of current speakers; linguistic and non-linguistic knowledge is therefore at least partially independent, and so language and thought will not always closely parallel one another; the beliefs people express about their concepts and word meanings may not accurately reflect the implicit knowledge they draw on in interacting with and talking about the world; and only by carefully observing actual word use can we understand how word meanings come about and how linguistic knowledge is used to select words for communication.},
  langid = {english},
  keywords = {Bilingual,Categorization,Concepts,Language,Lexicon,Semantics,Sense evolution,Sustainability,Thought,Word learning,Word meaning},
  file = {/Users/ron/Zotero/storage/KLMNCS53/Malt - Representing the World in Language and Thought.pdf;/Users/ron/Zotero/storage/UVXW2YTZ/tops.html}
}

@article{manchinProgramGenerationDiverse,
  title = {Program {{Generation}} from {{Diverse Video Demonstrations}}},
  author = {Manchin, Anthony},
  abstract = {The ability to use inductive reasoning to extract general rules from multiple observations is a vital indicator of intelligence. As humans, we use this ability to not only interpret the world around us, but also to predict the outcomes of the various interactions we experience. Generalising over multiple observations is a task that has historically presented difficulties for machines to grasp, especially when requiring computer vision. In this paper, we propose a model that can extract general rules from video demonstrations by simultaneously performing summarisation and translation. Our approach differs from prior works by framing the problem as a multi-sequence-to-sequence task, wherein summarisation is learnt by the model. This allows our model to utilise edge cases that would otherwise be suppressed or discarded by traditional summarisation techniques. Additionally, we show that our approach can handle noisy specifications without the need for additional filtering methods. We evaluate our model by synthesising programs from video demonstrations in the Vizdoom environment achieving state-of-the-art results with a relative increase of 11.75\% program accuracy on prior works.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/8VYKF75E/Manchin - Program Generation from Diverse Video Demonstratio.pdf}
}

@inproceedings{maoGrammarBasedGroundedLexicon2021,
  title = {Grammar-{{Based Grounded Lexicon Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mao, Jiayuan and Shi, Freda and Wu, Jiajun and Levy, Roger and Tenenbaum, Josh},
  year = {2021},
  volume = {34},
  pages = {7865--7878},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-05-08},
  file = {/Users/ron/Zotero/storage/CNTJJ5YJ/Mao et al. - 2021 - Grammar-Based Grounded Lexicon Learning.pdf}
}

@article{martinsHowChildrenPerceive2014,
  title = {How Children Perceive Fractals: {{Hierarchical}} Self-Similarity and Cognitive Development},
  shorttitle = {How Children Perceive Fractals},
  author = {Martins, Maur{\'i}cio Dias and Laaha, Sabine and Freiberger, Eva Maria and Choi, Soonja and Fitch, W. Tecumseh},
  year = {2014},
  month = oct,
  journal = {Cognition},
  volume = {133},
  number = {1},
  pages = {10--24},
  issn = {0010-0277},
  urldate = {2024-01-23},
  abstract = {The ability to understand and generate hierarchical structures is a crucial component of human cognition, available in language, music, mathematics and problem solving. Recursion is a particularly useful mechanism for generating complex hierarchies by means of self-embedding rules. In the visual domain, fractals are recursive structures in which simple transformation rules generate hierarchies of infinite depth. Research on how children acquire these rules can provide valuable insight into the cognitive requirements and learning constraints of recursion. Here, we used fractals to investigate the acquisition of recursion in the visual domain, and probed for correlations with grammar comprehension and general intelligence. We compared second (n=26) and fourth graders (n=26) in their ability to represent two types of rules for generating hierarchical structures: Recursive rules, on the one hand, which generate new hierarchical levels; and iterative rules, on the other hand, which merely insert items within hierarchies without generating new levels. We found that the majority of fourth graders, but not second graders, were able to represent both recursive and iterative rules. This difference was partially accounted by second graders' impairment in detecting hierarchical mistakes, and correlated with between-grade differences in grammar comprehension tasks. Empirically, recursion and iteration also differed in at least one crucial aspect: While the ability to learn recursive rules seemed to depend on the previous acquisition of simple iterative representations, the opposite was not true, i.e., children were able to acquire iterative rules before they acquired recursive representations. These results suggest that the acquisition of recursion in vision follows learning constraints similar to the acquisition of recursion in language, and that both domains share cognitive resources involved in hierarchical processing.},
  keywords = {Development,Hierarchy,Iteration,Language evolution,Recursion,Visuo-spatial},
  file = {/Users/ron/Zotero/storage/W7WWWDGF/S0010027714000997.html}
}

@article{matsuoDeepLearningReinforcement2022,
  title = {Deep Learning, Reinforcement Learning, and World Models},
  author = {Matsuo, Yutaka and LeCun, Yann and Sahani, Maneesh and Precup, Doina and Silver, David and Sugiyama, Masashi and Uchibe, Eiji and Morimoto, Jun},
  year = {2022},
  month = aug,
  journal = {Neural Networks},
  volume = {152},
  pages = {267--275},
  issn = {0893-6080},
  urldate = {2024-01-27},
  abstract = {Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the ``Deep Learning and Reinforcement Learning'' session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.},
  keywords = {Artificial intelligence,Deep learning,Machine learning,Reinforcement learning,World models},
  file = {/Users/ron/Zotero/storage/85WS7SAZ/Matsuo et al. - 2022 - Deep learning, reinforcement learning, and world m.pdf;/Users/ron/Zotero/storage/DIK2B2MW/S0893608022001150.html}
}

@article{mazzagliaFreeEnergyPrinciple2022,
  title = {The {{Free Energy Principle}} for {{Perception}} and {{Action}}: {{A Deep Learning Perspective}}},
  shorttitle = {The {{Free Energy Principle}} for {{Perception}} and {{Action}}},
  author = {Mazzaglia, Pietro and Verbelen, Tim and {\c C}atal, Ozan and Dhoedt, Bart},
  year = {2022},
  month = feb,
  journal = {Entropy},
  volume = {24},
  number = {2},
  eprint = {2207.06415},
  primaryclass = {cs, q-bio},
  pages = {301},
  issn = {1099-4300},
  urldate = {2023-09-28},
  abstract = {The free energy principle, and its corollary active inference, constitute a bio-inspired theory that assumes biological agents act to remain in a restricted set of preferred states of the world, i.e., they minimize their free energy. Under this principle, biological agents learn a generative model of the world and plan actions in the future that will maintain the agent in an homeostatic state that satisfies its preferences. This framework lends itself to being realized in silico, as it comprehends important aspects that make it computationally affordable, such as variational inference and amortized planning. In this work, we investigate the tool of deep learning to design and realize artificial agents based on active inference, presenting a deep-learning oriented presentation of the free energy principle, surveying works that are relevant in both machine learning and active inference areas, and discussing the design choices that are involved in the implementation process. This manuscript probes newer perspectives for the active inference framework, grounding its theoretical aspects into more pragmatic affairs, offering a practical guide to active inference newcomers and a starting point for deep learning practitioners that would like to investigate implementations of the free energy principle.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/ron/Zotero/storage/UPXE9BT5/Mazzaglia et al. - 2022 - The Free Energy Principle for Perception and Actio.pdf;/Users/ron/Zotero/storage/HH5BHAE8/2207.html}
}

@misc{mirchandaniLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{General Pattern Machines}}},
  author = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04721},
  eprint = {2307.04721},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-20},
  abstract = {We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Robotics},
  file = {/Users/ron/Zotero/storage/GVWUPXFA/Mirchandani et al. - 2023 - Large Language Models as General Pattern Machines.pdf;/Users/ron/Zotero/storage/N9D6KYJP/2307.html}
}

@article{moralesRepresentationLearningConcepts,
  title = {On the {{Representation}} and {{Learning}} of {{Concepts}}: {{Programs}}, {{Types}}, and {{Bayes}}},
  author = {Morales, Lucas Eduardo},
  pages = {145},
  abstract = {This thesis develops computational models of cognition with a focus on concept representation and learning. We start with brief philosophical discourse accompanied by empirical findings and theories from developmental science. We review many formal foundations of computation as well as modern approaches to the problem of program induction {\textemdash} the learning of structure within those representations. We show our own research on program induction focused on its application for language bootstrapping. We then demonstrate our approach for augmenting a class of machine learning algorithms to enable domain-general learning by applying it to a program induction algorithm. Finally, we present our own computational account of concepts and cognition.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/D4S42XJM/Morales - On the Representation and Learning of Concepts Pr.pdf}
}

@article{mordatchConceptLearningEnergyBased2018,
  title = {Concept {{Learning}} with {{Energy-Based Models}}},
  author = {Mordatch, Igor},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.02486 [cs]},
  eprint = {1811.02486},
  primaryclass = {cs},
  urldate = {2022-05-19},
  abstract = {Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at sites.google.com/site/energyconceptmodels},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/K9ALTIGW/Mordatch - 2018 - Concept Learning with Energy-Based Models.pdf;/Users/ron/Zotero/storage/SE73GNA3/1811.html}
}

@book{murphy2019introduction,
  title = {Introduction to {{AI}} Robotics},
  author = {Murphy, Robin R},
  year = {2019},
  publisher = {{MIT press}}
}

@book{murphyMachineLearningProbabilistic2013,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2013},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {4. print. (fixed many typos)},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-01802-9},
  langid = {english},
  file = {/Users/ron/Zotero/storage/ELUXKYUL/Murphy - 2013 - Machine learning a probabilistic perspective.pdf}
}

@article{nagao1984framework,
  title = {A Framework of a Mechanical Translation between {{Japanese}} and {{English}} by Analogy Principle},
  author = {Nagao, Makoto},
  year = {1984},
  journal = {Artificial and human intelligence},
  pages = {351--354}
}

@misc{NEF_principles,
  title = {{{CNRGlab}} @ {{UWaterloo}}: {{Research}}},
  author = {Eliasmith, Chris},
  year = {2013}
}

@inproceedings{nickelPoincareEmbeddingsLearning2017,
  title = {Poincar{\'e} {{Embeddings}} for {{Learning Hierarchical Representations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Nickel, Maximillian and Kiela, Douwe},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-05-08},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar{\'e} ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar{\'e} embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  file = {/Users/ron/Zotero/storage/V7QLFFDV/Nickel and Kiela - 2017 - Poincaré Embeddings for Learning Hierarchical Repr.pdf}
}

@article{nikolicPractopoiesisHowLife2015,
  title = {Practopoiesis: {{Or}} How Life Fosters a Mind},
  shorttitle = {Practopoiesis},
  author = {Nikoli{\'c}, Danko},
  year = {2015},
  month = may,
  journal = {Journal of Theoretical Biology},
  volume = {373},
  pages = {40--61},
  issn = {0022-5193},
  urldate = {2023-05-08},
  abstract = {The mind is a biological phenomenon. Thus, biological principles of organization should also be the principles underlying mental operations. Practopoiesis states that the key for achieving intelligence through adaptation is an arrangement in which mechanisms laying at a lower level of organization, by their operations and interaction with the environment, enable creation of mechanisms laying at a higher level of organization. When such an organizational advance of a system occurs, it is called a traverse. A case of traverse is when plasticity mechanisms (at a lower level of organization), by their operations, create a neural network anatomy (at a higher level of organization). Another case is the actual production of behavior by that network, whereby the mechanisms of neuronal activity operate to create motor actions. Practopoietic theory explains why the adaptability of a system increases with each increase in the number of traverses. With a larger number of traverses, a system can be relatively small and yet, produce a higher degree of adaptive/intelligent behavior than a system with a lower number of traverses. The present analyses indicate that the two well-known traverses {\textendash} neural plasticity and neural activity {\textendash} are not sufficient to explain human mental capabilities. At least one additional traverse is needed, which is named anapoiesis for its contribution in reconstructing knowledge e.g., from long-term memory into working memory. The conclusions bear implications for brain theory, the mind{\textendash}body explanatory gap, and developments of artificial intelligence technologies.},
  langid = {english},
  keywords = {Adaptive systems,Cybernetics,Intelligent behavior,Neurophysiology},
  file = {/Users/ron/Zotero/storage/2LA8E5TI/Nikolić - 2015 - Practopoiesis Or how life fosters a mind.pdf;/Users/ron/Zotero/storage/4BXZN64Q/S002251931500106X.html}
}

@article{nitschkePeacefulPillHandbook,
  title = {The {{Peaceful Pill Handbook}}},
  author = {Nitschke, Dr Philip},
  langid = {english},
  file = {/Users/ron/Zotero/storage/D57D87B5/Nitschke - The Peaceful Pill Handbook.pdf}
}

@misc{noauthor_dreaming_nodate,
  title = {Dreaming and {{Narration}} {\textbar} the Living Handbook of Narratology},
  urldate = {2023-05-28},
  file = {/Users/ron/Zotero/storage/D9KYKZ3X/70.html}
}

@article{noriEfficientSynthesisProbabilistic2015,
  title = {Efficient Synthesis of Probabilistic Programs},
  author = {Nori, Aditya V. and Ozair, Sherjil and Rajamani, Sriram K. and Vijaykeerthy, Deepak},
  year = {2015},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {50},
  number = {6},
  pages = {208--217},
  issn = {0362-1340},
  urldate = {2024-01-23},
  abstract = {We show how to automatically synthesize probabilistic programs from real-world datasets. Such a synthesis is feasible due to a combination of two techniques: (1) We borrow the idea of ``sketching'' from synthesis of deterministic programs, and allow the programmer to write a skeleton program with ``holes''. Sketches enable the programmer to communicate domain-specific intuition about the structure of the desired program and prune the search space, and (2) we design an efficient Markov Chain Monte Carlo (MCMC) based synthesis algorithm to instantiate the holes in the sketch with program fragments. Our algorithm efficiently synthesizes a probabilistic program that is most consistent with the data. A core difficulty in synthesizing probabilistic programs is computing the likelihood L(P {\textbar} D) of a candidate program P generating data D. We propose an approximate method to compute likelihoods using mixtures of Gaussian distributions, thereby avoiding expensive computation of integrals. The use of such approximations enables us to speed up evaluation of the likelihood of candidate programs by a factor of 1000, and makes Markov Chain Monte Carlo based search feasible. We have implemented our algorithm in a tool called PSKETCH, and our results are encouraging PSKETCH is able to automatically synthesize 16 non-trivial real-world probabilistic programs.},
  keywords = {Markov Chain Monte Carlo,Probabilistic Programming,Program Synthesis}
}

@article{norouzi2012hamming,
  title = {Hamming Distance Metric Learning},
  author = {Norouzi, Mohammad and Fleet, David J and Salakhutdinov, Russ R},
  year = {2012},
  journal = {Advances in neural information processing systems},
  volume = {25}
}

@misc{nyeImprovingCoherenceConsistency2021,
  title = {Improving {{Coherence}} and {{Consistency}} in {{Neural Sequence Models}} with {{Dual-System}}, {{Neuro-Symbolic Reasoning}}},
  author = {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B. and Lake, Brenden M.},
  year = {2021},
  month = dec,
  number = {arXiv:2107.02794},
  eprint = {2107.02794},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-10},
  abstract = {Human reasoning can often be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models -- which have been increasingly successful at performing complex, structured tasks -- exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/8L35HG43/Nye et al. - 2021 - Improving Coherence and Consistency in Neural Sequ.pdf;/Users/ron/Zotero/storage/E86K25FW/2107.html}
}

@misc{nyeLearningInferProgram2019,
  title = {Learning to {{Infer Program Sketches}}},
  author = {Nye, Maxwell and Hewitt, Luke and Tenenbaum, Joshua and {Solar-Lezama}, Armando},
  year = {2019},
  month = jun,
  number = {arXiv:1902.06349},
  eprint = {1902.06349},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-12},
  abstract = {Our goal is to build systems which write code automatically from the kinds of specifications humans can most easily provide, such as examples and natural language instruction. The key idea of this work is that a flexible combination of pattern recognition and explicit reasoning can be used to solve these complex programming problems. We propose a method for dynamically integrating these types of information. Our novel intermediate representation and training algorithm allow a program synthesis system to learn, without direct supervision, when to rely on pattern recognition and when to perform symbolic search. Our model matches the memorization and generalization performance of neural synthesis and symbolic search, respectively, and achieves state-of-the-art performance on a dataset of simple English description-to-code programming problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/UU6X29X5/Nye et al. - 2019 - Learning to Infer Program Sketches.pdf;/Users/ron/Zotero/storage/BI2Y8GYL/1902.html}
}

@article{oizumiPhenomenologyMechanismsConsciousness2014,
  title = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}: {{Integrated Information Theory}} 3.0},
  shorttitle = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}},
  author = {Oizumi, Masafumi and Albantakis, Larissa and Tononi, Giulio},
  editor = {Sporns, Olaf},
  year = {2014},
  month = may,
  journal = {PLoS Computational Biology},
  volume = {10},
  number = {5},
  pages = {e1003588},
  issn = {1553-7358},
  urldate = {2024-01-18},
  abstract = {This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific {\textendash} it is what it is by how it differs from alternative experiences; integration says that it is unified {\textendash} irreducible to noninterdependent components; exclusion says that it has unique borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ``differences that make a difference'' within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (MICS, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a complex. According to IIT, a MICS specifies the quality of an experience and integrated information WMax its quantity. From the theory follow several results, including: a system of mechanisms may condense into a major complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true ``zombies'' {\textendash} unconscious feed-forward systems that are functionally equivalent to conscious complexes.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/43TWUZEQ/Oizumi et al. - 2014 - From the Phenomenology to the Mechanisms of Consci.pdf}
}

@inproceedings{oliveiraAbstractSyntaxGraphs2013,
  title = {Abstract Syntax Graphs for Domain Specific Languages},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2013 Workshop on {{Partial}} Evaluation and Program Manipulation},
  author = {Oliveira, Bruno C. D. S. and L{\"o}h, Andres},
  year = {2013},
  month = jan,
  pages = {87--96},
  publisher = {{ACM}},
  address = {{Rome Italy}},
  urldate = {2023-05-08},
  abstract = {An important problem in the context of embedded domain specific languages (EDSLs) is how to provide easy to use, yet expressive representations of abstract syntax. So far providing user-friendly encodings of abstract syntax that enable operations that observe or preserve sharing and recursion has proved to be quite elusive.},
  isbn = {978-1-4503-1842-6},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CE6IREWV/Oliveira and Löh - 2013 - Abstract syntax graphs for domain specific languag.pdf}
}

@article{palmer1978fundamental,
  title = {Fundamental Aspects of Cognitive Representation},
  author = {Palmer, Stephen},
  year = {1978}
}

@misc{parkSocialSimulacraCreating2022,
  title = {Social {{Simulacra}}: {{Creating Populated Prototypes}} for {{Social Computing Systems}}},
  shorttitle = {Social {{Simulacra}}},
  author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2022},
  month = aug,
  number = {arXiv:2208.04024},
  eprint = {2208.04024},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-27},
  abstract = {Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer's description of a community's design -- goal, rules, and member personas -- and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of "what if?" scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models' training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/ron/Zotero/storage/4E6SP2GB/Park et al. - 2022 - Social Simulacra Creating Populated Prototypes fo.pdf;/Users/ron/Zotero/storage/KI8YQZVB/2208.html}
}

@article{parkStructuralFunctionalBrain2013,
  title = {Structural and {{Functional Brain Networks}}: {{From Connections}} to {{Cognition}}},
  shorttitle = {Structural and {{Functional Brain Networks}}},
  author = {Park, Hae-Jeong and Friston, Karl},
  year = {2013},
  month = nov,
  journal = {Science},
  volume = {342},
  number = {6158},
  pages = {1238411},
  issn = {0036-8075, 1095-9203},
  urldate = {2024-01-23},
  abstract = {How rich functionality emerges from the invariant structural architecture of the brain remains a major mystery in neuroscience. Recent applications of network theory and theoretical neuroscience to large-scale brain networks have started to dissolve this mystery. Network analyses suggest that hierarchical modular brain networks are particularly suited to facilitate local (segregated) neuronal operations and the global integration of segregated functions. Although functional networks are constrained by structural connections, context-sensitive integration during cognition tasks necessarily entails a divergence between structural and functional networks. This degenerate (many-to-one) function-structure mapping is crucial for understanding the nature of brain networks. The emergence of dynamic functional networks from static structural connections calls for a formal (computational) approach to neuronal information processing that may resolve this dialectic between structure and function.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/8ZFLXLL4/Park and Friston - 2013 - Structural and Functional Brain Networks From Con.pdf}
}

@article{pastraMinimalistGrammarAction2012,
  title = {The Minimalist Grammar of Action},
  author = {Pastra, Katerina and Aloimonos, Yiannis},
  year = {2012},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  number = {1585},
  pages = {103},
  publisher = {{The Royal Society}},
  urldate = {2023-03-14},
  abstract = {Language and action have been found to share a common neural basis and in particular a common `syntax', an analogous hierarchical and compositional organization. While language structure analysis has led to the formulation of different ...},
  langid = {english},
  pmid = {22106430},
  file = {/Users/ron/Zotero/storage/Q2MB57PB/PMC3223786.html}
}

@article{pavlickSymbolsGroundingLarge2023,
  title = {Symbols and Grounding in Large Language Models},
  author = {Pavlick, Ellie},
  year = {2023},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {381},
  number = {2251},
  pages = {20220041},
  issn = {1364-503X, 1471-2962},
  urldate = {2023-07-12},
  abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study~of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models' performance on challenging language understanding tasks, this article argues that the answer depends on models' underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs' ability (or lack thereof) to offer insights on human language representation and understanding.             This article is part of a discussion meeting issue `Cognitive artificial intelligence'.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/5BKZPF4T/Pavlick - 2023 - Symbols and grounding in large language models.pdf}
}

@inproceedings{pearlTheoreticalImpedimentsMachine2018,
  title = {Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Pearl, Judea},
  year = {2018},
  month = feb,
  pages = {3--3},
  publisher = {{ACM}},
  address = {{Marina Del Rey CA USA}},
  urldate = {2023-10-28},
  abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.},
  isbn = {978-1-4503-5581-0},
  langid = {english},
  file = {/Users/ron/Zotero/storage/JSCDKVT8/Pearl - 2018 - Theoretical Impediments to Machine Learning With S.pdf}
}

@article{pearsonHumanImaginationCognitive2019,
  title = {The Human Imagination: The Cognitive Neuroscience of Visual Mental Imagery},
  shorttitle = {The Human Imagination},
  author = {Pearson, Joel},
  year = {2019},
  month = oct,
  journal = {Nature Reviews Neuroscience},
  volume = {20},
  number = {10},
  pages = {624--634},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  urldate = {2024-01-05},
  abstract = {Mental imagery can be advantageous, unnecessary and even clinically disruptive. With methodological constraints now overcome, research has shown that visual imagery involves a network of brain areas from the frontal cortex to sensory areas, overlapping with the~default mode network, and can function much like a weak version of afferent perception. Imagery vividness and strength range from completely absent (aphantasia) to photo-like (hyperphantasia). Both the anatomy and function of the primary visual cortex are related to visual imagery. The use of imagery as a tool has been linked to many compound cognitive processes and imagery plays both symptomatic and mechanistic roles in neurological and mental disorders and treatments.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Object vision,Sensory systems,Working memory}
}

@inproceedings{pengRethinkingPositionalEncoding2022,
  title = {Rethinking {{Positional Encoding}} in {{Tree Transformer}} for {{Code Representation}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Peng, Han and Li, Ge and Zhao, Yunfei and Jin, Zhi},
  year = {2022},
  month = dec,
  pages = {3204--3214},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  urldate = {2023-05-12},
  abstract = {Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we propose a novel tree Transformer encoding node positions based on our new description method for tree structures.Technically, local and global soft bias shown in previous works is both introduced as positional encodings of our Transformer model.Our model finally outperforms strong baselines on code summarization and completion tasks across two languages, demonstrating our model's effectiveness.Besides, extensive experiments and ablation study shows that combining both local and global paradigms is still helpful in improving model performance. We release our code at https://github.com/AwdHanPeng/TreeTransformer.},
  file = {/Users/ron/Zotero/storage/BZWW6KKS/Peng et al. - 2022 - Rethinking Positional Encoding in Tree Transformer.pdf}
}

@article{piantadosiComputationalOriginRepresentation2021,
  title = {The {{Computational Origin}} of {{Representation}}},
  author = {Piantadosi, Steven T.},
  year = {2021},
  month = mar,
  journal = {Minds and Machines},
  volume = {31},
  number = {1},
  pages = {1--58},
  issn = {0924-6495, 1572-8641},
  urldate = {2022-12-12},
  abstract = {Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations{\textemdash}whatever they are{\textemdash}must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantification, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an ``assembly language'' for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CYEV29MV/Piantadosi - 2021 - The Computational Origin of Representation.pdf}
}

@article{piantadosiLogicalPrimitivesThought20160414,
  title = {The Logical Primitives of Thought: {{Empirical}} Foundations for Compositional Cognitive Models.},
  shorttitle = {The Logical Primitives of Thought},
  author = {Piantadosi, Steven T. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  year = {20160414},
  journal = {Psychological Review},
  volume = {123},
  number = {4},
  pages = {392},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1471},
  urldate = {2022-12-25},
  file = {/Users/ron/Zotero/storage/3A6BFDZZ/2016-18127-001.html}
}

@misc{piantadosiMeaningReferenceLarge2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantadosi, Steven T. and Hill, Felix},
  year = {2022},
  month = aug,
  number = {arXiv:2208.02957},
  eprint = {2208.02957},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-10},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ron/Zotero/storage/SEV26R2Z/Piantadosi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/ron/Zotero/storage/GVJLHDWH/2208.html}
}

@article{piantadosiModernLanguageModels,
  title = {Modern Language Models Refute {{Chomsky}}'s Approach to Language},
  author = {Piantadosi, Steven T},
  langid = {english},
  file = {/Users/ron/Zotero/storage/9IJEBYEY/Piantadosi - Modern language models refute Chomsky’s approach t.pdf}
}

@article{piccininiComputationRepresentation2008,
  title = {Computation without {{Representation}}},
  author = {Piccinini, Gualtiero},
  year = {2008},
  month = jan,
  journal = {Philosophical Studies},
  volume = {137},
  number = {2},
  pages = {205--241},
  issn = {1573-0883},
  urldate = {2023-03-14},
  abstract = {The received view is that computational states are individuated at least in part by their semantic properties. I offer an alternative, according to which computational states are individuated by their functional properties. Functional properties are specified by a mechanistic explanation without appealing to any semantic properties. The primary purpose of this paper is to formulate the alternative view of computational individuation, point out that it supports a robust notion of computational explanation, and defend it on the grounds of how computational states are individuated within computability theory and computer science. A secondary purpose is to show that existing arguments for the semantic view are defective.},
  langid = {english},
  keywords = {Computational State,Mechanistic Explanation,Semantic Property,Semantic View,Syntactic Property}
}

@article{piccininiComputationRepresentationCognitive2018,
  title = {Computation and {{Representation}} in {{Cognitive Neuroscience}}},
  author = {Piccinini, Gualtiero},
  year = {2018},
  month = mar,
  journal = {Minds and Machines},
  volume = {28},
  number = {1},
  pages = {1--6},
  issn = {1572-8641},
  urldate = {2024-01-23},
  langid = {english},
  file = {/Users/ron/Zotero/storage/I5GS5RTA/Piccinini - 2018 - Computation and Representation in Cognitive Neuros.pdf}
}

@article{piccininiMindNeuralSoftware2010,
  title = {The {{Mind}} as {{Neural Software}}? {{Understanding Functionalism}}, {{Computationalism}}, and {{Computational Functionalism}}},
  shorttitle = {The {{Mind}} as {{Neural Software}}?},
  author = {Piccinini, Gualtiero},
  year = {2010},
  journal = {Philosophy and Phenomenological Research},
  volume = {81},
  number = {2},
  pages = {269--311},
  issn = {1933-1592},
  urldate = {2023-05-09},
  abstract = {Defending or attacking either functionalism or computationalism requires clarity on what they amount to and what evidence counts for or against them. My goal here is not to evaluate their plausibility. My goal is to formulate them and their relationship clearly enough that we can determine which type of evidence is relevant to them. I aim to dispel some sources of confusion that surround functionalism and computationalism, recruit recent philosophical work on mechanisms and computation to shed light on them, and clarify how functionalism and computationalism may or may not legitimately come together.},
  langid = {english},
  keywords = {Done},
  file = {/Users/ron/Zotero/storage/AYQA9JFM/Piccinini - 2010 - The Mind as Neural Software Understanding Functio.pdf;/Users/ron/Zotero/storage/BYH3NRX8/j.1933-1592.2010.00356.html}
}

@article{piccininiNeuralComputationComputational2013,
  title = {Neural {{Computation}} and the {{Computational Theory}} of {{Cognition}}},
  author = {Piccinini, Gualtiero and Bahar, Sonya},
  year = {2013},
  journal = {Cognitive Science},
  volume = {37},
  number = {3},
  pages = {453--488},
  issn = {1551-6709},
  urldate = {2023-05-11},
  abstract = {We begin by distinguishing computationalism from a number of other theses that are sometimes conflated with it. We also distinguish between several important kinds of computation: computation in a generic sense, digital computation, and analog computation. Then, we defend a weak version of computationalism{\textemdash}neural processes are computations in the generic sense. After that, we reject on empirical grounds the common assimilation of neural computation to either analog or digital computation, concluding that neural computation is sui generis. Analog computation requires continuous signals; digital computation requires strings of digits. But current neuroscientific evidence indicates that typical neural signals, such as spike trains, are graded like continuous signals but are constituted by discrete functional elements (spikes); thus, typical neural signals are neither continuous signals nor strings of digits. It follows that neural computation is sui generis. Finally, we highlight three important consequences of a proper understanding of neural computation for the theory of cognition. First, understanding neural computation requires a specially designed mathematical theory (or theories) rather than the mathematical theories of analog or digital computation. Second, several popular views about neural computation turn out to be incorrect. Third, computational theories of cognition that rely on non-neural notions of computation ought to be replaced or reinterpreted in terms of neural computation.},
  langid = {english},
  keywords = {Analog computation,Computational theory of cognition,Digital computation,Neural computation,Spike trains},
  file = {/Users/ron/Zotero/storage/FYE4FDVS/Piccinini and Bahar - 2013 - Neural Computation and the Computational Theory of.pdf;/Users/ron/Zotero/storage/E43CBLGH/cogs.html}
}

@article{piccininiSituatedNeuralRepresentations2022,
  title = {Situated {{Neural Representations}}: {{Solving}} the {{Problems}} of {{Content}}},
  shorttitle = {Situated {{Neural Representations}}},
  author = {Piccinini, Gualtiero},
  year = {2022},
  journal = {Frontiers in Neurorobotics},
  volume = {16},
  issn = {1662-5218},
  urldate = {2023-07-12},
  abstract = {Situated approaches to cognition maintain that cognition is embodied, embedded, enactive, and affective (and extended, but that is not relevant here). Situated approaches are often pitched as alternatives to computational and representational approaches, according to which cognition is computation over representations. I argue that, far from being opposites, situatedness and neural representation are more deeply intertwined than anyone suspected. To show this, I introduce a neurocomputational account of cognition that relies on neural representations. I argue not only that this account is compatible with (non-question-begging) situated approaches, but also that it requires embodiment, embeddedness, enaction, and affect at its very core. That is, constructing neural representations and their semantic content, and learning computational processes appropriate for their content, requires a tight dynamic interaction between nervous system, body, and environment. Most importantly, I argue that situatedness is needed to give a satisfactory account of neural representation: neurocognitive systems that are embodied, embedded, affective, dynamically interact with their environment, and use feedback from their interaction to shape their own representations and computations (1) can construct neural representations with original semantic content, (2) their neural vehicles and the way they are processed are automatically coordinated with their content, (3) such content is causally efficacious, (4) is determinate enough for the system's purposes, (5) represents the distal stimulus, and (6) can misrepresent. This proposal hints at what is needed to build artifacts with some of the basic cognitive capacities possessed by neurocognitive systems.},
  file = {/Users/ron/Zotero/storage/INP8WSHM/Piccinini - 2022 - Situated Neural Representations Solving the Probl.pdf}
}

@book{pinker2003blank,
  title = {The Blank Slate: {{The}} Modern Denial of Human Nature},
  author = {Pinker, Steven},
  year = {2003},
  publisher = {{Penguin}}
}

@article{pournarasHolarchicStructuresDecentralized2020,
  title = {Holarchic Structures for Decentralized Deep Learning: A Performance Analysis},
  shorttitle = {Holarchic Structures for Decentralized Deep Learning},
  author = {Pournaras, Evangelos and Yadhunathan, Srivatsan and Diaconescu, Ada},
  year = {2020},
  month = mar,
  journal = {Cluster Computing},
  volume = {23},
  number = {1},
  pages = {219--240},
  issn = {1573-7543},
  urldate = {2023-05-08},
  abstract = {Structure plays a key role in learning performance. In centralized computational systems, hyperparameter optimization and regularization techniques such as dropout are computational means to enhance learning performance by adjusting the deep hierarchical structure. However, in decentralized deep learning by the Internet of Things, the structure is an actual network of autonomous interconnected devices such as smart phones that interact via complex network protocols. Self-adaptation of the learning structure is a challenge. Uncertainties such as network latency, node and link failures or even bottlenecks by limited processing capacity and energy availability can significantly downgrade learning performance. Network self-organization and self-management is complex, while it requires additional computational and network resources that hinder the feasibility of decentralized deep learning. In contrast, this paper introduces a self-adaptive learning approach based on holarchic learning structures for exploring, mitigating and boosting learning performance in distributed environments with uncertainties. A large-scale performance analysis with 864,000 experiments fed with synthetic and real-world data from smart grid and smart city pilot projects confirm the cost-effectiveness of holarchic structures for decentralized deep learning.},
  langid = {english},
  keywords = {Deep learning,Holarchy,Multi-agent system,Optimization,Resilience,Smart city},
  file = {/Users/ron/Zotero/storage/78WANKEM/Pournaras et al. - 2020 - Holarchic structures for decentralized deep learni.pdf}
}

@article{qin2020back,
  title = {Back to the Future: {{Unsupervised}} Backprop-Based Decoding for Counterfactual and Abductive Commonsense Reasoning},
  author = {Qin, Lianhui and Shwartz, Vered and West, Peter and Bhagavatula, Chandra and Hwang, Jena and Bras, Ronan Le and Bosselut, Antoine and Choi, Yejin},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.05906},
  eprint = {2010.05906},
  archiveprefix = {arxiv}
}

@article{qiuPROGRAMMATICREINFORCEMENTLEARNING2022,
  title = {{{PROGRAMMATIC REINFORCEMENT LEARNING WITHOUT ORACLES}}},
  author = {Qiu, Wenjie and Zhu, He},
  year = {2022},
  abstract = {Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/NM3SBVJJ/Qiu and Zhu - 2022 - PROGRAMMATIC REINFORCEMENT LEARNING WITHOUT ORACLE.pdf}
}

@article{quilty-dunnBestGameTown2022,
  title = {The Best Game in Town: {{The}} Reemergence of the Language-of-Thought Hypothesis across the Cognitive Sciences},
  shorttitle = {The Best Game in Town},
  author = {{Quilty-Dunn}, Jake and Porot, Nicolas and Mandelbaum, Eric},
  year = {2022},
  month = dec,
  journal = {The Behavioral and Brain Sciences},
  volume = {46},
  pages = {e261},
  issn = {1469-1825},
  abstract = {Mental representations remain the central posits of psychology after many decades of scrutiny. However, there is no consensus about the representational format(s) of biological cognition. This paper provides a survey of evidence from computational cognitive psychology, perceptual psychology, developmental psychology, comparative psychology, and social psychology, and concludes that one type of format that routinely crops up is the language-of-thought (LoT). We outline six core properties of LoTs: (i) discrete constituents; (ii) role-filler independence; (iii) predicate-argument structure; (iv) logical operators; (v) inferential promiscuity; and (vi) abstract content. These properties cluster together throughout cognitive science. Bayesian computational modeling, compositional features of object perception, complex infant and animal reasoning, and automatic, intuitive cognition in adults all implicate LoT-like structures. Instead of regarding LoT as a relic of the previous century, researchers in cognitive science and philosophy-of-mind must take seriously the explanatory breadth of LoT-based architectures. We grant that the mind may harbor many formats and architectures, including iconic and associative structures as well as deep-neural-network-like architectures. However, as computational/representational approaches to the mind continue to advance, classical compositional symbolic structures - that is, LoTs - only prove more flexible and well-supported over time.},
  langid = {english},
  pmid = {36471543},
  keywords = {Adult,animal cognition,Animals,automaticity,Bayes Theorem,Cognition,cognitive architecture,Cognitive Science,deep learning,dual-process theories,Humans,implicit attitudes,infant cognition,Language,language-of-thought,object files,visual cognition},
  file = {/Users/ron/Zotero/storage/XN5BHY5F/Quilty-Dunn et al. - 2022 - The best game in town The reemergence of the lang.pdf}
}

@book{ramonycajalAdviceYoungInvestigator1999,
  title = {Advice for a Young Investigator},
  author = {{Ram{\'o}n y Cajal}, Santiago},
  year = {1999},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18191-4},
  langid = {english},
  lccn = {Q180.A1 R33313 1999},
  keywords = {Research,Scientists},
  file = {/Users/ron/Zotero/storage/A6TTCPV8/Ramón y Cajal - 1999 - Advice for a young investigator.pdf}
}

@article{ramsteadGenerativeModelsGenerative2022,
  title = {From {{Generative Models}} to {{Generative Passages}}: {{A Computational Approach}} to ({{Neuro}}) {{Phenomenology}}},
  shorttitle = {From {{Generative Models}} to {{Generative Passages}}},
  author = {Ramstead, Maxwell J. D. and Seth, Anil K. and Hesp, Casper and {Sandved-Smith}, Lars and Mago, Jonas and Lifshitz, Michael and Pagnoni, Giuseppe and Smith, Ryan and Dumas, Guillaume and Lutz, Antoine and Friston, Karl and Constant, Axel},
  year = {2022},
  journal = {Review of Philosophy and Psychology},
  volume = {13},
  number = {4},
  pages = {829--857},
  issn = {1878-5158},
  urldate = {2023-07-12},
  abstract = {This paper presents a version of neurophenomenology based on generative modelling techniques developed in computational neuroscience and biology. Our approach can be described as computational phenomenology because it applies methods originally developed in computational modelling to provide a formal model of the descriptions of lived experience in the phenomenological tradition of philosophy (e.g., the work of Edmund Husserl, Maurice Merleau-Ponty, etc.). The first section presents a brief review of the overall project to naturalize phenomenology. The second section presents and evaluates philosophical objections to that project and situates our version of computational phenomenology with respect to these projects. The third section reviews the generative modelling framework. The final section presents our approach in detail. We conclude by discussing how our approach differs from previous attempts to use generative modelling to help understand consciousness. In summary, we describe a version of computational phenomenology which uses generative modelling to construct a computational model of the inferential or interpretive processes that best explain this or that kind of lived experience.},
  pmcid = {PMC8932094},
  pmid = {35317021},
  file = {/Users/ron/Zotero/storage/UT6PPX79/Ramstead et al. - 2022 - From Generative Models to Generative Passages A C.pdf}
}

@article{ramsteadMultiscaleIntegrationInternalism2021,
  title = {Multiscale Integration: Beyond Internalism and Externalism},
  shorttitle = {Multiscale Integration},
  author = {Ramstead, Maxwell J. D. and Kirchhoff, Michael D. and Constant, Axel and Friston, Karl J.},
  year = {2021},
  journal = {Synthese},
  volume = {198},
  number = {Suppl 1},
  pages = {41--70},
  issn = {0039-7857},
  abstract = {We present a multiscale integrationist interpretation of the boundaries of cognitive systems, using the Markov blanket formalism of the variational free energy principle. This interpretation is intended as a corrective for the philosophical debate over internalist and externalist interpretations of cognitive boundaries; we stake out a compromise position. We first survey key principles of new radical (extended, enactive, embodied) views of cognition. We then describe an internalist interpretation premised on the Markov blanket formalism. Having reviewed these accounts, we develop our positive multiscale account. We argue that the statistical seclusion of internal from external states of the system-entailed by the existence of a Markov boundary-can coexist happily with the multiscale integration of the system through its dynamics. Our approach does not privilege any given boundary (whether it be that of the brain, body, or world), nor does it argue that all boundaries are equally prescient. We argue that the relevant boundaries of cognition depend on the level being characterised and the explanatory interests that guide investigation. We approach the issue of how and where to draw the boundaries of cognitive systems through a multiscale ontology of cognitive systems, which offers a multidisciplinary research heuristic for cognitive science.},
  langid = {english},
  pmcid = {PMC7873008},
  pmid = {33627890},
  keywords = {Boundaries of cognition,Embodied cognition,Enactive cognition,Externalism,Internalism,Markov blankets,Variational free energy principle},
  file = {/Users/ron/Zotero/storage/JYAJD9YN/Ramstead et al. - 2021 - Multiscale integration beyond internalism and ext.pdf}
}

@article{ramsteadTaleTwoDensities2020,
  title = {A Tale of Two Densities: Active Inference Is Enactive Inference},
  shorttitle = {A Tale of Two Densities},
  author = {Ramstead, Maxwell JD and Kirchhoff, Michael D and Friston, Karl J},
  year = {2020},
  month = aug,
  journal = {Adaptive Behavior},
  volume = {28},
  number = {4},
  pages = {225--239},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {1059-7123},
  urldate = {2023-10-23},
  abstract = {The aim of this article is to clarify how best to interpret some of the central constructs that underwrite the free-energy principle (FEP) {\textendash} and its corollary, active inference {\textendash} in theoretical neuroscience and biology: namely, the role that generative models and variational densities play in this theory. We argue that these constructs have been systematically misrepresented in the literature, because of the conflation between the FEP and active inference, on the one hand, and distinct (albeit closely related) Bayesian formulations, centred on the brain {\textendash} variously known as predictive processing, predictive coding or the prediction error minimisation framework. More specifically, we examine two contrasting interpretations of these models: a structural representationalist interpretation and an enactive interpretation. We argue that the structural representationalist interpretation of generative and recognition models does not do justice to the role that these constructs play in active inference under the FEP. We propose an enactive interpretation of active inference {\textendash} what might be called enactive inference. In active inference under the FEP, the generative and recognition models are best cast as realising inference and control {\textendash} the self-organising, belief-guided selection of action policies {\textendash} and do not have the properties ascribed by structural representationalists.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/VYUIJF32/Ramstead et al. - 2020 - A tale of two densities active inference is enact.pdf}
}

@article{rauss_what_2013,
  title = {What Is {{Bottom-Up}} and {{What}} Is {{Top-Down}} in {{Predictive Coding}}?},
  author = {Rauss, Karsten and Pourtois, Gilles},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  urldate = {2022-05-14},
  abstract = {Everyone knows what bottom-up is, and how it is different from top-down. At least one is tempted to think so, given that both terms are ubiquitously used, but only rarely defined in the psychology and neuroscience literature. In this review, we highlight the problems and limitations of our current understanding of bottom-up and top-down processes, and we propose a reformulation of this distinction in terms of predictive coding.},
  file = {/Users/ron/Zotero/storage/INT5KSTL/Rauss and Pourtois - 2013 - What is Bottom-Up and What is Top-Down in Predicti.pdf}
}

@article{raussWhatBottomUpWhat2013,
  title = {What Is {{Bottom-Up}} and {{What}} Is {{Top-Down}} in {{Predictive Coding}}?},
  author = {Rauss, Karsten and Pourtois, Gilles},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  urldate = {2022-05-14},
  abstract = {Everyone knows what bottom-up is, and how it is different from top-down. At least one is tempted to think so, given that both terms are ubiquitously used, but only rarely defined in the psychology and neuroscience literature. In this review, we highlight the problems and limitations of our current understanding of bottom-up and top-down processes, and we propose a reformulation of this distinction in terms of predictive coding.},
  file = {/Users/ron/Zotero/storage/ZJLPPHZL/Rauss and Pourtois - 2013 - What is Bottom-Up and What is Top-Down in Predicti.pdf}
}

@article{rohrmeierPrinciplesStructureBuilding2015,
  title = {Principles of Structure Building in Music, Language and Animal Song},
  author = {Rohrmeier, Martin and Zuidema, Willem and Wiggins, Geraint A. and Scharff, Constance},
  year = {2015},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {370},
  number = {1664},
  pages = {20140097},
  publisher = {{Royal Society}},
  urldate = {2023-12-21},
  abstract = {Human language, music and a variety of animal vocalizations constitute ways of sonic communication that exhibit remarkable structural complexity. While the complexities of language and possible parallels in animal communication have been discussed intensively, reflections on the complexity of music and animal song, and their comparisons, are underrepresented. In some ways, music and animal songs are more comparable to each other than to language as propositional semantics cannot be used as indicator of communicative success or wellformedness, and notions of grammaticality are less easily defined. This review brings together accounts of the principles of structure building in music and animal song. It relates them to corresponding models in formal language theory, the extended Chomsky hierarchy (CH), and their probabilistic counterparts. We further discuss common misunderstandings and shortcomings concerning the CH and suggest ways to move beyond. We discuss language, music and animal song in the context of their function and motivation and further integrate problems and issues that are less commonly addressed in the context of language, including continuous event spaces, features of sound and timbre, representation of temporality and interactions of multiple parallel feature streams. We discuss these aspects in the light of recent theoretical, cognitive, neuroscientific and modelling research in the domains of music, language and animal song.},
  keywords = {animal vocalization,Chomsky hierarchy,comparative perspective,computational modelling,language,music},
  file = {/Users/ron/Zotero/storage/HV94IRLD/Rohrmeier et al. - 2015 - Principles of structure building in music, languag.pdf}
}

@book{rosenDiscreteMathematicsIts2012,
  title = {Discrete Mathematics and Its Applications},
  author = {Rosen, Kenneth H.},
  year = {2012},
  edition = {7th ed},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  isbn = {978-0-07-338309-5},
  langid = {english},
  lccn = {QA39.3 .R67 2012},
  keywords = {Computer science,Mathematics},
  file = {/Users/ron/Zotero/storage/KDAP6SAN/Rosen - 2012 - Discrete mathematics and its applications.pdf}
}

@article{ruleChildHacker2020,
  title = {The {{Child}} as {{Hacker}}},
  author = {Rule, Joshua S. and Tenenbaum, Joshua B. and Piantadosi, Steven T.},
  year = {2020},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {11},
  pages = {900--915},
  issn = {13646613},
  urldate = {2022-05-03},
  langid = {english},
  file = {/Users/ron/Zotero/storage/TVAQQX5F/Rule et al. - 2020 - The Child as Hacker.pdf}
}

@article{sable-meyerLanguageThoughtMental2022,
  title = {A Language of Thought for the Mental Representation of Geometric Shapes},
  author = {{Sabl{\'e}-Meyer}, Mathias and Ellis, Kevin and Tenenbaum, Josh and Dehaene, Stanislas},
  year = {2022},
  month = dec,
  journal = {Cognitive Psychology},
  volume = {139},
  pages = {101527},
  issn = {00100285},
  urldate = {2022-12-07},
  abstract = {In various cultures and at all spatial scales, humans produce a rich complexity of geometric shapes such as lines, circles or spirals. Here, we propose that humans possess a language of thought for geometric shapes that can produce line drawings as recursive combinations of a minimal set of geometric primitives. We present a programming language, similar to Logo, that combines discrete numbers and continuous integration to form higher-level structures based on repetition, concatenation and embedding, and we show that the simplest programs in this lan\- guage generate the fundamental geometric shapes observed in human cultures. On the perceptual side, we propose that shape perception in humans involves searching for the shortest program that correctly draws the image (program induction). A consequence of this framework is that the mental difficulty of remembering a shape should depend on its minimum description length (MDL) in the proposed language. In two experiments, we show that encoding and processing of geometric shapes is well predicted by MDL. Furthermore, our hypotheses predict additive laws for the psychological complexity of repeated, concatenated or embedded shapes, which we confirm experimentally.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CL5R2SDF/Sablé-Meyer et al. - 2022 - A language of thought for the mental representatio.pdf}
}

@inproceedings{salakhutdinovOneShotLearningHierarchical2012,
  title = {One-{{Shot Learning}} with a {{Hierarchical Nonparametric Bayesian Model}}},
  booktitle = {Proceedings of {{ICML Workshop}} on {{Unsupervised}} and {{Transfer Learning}}},
  author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua and Torralba, Antonio},
  year = {2012},
  month = jun,
  pages = {195--206},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2024-01-26},
  abstract = {We develop a hierarchical Bayesian model that learns categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances. The model discovers how to group categories into meaningful super-categories that express different priors for new classes. Given a single example of a novel category, we can efficiently infer which super-category the novel category belongs to, and thereby estimate not only the new categories mean but also an appropriate similarity metric based on parameters inherited from the super-category. On MNIST and MSR Cambridge image datasets the model learns useful representations of novel categories based on just a single training example, and performs significantly better than simpler hierarchical Bayesian approaches. It can also discover new categories in a completely unsupervised fashion, given just one or a few examples.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/C627KJGR/Salakhutdinov et al. - 2012 - One-Shot Learning with a Hierarchical Nonparametri.pdf}
}

@misc{santoroSymbolicBehaviourArtificial2022,
  title = {Symbolic {{Behaviour}} in {{Artificial Intelligence}}},
  author = {Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
  year = {2022},
  month = jan,
  number = {arXiv:2102.03406},
  eprint = {2102.03406},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/6YDV96L2/Santoro et al. - 2022 - Symbolic Behaviour in Artificial Intelligence.pdf;/Users/ron/Zotero/storage/LMG7CQZY/2102.html}
}

@misc{sapolskyBiologyHumanBehavior,
  title = {Biology and {{Human Behavior}}: {{The Neurological Origins}} of {{Individuality}}, 2nd {{Edition}}: (526622012-001)},
  shorttitle = {Biology and {{Human Behavior}}},
  author = {Sapolsky, Robert},
  publisher = {{American Psychological Association}},
  urldate = {2024-01-18},
  langid = {english},
  file = {/Users/ron/Zotero/storage/L356FEK2/Sapolsky - Biology and Human Behavior The Neurological Origi.pdf}
}

@article{schlegelComparisonVectorSymbolic2022,
  title = {A Comparison of Vector Symbolic Architectures},
  author = {Schlegel, Kenny and Neubert, Peer and Protzel, Peter},
  year = {2022},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {6},
  pages = {4523--4555},
  issn = {1573-7462},
  urldate = {2023-03-14},
  abstract = {Vector Symbolic Architectures combine a high-dimensional vector space with a set of carefully designed operators in order to perform symbolic computations with large numerical vectors. Major goals are the exploitation of their representational power and ability to deal with fuzziness and ambiguity. Over the past years, several VSA implementations have been proposed. The available implementations differ in the underlying vector space and the particular implementations of the VSA operators. This paper provides an overview of eleven available VSA implementations and discusses their commonalities and differences in the underlying vector space and operators. We create a taxonomy of available binding operations and show an important ramification for non self-inverse binding operations using an example from analogical reasoning. A~main contribution is the experimental comparison of the available implementations in order to evaluate (1) the capacity of bundles, (2) the approximation quality of non-exact unbinding operations, (3) the influence of combining binding and bundling operations on the query answering performance, and (4) the performance on two example applications: visual place- and language-recognition. We expect this comparison and systematization to be relevant for development of VSAs, and to support the selection of an appropriate VSA for a particular task. The implementations are available.},
  langid = {english},
  keywords = {High-dimensional computing,Hyperdimensional computing,Hypervectors,Vector symbolic architectures},
  file = {/Users/ron/Zotero/storage/8GFIAAHZ/Schlegel et al. - 2022 - A comparison of vector symbolic architectures.pdf}
}

@misc{schmidhuberAlgorithmicTheoriesEverything2000,
  title = {Algorithmic {{Theories}} of {{Everything}}},
  author = {Schmidhuber, Juergen},
  year = {2000},
  month = dec,
  number = {arXiv:quant-ph/0011122},
  eprint = {quant-ph/0011122},
  publisher = {{arXiv}},
  urldate = {2023-05-27},
  abstract = {The probability distribution P from which the history of our universe is sampled represents a theory of everything or TOE. We assume P is formally describable. Since most (uncountably many) distributions are not, this imposes a strong inductive bias. We show that P(x) is small for any universe x lacking a short description, and study the spectrum of TOEs spanned by two Ps, one reflecting the most compact constructive descriptions, the other the fastest way of computing everything. The former derives from generalizations of traditional computability, Solomonoff's algorithmic probability, Kolmogorov complexity, and objects more random than Chaitin's Omega, the latter from Levin's universal search and a natural resource-oriented postulate: the cumulative prior probability of all x incomputable within time t by this optimal algorithm should be 1/t. Between both Ps we find a universal cumulatively enumerable measure that dominates traditional enumerable measures; any such CEM must assign low probability to any universe lacking a short enumerating program. We derive P-specific consequences for evolving observers, inductive reasoning, quantum physics, philosophy, and the expected duration of our universe.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Machine Learning,High Energy Physics - Theory,Mathematical Physics,Physics - Computational Physics,Quantum Physics},
  file = {/Users/ron/Zotero/storage/RVVZQZFT/Schmidhuber - 2000 - Algorithmic Theories of Everything.pdf;/Users/ron/Zotero/storage/CESJHG3H/0011122.html}
}

@article{scholkopfCausalRepresentationLearning2021,
  title = {Toward {{Causal Representation Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  year = {2021},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {5},
  pages = {612--634},
  issn = {1558-2256},
  urldate = {2023-10-26},
  abstract = {The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  file = {/Users/ron/Zotero/storage/FEDKDCNE/Schölkopf et al. - 2021 - Toward Causal Representation Learning.pdf}
}

@article{schwartzBehavioralNeuralConstraints2017,
  title = {Behavioral and Neural Constraints on Hierarchical Representations},
  author = {Schwartz, Odelia and Giraldo, Luis Gonzalo Sanchez},
  year = {2017},
  month = mar,
  journal = {Journal of Vision},
  volume = {17},
  number = {3},
  pages = {13},
  issn = {1534-7362},
  urldate = {2024-01-23},
  abstract = {Biederman (1987) proposed that primitive parts (termed ``geons,'' from geometrical eons) of blocks, cylinders, spheres, and wedges, are important for object recognition. He further argued that recognizing objects, similar to recognizing speech from phonemes, relies on a modest number of such geon parts, and the arrangement of these parts. Although there are an infinite number of ways that wholes can be constructed from their lower level parts, the allowable constructions respect particular constraints (see discussion on compositionality in Bienenstock \& Geman, 1995; Bienenstock, Geman, \& Potter, 1997). Statistical constraints arise because of coherence between wholes and parts, a prominent example of which is the geometric arrangement of the parts (Felzenszwalb \& Huttenlocher, 2005; Felzenszwalb, Girshick, McAllester, \& Ramanan, 2010; Felzenszwalb, McAllester, \& Ramanan, 2008; Fergus, Perona, \& Zisserman, 2003; Fischler \& Elschlager, 1973; Sudderth, Torralba, Freeman, \& Willsky, 2005). That generation follows particular rules implies that its recognition inverse will possess certain properties. We describe this as a partnering principle. For the case of part-whole generation, the partnering recognition principle is a form of binding. This determines how to align parts with the roles they play in the putative wholes, and thereby represent them appropriately. This process thus identifies the parts (e.g., wings, a beak, or legs) and their coherence to infer the whole (bird), a process which can proceed hierarchically up to the whole forest scene Also associated with part-whole generation is reuse, since many parts can be made the same, or at least have similar constructs (e.g., two related wings for each bird; many organized feathers on each wing).},
  langid = {english},
  file = {/Users/ron/Zotero/storage/QHNBM48X/Schwartz and Giraldo - 2017 - Behavioral and neural constraints on hierarchical .pdf}
}

@article{segerCategoryLearningBrain2010,
  title = {Category {{Learning}} in the {{Brain}}},
  author = {Seger, Carol A. and Miller, Earl K.},
  year = {2010},
  journal = {Annual review of neuroscience},
  volume = {33},
  pages = {203--219},
  issn = {0147-006X},
  urldate = {2023-05-08},
  abstract = {The ability to group items and events into functional categories is a fundamental characteristic of sophisticated thought. It is subserved by plasticity in many neural systems, including neocortical regions (sensory, prefrontal, parietal, and motor cortex), the medial temporal lobe, the basal ganglia, and midbrain dopaminergic systems. These systems interact during category learning. Corticostriatal loops may mediate recursive, bootstrapping interactions between fast reward-gated plasticity in the basal ganglia and slow reward-shaded plasticity in the cortex. This can provide a balance between acquisition of details of experiences and generalization across them. Interactions between the corticostriatal loops can integrate perceptual, response, and feedback-related aspects of the task and mediate the shift from novice to skilled performance. The basal ganglia and medial temporal lobe interact competitively or cooperatively, depending on the demands of the learning task.},
  pmcid = {PMC3709834},
  pmid = {20572771},
  file = {/Users/ron/Zotero/storage/KNL3WNFL/Seger and Miller - 2010 - Category Learning in the Brain.pdf}
}

@incollection{sep-goedel-incompleteness,
  title = {G{\"o}del's Incompleteness Theorems},
  booktitle = {The {{Stanford}} Encyclopedia of Philosophy},
  author = {Raatikainen, Panu},
  editor = {Zalta, Edward N.},
  year = {2022},
  edition = {Spring 2022},
  publisher = {{Metaphysics Research Lab, Stanford University}}
}

@incollection{sep-language-thought,
  title = {The Language of Thought Hypothesis},
  booktitle = {The {{Stanford}} Encyclopedia of Philosophy},
  author = {Rescorla, Michael},
  editor = {Zalta, Edward N.},
  year = {2019},
  edition = {Summer 2019},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  howpublished = {{$<$}a href="https:[//plato.stanford.edu/archives/sum2019/entries/language-thought/](https:////plato.stanford.edu/archives/sum2019/entries/language-thought/)"{$>$}https:[//plato.stanford.edu/archives/sum2019/entries/language-thought/](https:////plato.stanford.edu/archives/sum2019/entries/language-thought/){$<$}/a{$>$}}
}

@incollection{sep-leibniz-logic-influence,
  title = {Leibniz's Influence on 19th Century Logic},
  booktitle = {The {{Stanford}} Encyclopedia of Philosophy},
  author = {Peckhaus, Volker},
  editor = {Zalta, Edward N.},
  year = {2018},
  edition = {Winter 2018},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  howpublished = {{$<$}a href="[https://plato.stanford.edu/archives/win2018/entries/leibniz-logic-influence/](https://plato.stanford.edu/archives/win2018/entries/leibniz-logic-influence/)"{$>$}[https://plato.stanford.edu/archives/win2018/entries/leibniz-logic-influence/](https://plato.stanford.edu/archives/win2018/entries/leibniz-logic-influence/){$<$}/a{$>$}}
}

@article{sethTheoriesConsciousness2022,
  title = {Theories of Consciousness},
  author = {Seth, Anil K. and Bayne, Tim},
  year = {2022},
  month = may,
  journal = {Nature Reviews Neuroscience},
  issn = {1471-003X, 1471-0048},
  urldate = {2022-05-18},
  abstract = {Recent years have seen a blossoming of theories about the biological and physical basis of consciousness. Good theories guide empirical research, allowing us to interpret data, develop new experimental techniques and expand our capacity to manipulate the phenomenon of interest. Indeed, it is only when couched in terms of a theory that empirical discoveries can ultimately deliver a satisfying understanding of a phenomenon. However, in the case of consciousness, it is unclear how current theories relate to each other, or whether they can be empirically distinguished. To clarify this complicated landscape, we review four prominent theoretical approaches to consciousness: higher-o rder theories, global workspace theories, re-e ntry and predictive processing theories and integrated information theory. We describe the key characteristics of each approach by identifying which aspects of consciousness they propose to explain, what their neurobiological commitments are and what empirical data are adduced in their support. We consider how some prominent empirical debates might distinguish among these theories, and we outline three ways in which theories need to be developed to deliver a mature regimen of theory-t esting in the neuroscience of consciousness. There are good reasons to think that the iterative development, testing and comparison of theories of consciousness will lead to a deeper understanding of this most profound of mysteries.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/R5AJQRL3/Seth and Bayne - 2022 - Theories of consciousness.pdf}
}

@article{shannonMathematicalTheoryCommunication,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  langid = {english},
  file = {/Users/ron/Zotero/storage/2W8QU8R7/Shannon - A Mathematical Theory of Communication.pdf}
}

@article{shindoAlphaILPThinking2023,
  title = {\$\${\textbackslash}alpha\$\${{ILP}}: Thinking Visual Scenes as Differentiable Logic Programs},
  shorttitle = {\$\${\textbackslash}alpha\$\${{ILP}}},
  author = {Shindo, Hikaru and Pfanschilling, Viktor and Dhami, Devendra Singh and Kersting, Kristian},
  year = {2023},
  month = may,
  journal = {Machine Learning},
  volume = {112},
  number = {5},
  pages = {1465--1497},
  issn = {1573-0565},
  urldate = {2023-05-08},
  abstract = {Deep neural learning has shown remarkable performance at learning representations for visual object categorization. However, deep neural networks such as CNNs do not explicitly encode objects and relations among them. This limits their success on tasks that require a deep logical understanding of visual scenes, such as Kandinsky patterns and Bongard problems. To overcome these limitations, we introduce \$\${\textbackslash}alpha \{{\textbackslash}textit\{ILP\}\}\$\$, a novel differentiable inductive logic programming framework that learns to represent scenes as logic programs{\textemdash}intuitively, logical atoms correspond to objects, attributes, and relations, and clauses encode high-level scene information. \$\${\textbackslash}alpha\$\$ILP has an end-to-end reasoning architecture from visual inputs. Using it, \$\${\textbackslash}alpha\$\$ILP performs differentiable inductive logic programming on complex visual scenes, i.e., the logical rules are learned by gradient descent. Our extensive experiments on Kandinsky patterns and CLEVR-Hans benchmarks demonstrate the accuracy and efficiency of \$\${\textbackslash}alpha \{{\textbackslash}textit\{ILP\}\}\$\$in learning complex visual-logical concepts.},
  langid = {english},
  keywords = {Differentiable reasoning,Inductive logic programming,Neuro-symbolic AI,Object-centric learning},
  file = {/Users/ron/Zotero/storage/UECDBWYK/Shindo et al. - 2023 - $$alpha$$ILP thinking visual scenes as different.pdf}
}

@article{shmueliExplainPredict2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  year = {2010},
  month = aug,
  journal = {Statistical Science},
  volume = {25},
  number = {3},
  issn = {0883-4237},
  urldate = {2024-01-27},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/HGSAZIJ4/Shmueli - 2010 - To Explain or to Predict.pdf}
}

@article{silverCognitiveNeuroscienceFunctional2018,
  title = {Cognitive {{Neuroscience}}: {{Functional Specialization}} in {{Human Cerebellum}}},
  shorttitle = {Cognitive {{Neuroscience}}},
  author = {Silver, Michael A.},
  year = {2018},
  month = nov,
  journal = {Current biology : CB},
  volume = {28},
  number = {21},
  pages = {R1256-R1258},
  issn = {0960-9822},
  urldate = {2024-01-23},
  abstract = {A new brain imaging study reveals that the human cerebellum contains a region that represents visual space that is dissociable from a region displaying visual memory-related activity, with both regions exhibiting precise functional coupling with corresponding cerebral cortical areas.},
  pmcid = {PMC6223660},
  pmid = {30399350},
  file = {/Users/ron/Zotero/storage/CAKZJ87X/Silver - 2018 - Cognitive Neuroscience Functional Specialization .pdf}
}

@article{silverRewardEnough2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  year = {2021},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {00043702},
  urldate = {2024-01-27},
  langid = {english},
  file = {/Users/ron/Zotero/storage/KX9C7A8R/Silver et al. - 2021 - Reward is enough.pdf}
}

@misc{simmons-edlerProgramSynthesisReinforcement2018,
  title = {Program {{Synthesis Through Reinforcement Learning Guided Tree Search}}},
  author = {{Simmons-Edler}, Riley and Miltner, Anders and Seung, Sebastian},
  year = {2018},
  month = jun,
  number = {arXiv:1806.02932},
  eprint = {1806.02932},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {Program Synthesis is the task of generating a program from a provided specification. Traditionally, this has been treated as a search problem by the programming languages (PL) community and more recently as a supervised learning problem by the machine learning community. Here, we propose a third approach, representing the task of synthesizing a given program as a Markov decision process solvable via reinforcement learning(RL). From observations about the states of partial programs, we attempt to find a program that is optimal over a provided reward metric on pairs of programs and states. We instantiate this approach on a subset of the RISC-V assembly language operating on floating point numbers, and as an optimization inspired by search-based techniques from the PL community, we combine RL with a priority search tree. We evaluate this instantiation and demonstrate the effectiveness of our combined method compared to a variety of baselines, including a pure RL ablation and a state of the art Markov chain Monte Carlo search method on this task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Programming Languages},
  file = {/Users/ron/Zotero/storage/WEL8Z45U/Simmons-Edler et al. - 2018 - Program Synthesis Through Reinforcement Learning G.pdf;/Users/ron/Zotero/storage/W6YSBV4A/1806.html}
}

@article{sinapayenReactiveProactiveInductive2020,
  title = {Reactive, {{Proactive}}, and {{Inductive Agents}}: {{An Evolutionary Path}} for {{Biological}} and {{Artificial Spiking Networks}}},
  shorttitle = {Reactive, {{Proactive}}, and {{Inductive Agents}}},
  author = {Sinapayen, Lana and Masumori, Atsushi and Ikegami, Takashi},
  year = {2020},
  journal = {Frontiers in Computational Neuroscience},
  volume = {13},
  issn = {1662-5188},
  urldate = {2024-01-28},
  abstract = {Complex environments provide structured yet variable sensory inputs. To best exploit information from these environments, organisms must evolve the ability to anticipate consequences of new stimuli, and act on these predictions. We propose an evolutionary path for neural networks, leading an organism from reactive behavior to simple proactive behavior and from simple proactive behavior to induction-based behavior. Based on earlier in-vitro and in-silico experiments, we define the conditions necessary in a network with spike-timing dependent plasticity for the organism to go from reactive to proactive behavior. Our results support the existence of specific evolutionary steps and four conditions necessary for embodied neural networks to evolve predictive and inductive abilities from an initial reactive strategy.},
  file = {/Users/ron/Zotero/storage/HF4AG6S6/Sinapayen et al. - 2020 - Reactive, Proactive, and Inductive Agents An Evol.pdf}
}

@article{smithHallucinationConfabulationNeuroanatomy2023,
  title = {Hallucination or {{Confabulation}}? {{Neuroanatomy}} as Metaphor in {{Large Language Models}}},
  shorttitle = {Hallucination or {{Confabulation}}?},
  author = {Smith, Andrew L. and Greaves, Felix and Panch, Trishan},
  year = {2023},
  month = nov,
  journal = {PLOS Digital Health},
  volume = {2},
  number = {11},
  pages = {e0000388},
  publisher = {{Public Library of Science}},
  issn = {2767-3170},
  urldate = {2023-12-26},
  langid = {english},
  keywords = {Artificial intelligence,Hallucinations,Language,Left hemisphere,Right hemisphere,Schizophrenia,Sensory perception,Traumatic brain injury},
  file = {/Users/ron/Zotero/storage/JRRI4N5G/Smith et al. - 2023 - Hallucination or Confabulation Neuroanatomy as me.pdf}
}

@book{sommervilleSoftwareEngineering2011,
  title = {Software Engineering},
  author = {Sommerville, Ian},
  year = {2011},
  edition = {9th ed},
  publisher = {{Pearson}},
  address = {{Boston}},
  isbn = {978-0-13-703515-1 978-0-13-705346-9},
  langid = {english},
  lccn = {QA76.758 .S657 2011},
  keywords = {Software engineering},
  annotation = {OCLC: ocn462909026},
  file = {/Users/ron/Zotero/storage/RQX46PQR/Sommerville - 2011 - Software engineering.pdf}
}

@inproceedings{stewart2012spaun,
  title = {Spaun: {{A}} Perception-Cognition-Action Model Using Spiking Neurons},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Stewart, Terrence and Choo, Feng-Xuan and Eliasmith, Chris},
  year = {2012},
  volume = {34}
}

@article{stewart2012technical,
  title = {A Technical Overview of the Neural Engineering Framework},
  author = {Stewart, Terrence C},
  year = {2012},
  journal = {University of Waterloo}
}

@article{stockPlantScienceAge2024,
  title = {Plant Science in the Age of Simulation Intelligence},
  author = {Stock, Michiel and Pieters, Olivier and De Swaef, Tom and {wyffels}, Francis},
  year = {2024},
  journal = {Frontiers in Plant Science},
  volume = {14},
  issn = {1664-462X},
  urldate = {2024-01-23},
  abstract = {Historically, plant and crop sciences have been quantitative fields that intensively use measurements and modeling. Traditionally, researchers choose between two dominant modeling approaches: mechanistic plant growth models or data-driven, statistical methodologies. At the intersection of both paradigms, a novel approach referred to as ``simulation intelligence'', has emerged as a powerful tool for comprehending and controlling complex systems, including plants and crops. This work explores the transformative potential for the plant science community of the nine simulation intelligence motifs, from understanding molecular plant processes to optimizing greenhouse control. Many of these concepts, such as surrogate models and agent-based modeling, have gained prominence in plant and crop sciences. In contrast, some motifs, such as open-ended optimization or program synthesis, still need to be explored further. The motifs of simulation intelligence can potentially revolutionize breeding and precision farming towards more sustainable food production.},
  file = {/Users/ron/Zotero/storage/BRSCRYGY/Stock et al. - 2024 - Plant science in the age of simulation intelligenc.pdf}
}

@article{sunStochasticPredictionMultiAgent2019,
  title = {Stochastic {{Prediction}} of {{Multi-Agent Interactions}} from {{Partial Observations}}},
  author = {Sun, Chen and Karlsson, Per and Wu, Jiajun and Tenenbaum, Joshua B. and Murphy, Kevin},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.09641 [cs, stat]},
  eprint = {1902.09641},
  primaryclass = {cs, stat},
  urldate = {2022-05-03},
  abstract = {We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/9Y8U84ZE/Sun et al. - 2019 - Stochastic Prediction of Multi-Agent Interactions .pdf;/Users/ron/Zotero/storage/DMNZWJGF/1902.html}
}

@book{suttonReinforcementLearningIntroduction2020,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew},
  year = {2020},
  series = {Adaptive Computation and Machine Learning},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  file = {/Users/ron/Zotero/storage/LY54EJHC/Sutton and Barto - 2020 - Reinforcement learning an introduction.pdf}
}

@article{taylorGlobalLandscapeCognition2015,
  title = {The Global Landscape of Cognition: Hierarchical Aggregation as an Organizational Principle of Human Cortical Networks and Functions},
  shorttitle = {The Global Landscape of Cognition},
  author = {Taylor, P. and Hobbs, J. N. and Burroni, J. and Siegelmann, H. T.},
  year = {2015},
  month = dec,
  journal = {Scientific Reports},
  volume = {5},
  number = {1},
  pages = {18112},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  urldate = {2023-10-02},
  abstract = {Though widely hypothesized, limited evidence exists that human brain functions organize in global gradients of abstraction starting from sensory cortical inputs. Hierarchical representation is accepted in computational networks and tentatively in visual neuroscience, yet no direct holistic demonstrations exist in vivo. Our methods developed network models enriched with tiered directionality, by including input locations, a critical feature for localizing representation in networks generally. Grouped primary sensory cortices defined network inputs, displaying global connectivity to fused inputs. Depth-oriented networks guided analyses of fMRI databases ({\textasciitilde}17,000 experiments;{\textasciitilde}1/4 of fMRI literature). Formally, we tested whether network depth predicted localization of abstract versus concrete behaviors over the whole set of studied brain regions. For our results, new cortical graph metrics, termed network-depth, ranked all databased cognitive function activations by network-depth. Thus, we objectively sorted stratified landscapes of cognition, starting from grouped sensory inputs in parallel, progressing deeper into cortex. This exposed escalating amalgamation of function or abstraction with increasing network-depth, globally. Nearly 500 new participants confirmed our results. In conclusion, data-driven analyses defined a hierarchically ordered connectome, revealing a related continuum of cognitive function. Progressive functional abstraction over network depth may be a fundamental feature of brains and is observed in artificial networks.},
  copyright = {2015 The Author(s)},
  langid = {english},
  keywords = {Consciousness,Network models},
  file = {/Users/ron/Zotero/storage/PTHFRBHL/Taylor et al. - 2015 - The global landscape of cognition hierarchical ag.pdf}
}

@book{teslaFantasticInventionsNikola1993,
  title = {The Fantastic Inventions of {{Nikola Tesla}}},
  author = {Tesla, Nikola and Childress, David Hatcher},
  year = {1993},
  series = {The {{Lost}} Science Series},
  publisher = {{Adventures Unlimited}},
  address = {{Stelle, Ill}},
  isbn = {978-0-932813-19-0},
  langid = {english},
  lccn = {TK140.T4 A3 1993},
  keywords = {Biography,Electrical engineering,Electrical engineers,Inventions,Inventors,{Tesla, Nikola},United States},
  file = {/Users/ron/Zotero/storage/UHT424VA/Tesla and Childress - 1993 - The fantastic inventions of Nikola Tesla.pdf}
}

@article{thillImportanceRichEmbodiment2014,
  title = {On the Importance of a Rich Embodiment in the Grounding of Concepts: Perspectives from Embodied Cognitive Science and Computational Linguistics},
  shorttitle = {On the Importance of a Rich Embodiment in the Grounding of Concepts},
  author = {Thill, Serge and Pad{\'o}, Sebastian and Ziemke, Tom},
  year = {2014},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {6},
  number = {3},
  pages = {545--558},
  issn = {1756-8765},
  abstract = {The recent trend in cognitive robotics experiments on language learning, symbol grounding, and related issues necessarily entails a reduction of sensorimotor aspects from those provided by a human body to those that can be realized in machines, limiting robotic models of symbol grounding in this respect. Here, we argue that there is a need for modeling work in this domain to explicitly take into account the richer human embodiment even for concrete concepts that prima facie relate merely to simple actions, and illustrate this using distributional methods from computational linguistics which allow us to investigate grounding of concepts based on their actual usage. We also argue that these techniques have applications in theories and models of grounding, particularly in machine implementations thereof. Similarly, considering the grounding of concepts in human terms may be of benefit to future work in computational linguistics, in particular in going beyond "grounding" concepts in the textual modality alone. Overall, we highlight the overall potential for a mutually beneficial relationship between the two fields.},
  langid = {english},
  pmid = {24948385},
  keywords = {Cognition,Cognitive Science,Concept Formation,Concept grounding,Distributional semantics,Embodiment,Humans,Language,Linguistics,Machine language understanding,{Models, Theoretical},Robotics},
  file = {/Users/ron/Zotero/storage/S4ZH2WF8/Thill et al. - 2014 - On the importance of a rich embodiment in the grou.pdf}
}

@article{thillWhatCountsGrounded2016,
  title = {What's on the {{Inside Counts}}: {{A Grounded Account}} of {{Concept Acquisition}} and {{Development}}},
  shorttitle = {What's on the {{Inside Counts}}},
  author = {Thill, Serge and Twomey, Katherine E.},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  urldate = {2022-06-27},
  abstract = {Understanding the factors which affect the age of acquisition (AoA) of words and concepts is fundamental to understanding cognitive development more broadly. Traditionally, studies of AoA have taken two approaches, either exploring the effect of linguistic variables such as input frequency (e.g., Naigles and Hoff-Ginsberg, 1998) or the semantics of the underlying concept, such as concreteness or imageability (e.g., Bird et al., 2001). Embodied theories of cognition, meanwhile, assume that concepts, even relatively abstract ones, can be grounded in the embodied experience. While the focus of such discussions has been mainly on grounding in external modalities, more recently some have argued for the importance of interoceptive features, or grounding in complex modalities such as social interaction. In this paper, we argue for the integration and extension of these two strands of research. We demonstrate that the psycholinguistic factors traditionally considered to determine AoA are far from sufficient to account for the variability observed in AoA data. Given this gap, we propose groundability as a new conceptual tool that can measure the degree to which concepts are grounded both in external and, critically, internal modalities. We then present a mechanistic theory of conceptual representation that can account for groundability in addition to the existing variables argued to influence concept acquisition in both the developmental and embodied cognition literatures, and discuss its implications for future work in concept and cognitive development.},
  file = {/Users/ron/Zotero/storage/4JCAG82P/Thill and Twomey - 2016 - What's on the Inside Counts A Grounded Account of.pdf}
}

@article{turingComputingMachineryIntelligence1950,
  title = {Computing {{Machinery}} and {{Intelligence}}},
  author = {Turing, A. M.},
  year = {1950},
  journal = {Mind, New Series},
  volume = {59},
  number = {236},
  eprint = {2251299},
  eprinttype = {jstor},
  pages = {433--460},
  file = {/Users/ron/Zotero/storage/6N7ESP6B/Turing - 1950 - Computing Machinery and Intelligence.pdf}
}

@article{ullmanMindGamesGame2017,
  title = {Mind {{Games}}: {{Game Engines}} as an {{Architecture}} for {{Intuitive Physics}}},
  shorttitle = {Mind {{Games}}},
  author = {Ullman, Tomer D. and Spelke, Elizabeth and Battaglia, Peter and Tenenbaum, Joshua B.},
  year = {2017},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {21},
  number = {9},
  pages = {649--665},
  issn = {13646613},
  urldate = {2023-12-12},
  langid = {english},
  file = {/Users/ron/Zotero/storage/J3GZVJY2/Ullman et al. - 2017 - Mind Games Game Engines as an Architecture for In.pdf}
}

@article{ullmanTheoryLearningStochastic2012,
  title = {Theory Learning as Stochastic Search in the Language of Thought},
  author = {Ullman, Tomer D. and Goodman, Noah D. and Tenenbaum, Joshua B.},
  year = {2012},
  month = oct,
  journal = {Cognitive Development},
  series = {The {{Potential Contribution}} of {{Computational Modeling}} to the {{Study}} of {{Cognitive Development}}: {{When}}, and for {{What Topics}}?},
  volume = {27},
  number = {4},
  pages = {455--480},
  issn = {0885-2014},
  urldate = {2023-05-08},
  abstract = {We present an algorithmic model for the development of children's intuitive theories within a hierarchical Bayesian framework, where theories are described as sets of logical laws generated by a probabilistic context-free grammar. We contrast our approach with connectionist and other emergentist approaches to modeling cognitive development. While their subsymbolic representations provide a smooth error surface that supports efficient gradient-based learning, our symbolic representations are better suited to capturing children's intuitive theories but give rise to a harder learning problem, which can only be solved by exploratory search. Our algorithm attempts to discover the theory that best explains a set of observed data by performing stochastic search at two levels of abstraction: an outer loop in the space of theories and an inner loop in the space of explanations or models generated by each theory given a particular dataset. We show that this stochastic search is capable of learning appropriate theories in several everyday domains and discuss its dynamics in the context of empirical studies of children's learning.},
  langid = {english},
  keywords = {Algorithms,Bayesian models,Conceptual change,Intuitive theories,Language of thought},
  file = {/Users/ron/Zotero/storage/XVSR5YLQ/Ullman et al. - 2012 - Theory learning as stochastic search in the langua.pdf;/Users/ron/Zotero/storage/TG7Z99T3/S0885201412000445.html}
}

@article{vangervenComputationalFoundationsNatural2017,
  title = {Computational {{Foundations}} of {{Natural Intelligence}}},
  author = {{van Gerven}, Marcel},
  year = {2017},
  month = dec,
  journal = {Frontiers in Computational Neuroscience},
  volume = {11},
  pages = {112},
  issn = {1662-5188},
  urldate = {2022-04-13},
  abstract = {New developments in AI and neuroscience are revitalizing the quest to understanding natural intelligence, offering insight about how to equip machines with human-like capabilities. This paper reviews some of the computational principles relevant for understanding natural intelligence and, ultimately, achieving strong AI. After reviewing basic principles, a variety of computational modeling approaches is discussed. Subsequently, I concentrate on the use of artificial neural networks as a framework for modeling cognitive processes. This paper ends by outlining some of the challenges that remain to fulfill the promise of machines that show human-like intelligence.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/2D2EH52L/van Gerven - 2017 - Computational Foundations of Natural Intelligence.pdf}
}

@article{vanrooijTractableCognitionThesis2008,
  title = {The {{Tractable Cognition Thesis}}},
  author = {Van Rooij, Iris},
  year = {2008},
  journal = {Cognitive Science},
  volume = {32},
  number = {6},
  pages = {939--984},
  issn = {1551-6709},
  urldate = {2023-12-22},
  abstract = {The recognition that human minds/brains are finite systems with limited resources for computation has led some researchers to advance the Tractable Cognition thesis: Human cognitive capacities are constrained by computational tractability. This thesis, if true, serves cognitive psychology by constraining the space of computational-level theories of cognition. To utilize this constraint, a precise and workable definition of ``computational tractability'' is needed. Following computer science tradition, many cognitive scientists and psychologists define computational tractability as polynomial-time computability, leading to the P-Cognition thesis. This article explains how and why the P-Cognition thesis may be overly restrictive, risking the exclusion of veridical computational-level theories from scientific investigation. An argument is made to replace the P-Cognition thesis by the FPT-Cognition thesis as an alternative formalization of the Tractable Cognition thesis (here, FPT stands for fixed-parameter tractable). Possible objections to the Tractable Cognition thesis, and its proposed formalization, are discussed, and existing misconceptions are clarified.},
  langid = {english},
  keywords = {Cognitive modeling,Complexity theory,Computational-level theory,Constraint satisfaction,Intractability,NP-hard,Philosophy of computation,Philosophy of mind},
  file = {/Users/ron/Zotero/storage/KC7IHKGL/Van Rooij - 2008 - The Tractable Cognition Thesis.pdf;/Users/ron/Zotero/storage/UDA496KV/03640210801897856.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-10-19},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/Users/ron/Zotero/storage/BN69XYB3/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@misc{velickovicCLRSAlgorithmicReasoning2022,
  title = {The {{CLRS Algorithmic Reasoning Benchmark}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Badia, Adri{\`a} Puigdom{\`e}nech and Budden, David and Pascanu, Razvan and Banino, Andrea and Dashevskiy, Misha and Hadsell, Raia and Blundell, Charles},
  year = {2022},
  month = jun,
  number = {arXiv:2205.15659},
  eprint = {2205.15659},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-12-14},
  abstract = {Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. Several important works have investigated whether neural networks can effectively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorithmic data to evaluate specific hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards unified evaluation, we propose the CLRS Algorithmic Reasoning Benchmark, covering classical algorithms from the Introduction to Algorithms textbook. Our benchmark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/FA8F8CC8/Veličković et al. - 2022 - The CLRS Algorithmic Reasoning Benchmark.pdf;/Users/ron/Zotero/storage/X9YCWTLB/2205.html}
}

@misc{vemgalEmpiricalStudyEffectiveness2023,
  title = {An {{Empirical Study}} of the {{Effectiveness}} of {{Using}} a {{Replay Buffer}} on {{Mode Discovery}} in {{GFlowNets}}},
  author = {Vemgal, Nikhil and Lau, Elaine and Precup, Doina},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07674},
  eprint = {2307.07674},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-25},
  abstract = {Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, \$R(x)\$. GFlowNets are a special class of algorithms designed to generate diverse candidates, \$x\$, from a discrete set, by learning a policy that approximates the proportional sampling of \$R(x)\$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrate significant improvements in mode discovery when training with a replay buffer, compared to training only with trajectories generated on-policy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/9FIKGZBA/Vemgal et al. - 2023 - An Empirical Study of the Effectiveness of Using a.pdf;/Users/ron/Zotero/storage/UTHCQIKT/2307.html}
}

@book{vernonArtificialCognitiveSystems2014,
  title = {Artificial Cognitive Systems: A Primer},
  shorttitle = {Artificial Cognitive Systems},
  author = {Vernon, David},
  year = {2014},
  publisher = {{The MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-02838-7},
  langid = {english},
  lccn = {BF311 .V447 2014},
  keywords = {Artificial intelligence,Brain,Cognition,Development psychology},
  file = {/Users/ron/Zotero/storage/8S5TRFF5/Vernon - 2014 - Artificial cognitive systems a primer.pdf}
}

@article{vernonEmbodiedCognitionCircular2015,
  title = {Embodied Cognition and Circular Causality: On the Role of Constitutive Autonomy in the Reciprocal Coupling of Perception and Action},
  shorttitle = {Embodied Cognition and Circular Causality},
  author = {Vernon, David and Lowe, Robert and Thill, Serge and Ziemke, Tom},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  urldate = {2023-03-14},
  abstract = {The reciprocal coupling of perception and action in cognitive agents has been firmly established: perceptions guide action but so too do actions influence what is perceived. While much has been said on the implications of this for the agent's external behavior, less attention has been paid to what it means for the internal bodily mechanisms which underpin cognitive behavior. In this article, we wish to redress this by reasserting that the relationship between cognition, perception, and action involves a constitutive element as well as a behavioral element, emphasizing that the reciprocal link between perception and action in cognition merits a renewed focus on the system dynamics inherent in constitutive biological autonomy. Our argument centers on the idea that cognition, perception, and action are all dependent on processes focussed primarily on the maintenance of the agent's autonomy. These processes have an inherently circular nature{\textemdash}self-organizing, self-producing, and self-maintaining{\textemdash}and our goal is to explore these processes and suggest how they can explain the reciprocity of perception and action. Specifically, we argue that the reciprocal coupling is founded primarily on their endogenous roles in the constitutive autonomy of the agent and an associated circular causality of global and local processes of self-regulation, rather than being a mutual sensory-motor contingency that derives from exogenous behavior. Furthermore, the coupling occurs first and foremost via the internal milieu realized by the agent's organismic embodiment. Finally, we consider how homeostasis and the related concept of allostasis contribute to this circular self-regulation.},
  file = {/Users/ron/Zotero/storage/IQJ8PCHE/Vernon et al. - 2015 - Embodied cognition and circular causality on the .pdf}
}

@article{vitaliNetworkGlobalCorporate2011,
  title = {The Network of Global Corporate Control},
  author = {Vitali, Stefania and Glattfelder, James B. and Battiston, Stefano},
  year = {2011},
  month = oct,
  journal = {PLoS ONE},
  volume = {6},
  number = {10},
  eprint = {1107.5728},
  primaryclass = {physics, q-fin},
  pages = {e25995},
  issn = {1932-6203},
  urldate = {2024-01-18},
  abstract = {The structure of the control network of transnational corporations affects global market competition and financial stability. So far, only small national samples were studied and there was no appropriate methodology to assess control globally. We present the first investigation of the architecture of the international ownership network, along with the computation of the control held by each global player. We find that transnational corporations form a giant bow-tie structure and that a large portion of control flows to a small tightly-knit core of financial institutions. This core can be seen as an economic ``super-entity'' that raises new important issues both for researchers and policy makers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society,Quantitative Finance - General Finance},
  file = {/Users/ron/Zotero/storage/DA9FJSYJ/Vitali et al. - 2011 - The network of global corporate control.pdf}
}

@incollection{walshDreamingNarration2014,
  title = {Dreaming and {{Narration}}},
  booktitle = {Handbook of {{Narratology}}},
  author = {Walsh, Richard},
  editor = {H{\"u}hn, Peter and Meister, Jan Christoph and Pier, John and Schmid, Wolf},
  year = {2014},
  month = aug,
  pages = {138--148},
  publisher = {{DE GRUYTER}},
  urldate = {2023-05-28},
  isbn = {978-3-11-031634-6},
  langid = {english},
  file = {/Users/ron/Zotero/storage/ESFTEHWJ/Walsh - 2014 - Dreaming and Narration.pdf}
}

@article{wang2017dynamic,
  title = {Dynamic Neural Program Embedding for Program Repair},
  author = {Wang, Ke and Singh, Rishabh and Su, Zhendong},
  year = {2017},
  journal = {arXiv preprint arXiv:1711.07163},
  eprint = {1711.07163},
  archiveprefix = {arxiv}
}

@misc{wangLearningProgramRepresentations2023,
  title = {Learning {{Program Representations}} with a {{Tree-Structured Transformer}}},
  author = {Wang, Wenhan and Zhang, Kechi and Li, Ge and Liu, Shangqing and Li, Anran and Jin, Zhi and Liu, Yang},
  year = {2023},
  month = jan,
  number = {arXiv:2208.08643},
  eprint = {2208.08643},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-23},
  abstract = {Learning vector representations for programs is a critical step in applying deep learning techniques for program understanding tasks. Various neural network models are proposed to learn from tree-structured program representations, e.g., abstract syntax tree (AST) and concrete syntax tree (CST). However, most neural architectures either fail to capture longrange dependencies which are ubiquitous in programs, or cannot learn effective representations for syntax tree nodes, making them incapable of performing the node-level prediction tasks, e.g., bug localization. In this paper, we propose Tree-Transformer, a novel recursive tree-structured neural network to learn the vector representations for source codes. We propose a multihead attention mechanism to model the dependency between siblings and parent-children node pairs. Moreover, we propose a bi-directional propagation strategy to allow node information passing in two directions, bottom-up and top-down along trees. In this way, Tree-Transformer can learn the information of the node features as well as the global contextual information. The extensive experimental results show that our Tree-Transformer significantly outperforms the existing tree-based and graph-based program representation learning approaches in both the tree-level and node-level prediction tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/ron/Zotero/storage/SV3IEKJT/Wang et al. - 2023 - Learning Program Representations with a Tree-Struc.pdf}
}

@article{wangRepresentationSpatialSequences2019,
  title = {Representation of Spatial Sequences Using Nested Rules in Human Prefrontal Cortex},
  author = {Wang, Liping and Amalric, Marie and Fang, Wen and Jiang, Xinjian and Pallier, Christophe and Figueira, Santiago and Sigman, Mariano and Dehaene, Stanislas},
  year = {2019},
  month = feb,
  journal = {NeuroImage},
  volume = {186},
  pages = {245--255},
  issn = {1053-8119},
  urldate = {2023-05-08},
  abstract = {Memory for spatial sequences does not depend solely on the number of locations to be stored, but also on the presence of spatial regularities. Here, we show that the human brain quickly stores spatial sequences by detecting geometrical regularities at multiple time scales and encoding them in a format akin to a programming language. We measured gaze-anticipation behavior while spatial sequences of variable regularity were repeated. Participants' behavior suggested that they quickly discovered the most compact description of each sequence in a language comprising nested rules, and used these rules to compress the sequence in memory and predict the next items. Activity in dorsal inferior prefrontal cortex correlated with the amount of compression, while right dorsolateral prefrontal cortex encoded the presence of embedded structures. Sequence learning was accompanied by a progressive differentiation of multi-voxel activity patterns in these regions. We propose that humans are endowed with a simple ``language of geometry'' which recruits a dorsal prefrontal circuit for geometrical rules, distinct from but close to areas involved in natural language processing.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/CNCSVSSY/Wang et al. - 2019 - Representation of spatial sequences using nested r.pdf}
}

@inproceedings{wangTreeTransformerIntegrating2019,
  title = {Tree {{Transformer}}: {{Integrating Tree Structures}} into {{Self-Attention}}},
  shorttitle = {Tree {{Transformer}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Wang, Yaushian and Lee, Hung-Yi and Chen, Yun-Nung},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  year = {2019},
  month = nov,
  pages = {1061--1070},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  urldate = {2023-12-21},
  abstract = {Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed ``Constituent Attention'' module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.},
  file = {/Users/ron/Zotero/storage/SUQY25AG/Wang et al. - 2019 - Tree Transformer Integrating Tree Structures into.pdf}
}

@misc{wangWorldDreamerGeneralWorld2024,
  title = {{{WorldDreamer}}: {{Towards General World Models}} for {{Video Generation}} via {{Predicting Masked Tokens}}},
  shorttitle = {{{WorldDreamer}}},
  author = {Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Wang, Boyuan and Chen, Xinze and Lu, Jiwen},
  year = {2024},
  month = jan,
  number = {arXiv:2401.09985},
  eprint = {2401.09985},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-23},
  abstract = {World models play a crucial role in understanding and predicting the dynamics of the world, which is essential for video generation. However, existing world models are confined to specific scenarios such as gaming or driving, limiting their ability to capture the complexity of general world dynamic environments. Therefore, we introduce WorldDreamer, a pioneering world model to foster a comprehensive comprehension of general world physics and motions, which significantly enhances the capabilities of video generation. Drawing inspiration from the success of large language models, WorldDreamer frames world modeling as an unsupervised visual sequence modeling challenge. This is achieved by mapping visual inputs to discrete tokens and predicting the masked ones. During this process, we incorporate multi-modal prompts to facilitate interaction within the world model. Our experiments show that WorldDreamer excels in generating videos across different scenarios, including natural scenes and driving environments. WorldDreamer showcases versatility in executing tasks such as text-to-video conversion, image-tovideo synthesis, and video editing. These results underscore WorldDreamer's effectiveness in capturing dynamic elements within diverse general world environments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ron/Zotero/storage/TPLTN7B6/Wang et al. - 2024 - WorldDreamer Towards General World Models for Vid.pdf;/Users/ron/Zotero/storage/BPYVHMPH/2401.html}
}

@article{whitneyUnderstandingVisualConcepts2016,
  title = {Understanding {{Visual Concepts}} with {{Continuation Learning}}},
  author = {Whitney, William F. and Chang, Michael and Kulkarni, Tejas and Tenenbaum, Joshua B.},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.06822 [cs]},
  eprint = {1602.06822},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {We introduce a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/VT5H7LK3/Whitney et al. - 2016 - Understanding Visual Concepts with Continuation Le.pdf;/Users/ron/Zotero/storage/4JFZYCT3/1602.html}
}

@article{windridgeRepresentationalFluidityEmbodied2018,
  title = {Representational Fluidity in Embodied (Artificial) Cognition},
  author = {Windridge, David and Thill, Serge},
  year = {2018},
  month = oct,
  journal = {Biosystems},
  volume = {172},
  pages = {9--17},
  issn = {0303-2647},
  urldate = {2022-06-27},
  abstract = {Theories of embodied cognition agree that the body plays some role in human cognition, but disagree on the precise nature of this role. While it is (together with the environment) fundamentally engrained in the so-called 4E (or multi-E) cognition stance, there also exists interpretations wherein the body is merely an input/output interface for cognitive processes that are entirely computational. In the present paper, we show that even if one takes such a strong computationalist position, the role of the body must be more than an interface to the world. To achieve human cognition, the computational mechanisms of a cognitive agent must be capable not only of appropriate reasoning over a given set of symbolic representations; they must in addition be capable of updating the representational framework itself (leading to the titular representational fluidity). We demonstrate this by considering the necessary properties that an artificial agent with these abilities need to possess. The core of the argument is that these updates must be falsifiable in the Popperian sense while simultaneously directing representational shifts in a direction that benefits the agent. We show that this is achieved by the progressive, bottom-up symbolic abstraction of low-level sensorimotor connections followed by top-down instantiation of testable perception-action hypotheses. We then discuss the fundamental limits of this representational updating capacity, concluding that only fully embodied learners exhibiting such a priori perception-action linkages are able to sufficiently ground spontaneously-generated symbolic representations and exhibit the full range of human cognitive capabilities. The present paper therefore has consequences both for the theoretical understanding of human cognition, and for the design of autonomous artificial agents.},
  langid = {english},
  keywords = {Computationalism,Embodied cognition,Representational frameworks,Representational updating},
  file = {/Users/ron/Zotero/storage/J64CY8ND/Windridge and Thill - 2018 - Representational fluidity in embodied (artificial).pdf;/Users/ron/Zotero/storage/R2M6VGV3/S0303264718302028.html}
}

@article{windridgeUtilityDreamingGeneral2021,
  title = {On the Utility of Dreaming: {{A}} General Model for How Learning in Artificial Agents Can Benefit from Data Hallucination},
  shorttitle = {On the Utility of Dreaming},
  author = {Windridge, David and Svensson, Henrik and Thill, Serge},
  year = {2021},
  month = jun,
  journal = {Adaptive Behavior},
  volume = {29},
  number = {3},
  pages = {267--280},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {1059-7123},
  urldate = {2023-03-14},
  abstract = {We consider the benefits of dream mechanisms ? that is, the ability to simulate new experiences based on past ones ? in a machine learning context. Specifically, we are interested in learning for artificial agents that act in the world, and operationalize ?dreaming? as a mechanism by which such an agent can use its own model of the learning environment to generate new hypotheses and training data.We first show that it is not necessarily a given that such a data-hallucination process is useful, since it can easily lead to a training set dominated by spurious imagined data until an ill-defined convergence point is reached. We then analyse a notably successful implementation of a machine learning-based dreaming mechanism by Ha and Schmidhuber (Ha, D., \& Schmidhuber, J. (2018). World models. arXiv e-prints, arXiv:1803.10122). On that basis, we then develop a general framework by which an agent can generate simulated data to learn from in a manner that is beneficial to the agent. This, we argue, then forms a general method for an operationalized dream-like mechanism.We finish by demonstrating the general conditions under which such mechanisms can be useful in machine learning, wherein the implicit simulator inference and extrapolation involved in dreaming act without reinforcing inference error even when inference is incomplete.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/VG9MU887/Windridge et al. - 2021 - On the utility of dreaming A general model for ho.pdf}
}

@article{wolffInformationCompressionUnifying2019,
  title = {Information {{Compression}} as a {{Unifying Principle}} in {{Human Learning}}, {{Perception}}, and {{Cognition}}},
  author = {Wolff, J. Gerard},
  year = {2019},
  month = feb,
  journal = {Complexity},
  volume = {2019},
  pages = {1--38},
  issn = {1076-2787, 1099-0526},
  urldate = {2022-05-12},
  abstract = {This paper reviews evidence for the idea that much of human learning, perception, and cognition may be understood as information compression and often more specifically as ``information compression via the matching and unification of patterns'' (ICMUP). Evidence includes the following: information compression can mean selective advantage for any creature; the storage and utilisation of the relatively enormous quantities of sensory information would be made easier if the redundancy of incoming information was to be reduced; content words in natural languages, with their meanings, may be seen as ICMUP; other techniques for compression of information{\textemdash}such as class-inclusion hierarchies, schema-plus-correction, run-length coding, and part-whole hierarchies{\textemdash}may be seen in psychological phenomena; ICMUP may be seen in how we merge multiple views to make one, in recognition, in binocular vision, in how we can abstract object concepts via motion, in adaptation of sensory units in the eye of               Limulus               , the horseshoe crab, and in other examples of adaptation; the discovery of the segmental structure of language (words and phrases), grammatical inference, and the correction of over- and undergeneralisations in learning may be understood in terms of ICMUP; information compression may be seen in the perceptual               constancies               ; there is indirect evidence for ICMUP in human cognition via kinds of redundancy such as the decimal expansion of                                                   {$\pi$}                                               which are difficult for people to detect; much of the structure and workings of mathematics{\textemdash}an aid to human thinking{\textemdash}may be understood in terms of ICMUP; and there is additional evidence via the               SP Theory of Intelligence               and its realisation in the               SP Computer Model               . Three objections to the main thesis of this paper are described, with suggested answers. These ideas may be seen to be part of a ``Big Picture'' with six components, outlined in the paper.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/4UZLRVGM/Wolff - 2019 - Information Compression as a Unifying Principle in.pdf}
}

@book{wolfram2023chatgpt,
  title = {What Is {{ChatGPT}} Doing ... and Why Does It Work?},
  author = {Wolfram, S.},
  year = {2023},
  publisher = {{Wolfram Media, Incorporated}},
  isbn = {978-1-57955-081-3},
  lccn = {2023009927}
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor = {Liu, Qun and Schlangen, David},
  year = {2020},
  month = oct,
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  urldate = {2024-01-23},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  file = {/Users/ron/Zotero/storage/L9LLKBX4/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf}
}

@misc{wongLeveragingLanguageLearn2022,
  title = {Leveraging {{Language}} to {{Learn Program Abstractions}} and {{Search Heuristics}}},
  author = {Wong, Catherine and Ellis, Kevin and Tenenbaum, Joshua B. and Andreas, Jacob},
  year = {2022},
  month = may,
  number = {arXiv:2106.11053},
  eprint = {2106.11053},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-08-22},
  abstract = {Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, and generalizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains -- string editing, image composition, and abstract reasoning about scenes -- even when no natural language hints are available at test time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ron/Zotero/storage/HGKZ74HV/Wong et al. - 2022 - Leveraging Language to Learn Program Abstractions .pdf;/Users/ron/Zotero/storage/CFGZHYKF/2106.html}
}

@misc{wongWordModelsWorld2023,
  title = {From {{Word Models}} to {{World Models}}: {{Translating}} from {{Natural Language}} to the {{Probabilistic Language}} of {{Thought}}},
  shorttitle = {From {{Word Models}} to {{World Models}}},
  author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12672},
  eprint = {2306.12672},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-01},
  abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Symbolic Computation},
  file = {/Users/ron/Zotero/storage/R2PMHZIE/Wong et al. - 2023 - From Word Models to World Models Translating from.pdf;/Users/ron/Zotero/storage/GMSIGTYF/2306.html}
}

@book{wooldridgeIntroductionMultiagentSystems2009,
  title = {An Introduction to Multiagent Systems},
  author = {Wooldridge, Michael J.},
  year = {2009},
  edition = {2nd ed},
  publisher = {{John Wiley \& Sons}},
  address = {{Chichester, U.K}},
  isbn = {978-0-470-51946-2},
  langid = {english},
  lccn = {QA76.76.I58 W65 2009},
  keywords = {Intelligent agents (Computer software)},
  annotation = {OCLC: ocn246887666},
  file = {/Users/ron/Zotero/storage/B7U3L5X4/Wooldridge - 2009 - An introduction to multiagent systems.pdf}
}

@misc{xuLLMsAbstractionReasoning2023,
  title = {{{LLMs}} and the {{Abstraction}} and {{Reasoning Corpus}}: {{Successes}}, {{Failures}}, and the {{Importance}} of {{Object-based Representations}}},
  shorttitle = {{{LLMs}} and the {{Abstraction}} and {{Reasoning Corpus}}},
  author = {Xu, Yudong and Li, Wenhao and Vaezipoor, Pashootan and Sanner, Scott and Khalil, Elias B.},
  year = {2023},
  month = may,
  number = {arXiv:2305.18354},
  eprint = {2305.18354},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-02},
  abstract = {Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to "reason" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. Visualizations, GPT logs, and data are available at https://khalil-research.github.io/LLM4ARC.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ron/Zotero/storage/HG6DYJM7/Xu et al. - 2023 - LLMs and the Abstraction and Reasoning Corpus Suc.pdf;/Users/ron/Zotero/storage/I3ALKIWR/2305.html}
}

@article{yangOneModelLearning2022,
  title = {One Model for the Learning of Language},
  author = {Yang, Yuan and Piantadosi, Steven T.},
  year = {2022},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {5},
  pages = {e2021865119},
  issn = {0027-8424, 1091-6490},
  urldate = {2022-12-07},
  abstract = {Significance             It has long been hypothesized that language acquisition may be impossible without innate knowledge of the structures that occur in natural language. Here, we show that a domain general learning setup, originally developed in cognitive psychology to model rule learning, is able to acquire key pieces of natural language from relatively few examples of sentences. This develops a new approach to formalizing linguistic learning and highlights some features of language and language acquisition that may arise from general cognitive processes.           ,                             A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,                                a                 n                              ,                                                                                                                        (                         a                         b                         )                                              n                                                                                       , and                                                                                                                        \{                         a                         ,                         b                         \}                                              +                                                                                       ), context-free (e.g.,                                                                                               a                       n                                                                 b                       n                                          ,                     \,                                            a                       n                                                                 b                                                n                         +                         m                                                                                                              , and                                                                        x                                            x                       R                                                                                       ), and context-sensitive (e.g.,                                                                                               a                       n                                                                 b                       n                                                                 c                       n                                          ,                     \,                                            a                       n                                                                 b                       m                                                                 c                       n                                                                 d                       m                                                                                       , and               xx               ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/NYF4E3VI/Yang and Piantadosi - 2022 - One model for the learning of language}
}

@inproceedings{yangProgramSynthesisGuided2021,
  title = {Program {{Synthesis Guided Reinforcement Learning}} for {{Partially Observed Environments}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yang, Yichen and Inala, Jeevana Priya and Bastani, Osbert and Pu, Yewen and {Solar-Lezama}, Armando and Rinard, Martin},
  year = {2021},
  volume = {34},
  pages = {29669--29683},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-05-08},
  abstract = {A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. However, these approaches impose a high manual burden on the user since they must provide a guiding program for every new task. Partially observed environments further complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible configuration of the hidden regions of the environment. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncertainty. In our experiments, we show that our approach significantly outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent. Our results demonstrate that our approach can obtain the benefits of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.},
  file = {/Users/ron/Zotero/storage/GQ5DEEV7/Yang et al. - 2021 - Program Synthesis Guided Reinforcement Learning fo.pdf}
}

@misc{yildirim3DShapePerception2023,
  title = {{{3D Shape Perception Integrates Intuitive Physics}} and {{Analysis-by-Synthesis}}},
  author = {Yildirim, Ilker and Siegel, Max H. and Soltani, Amir A. and Chaudhari, Shraman Ray and Tenenbaum, Joshua B.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03711},
  eprint = {2301.03711},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  urldate = {2023-02-10},
  abstract = {Many surface cues support three-dimensional shape perception, but people can sometimes still see shape when these features are missing -- in extreme cases, even when an object is completely occluded, as when covered with a draped cloth. We propose a framework for 3D shape perception that explains perception in both typical and atypical cases as analysis-by-synthesis, or inference in a generative model of image formation: the model integrates intuitive physics to explain how shape can be inferred from deformations it causes to other objects, as in cloth-draping. Behavioral and computational studies comparing this account with several alternatives show that it best matches human observers in both accuracy and response times, and is the only model that correlates significantly with human performance on difficult discriminations. Our results suggest that bottom-up deep neural network models are not fully adequate accounts of human shape perception, and point to how machine vision systems might achieve more human-like robustness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/ron/Zotero/storage/Z8NZDUDW/Yildirim et al. - 2023 - 3D Shape Perception Integrates Intuitive Physics a.pdf;/Users/ron/Zotero/storage/57PDZQBS/2301.html}
}

@incollection{yinDesignAutonomousDNA2006,
  title = {Design of {{Autonomous DNA Cellular Automata}}},
  booktitle = {{{DNA Computing}}},
  author = {Yin, Peng and Sahu, Sudheer and Turberfield, Andrew J. and Reif, John H.},
  editor = {Carbone, Alessandra and Pierce, Niles A.},
  year = {2006},
  volume = {3892},
  pages = {399--416},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  urldate = {2024-01-18},
  abstract = {Recent experimental progress in DNA lattice construction, DNA robotics, and DNA computing provides the basis for designing DNA cellular computing devices, i.e. autonomous nano-mechanical DNA computing devices embedded in DNA lattices. Once assembled, DNA cellular computing devices can serve as reusable, compact computing devices that perform (universal) computation, and programmable robotics devices that demonstrate complex motion. As a prototype of such devices, we recently reported the design of an Autonomous DNA Turing Machine, which is capable of universal sequential computation, and universal translational motion, i.e. the motion of the head of a single tape universal mechanical Turing machine. In this paper, we describe the design of an Autonomous DNA Cellular Automaton (ADCA), which can perform parallel universal computation by mimicking a one-dimensional (1D) universal cellular automaton. In the computation process, this device, embedded in a 1D DNA lattice, also demonstrates well coordinated parallel motion. The key technical innovation here is a molecular mechanism that synchronizes pipelined ``molecular reaction waves'' along a 1D track, and in doing so, realizes parallel computation. We first describe the design of ADCA on an abstract level, and then present detailed DNA sequence level implementation using commercially available protein enzymes. We also discuss how to extend the 1D design to 2D.},
  isbn = {978-3-540-34161-1 978-3-540-34165-9},
  langid = {english},
  file = {/Users/ron/Zotero/storage/4XV4WS8D/Yin et al. - 2006 - Design of Autonomous DNA Cellular Automata.pdf}
}

@misc{yoshuabengioScalingService,
  title = {Scaling in the Service of Reasoning \& Model-Based {{ML}}},
  author = {Bengio, Yoshua},
  year = {2022}
}

@article{yuanEmergenceCausalityComplex2023,
  title = {Emergence and {{Causality}} in {{Complex Systems}}: {{A Survey}} on {{Causal Emergence}} and {{Related Quantitative Studies}}},
  author = {Yuan, Bing and Zhang, Jiang and Lyu, Aobo and Wu, Jiayun and Wang, Zhipeng and Yang, Mingzhe and Liu, Kaiwei and Mou, Muyun and Cui, Peng},
  year = {2023},
  abstract = {Emergence and causality are two fundamental concepts for understanding complex systems. They are interconnected. On one hand, emergence refers to the phenomenon where macroscopic properties cannot be solely attributed to the cause of individual properties. On the other hand, causality can exhibit emergence, meaning that new causal laws may arise as we increase the level of abstraction. Causal emergence theory aims to bridge these two concepts and even employs measures of causality to quantify emergence. This paper provides a comprehensive review of recent advancements in quantitative theories and applications of causal emergence. Two key problems are addressed: quantifying causal emergence and identifying it in data. Addressing the latter requires the use of machine learning techniques, thus establishing a connection between causal emergence and artificial intelligence. We highlighted that the architectures used for identifying causal emergence are shared by causal representation learning, causal model abstraction, and world model-based reinforcement learning. Consequently, progress in any of these areas can benefit the others. Potential applications and future perspectives are also discussed in the final section of the review.},
  langid = {english},
  file = {/Users/ron/Zotero/storage/3CGL34GY/Yuan et al. - 2023 - Emergence and Causality in Complex Systems A Surv.pdf}
}

@book{zadra_stickgold_2022,
  title = {When Brains Dream: {{Understanding}} the Science and {{Mystery}} of Our Dreaming Minds},
  author = {Zadra, Antonio and Stickgold, R.},
  year = {2022},
  publisher = {{W.W. Norton \& Company, Inc.}},
  address = {{New York, NY}}
}

@misc{zecevicCausalParrotsLarge2023,
  title = {Causal {{Parrots}}: {{Large Language Models May Talk Causality But Are Not Causal}}},
  shorttitle = {Causal {{Parrots}}},
  author = {Ze{\v c}evi{\'c}, Matej and Willig, Moritz and Dhami, Devendra Singh and Kersting, Kristian},
  year = {2023},
  month = aug,
  number = {arXiv:2308.13067},
  eprint = {2308.13067},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-22},
  abstract = {Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ron/Zotero/storage/273VYIQK/Zečević et al. - 2023 - Causal Parrots Large Language Models May Talk Cau.pdf;/Users/ron/Zotero/storage/AIGU2IXB/2308.html}
}

@article{zeithamovaBrainMechanismsConcept2019,
  title = {Brain {{Mechanisms}} of {{Concept Learning}}},
  author = {Zeithamova, Dagmar and Mack, Michael L. and Braunlich, Kurt and Davis, Tyler and Seger, Carol A. and van Kesteren, Marlieke T. R. and Wutz, Andreas},
  year = {2019},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {39},
  number = {42},
  pages = {8259--8266},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  urldate = {2023-05-08},
  abstract = {Concept learning, the ability to extract commonalities and highlight distinctions across a set of related experiences to build organized knowledge, is a critical aspect of cognition. Previous reviews have focused on concept learning research as a means for dissociating multiple brain systems. The current review surveys recent work that uses novel analytical approaches, including the combination of computational modeling with neural measures, focused on testing theories of specific computations and representations that contribute to concept learning. We discuss in detail the roles of the hippocampus, ventromedial prefrontal, lateral prefrontal, and lateral parietal cortices, and how their engagement is modulated by the coherence of experiences and the current learning goals. We conclude that the interaction of multiple brain systems relating to learning, memory, attention, perception, and reward support a flexible concept-learning mechanism that adapts to a range of category structures and incorporates motivational states, making concept learning a fruitful research domain for understanding the neural dynamics underlying complex behaviors.},
  chapter = {Symposium and Mini-Symposium},
  copyright = {Copyright {\textcopyright} 2019 the authors},
  langid = {english},
  pmid = {31619495},
  keywords = {categorization,computational modeling,fMRI,hippocampus,parietal cortex,prefrontal cortex},
  file = {/Users/ron/Zotero/storage/BL956KEY/Zeithamova et al. - 2019 - Brain Mechanisms of Concept Learning.pdf}
}

@article{zeitzARTCRAFTPROBLEM,
  title = {{{THE ART AND CRAFT OF PROBLEM SOLVING}}},
  author = {Zeitz, Paul},
  langid = {english},
  file = {/Users/ron/Zotero/storage/N7FLF548/Zeitz - THE ART AND CRAFT OF PROBLEM SOLVING.pdf}
}

@book{zenilComputableUniverseUnderstanding2013,
  title = {A Computable Universe: Understanding and Exploring Nature as Computation},
  shorttitle = {A Computable Universe},
  editor = {Zenil, Hector and Penrose, Roger},
  year = {2013},
  publisher = {{World Scientific}},
  address = {{Singapore ; Hackensack, N.J}},
  isbn = {978-981-4374-29-3},
  langid = {english},
  lccn = {QA267.7 .C676 2013},
  keywords = {Computational complexity},
  annotation = {OCLC: ocn758387132},
  file = {/Users/ron/Zotero/storage/MFDCS3GZ/Zenil and Penrose - 2013 - A computable universe understanding and exploring.pdf}
}

@book{zhaiTextDataManagement2016,
  title = {Text {{Data Management}} and {{Analysis}}: {{A Practical Introduction}} to {{Information Retrieval}} and {{Text Mining}}},
  shorttitle = {Text {{Data Management}} and {{Analysis}}},
  author = {Zhai, ChengXiang and Massung, Sean},
  year = {2016},
  month = jun,
  publisher = {{Association for Computing Machinery and Morgan \& Claypool}},
  address = {{New York, NY, USA}},
  urldate = {2024-01-27},
  isbn = {978-1-970001-17-4},
  langid = {english},
  file = {/Users/ron/Zotero/storage/LP7BZGHX/Zhai and Massung - 2016 - Text Data Management and Analysis A Practical Int.pdf}
}

@article{zhangConnectingConceptsBrain2020,
  title = {Connecting Concepts in the Brain by Mapping Cortical Representations of Semantic Relations},
  author = {Zhang, Yizhen and Han, Kuan and Worth, Robert and Liu, Zhongming},
  year = {2020},
  month = apr,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {1877},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  urldate = {2022-12-22},
  abstract = {In the brain, the semantic system is thought to store concepts. However, little is known about how it connects different concepts and infers semantic relations. To address this question, we collected hours of functional magnetic resonance imaging data from human subjects listening to natural stories. We developed a predictive model of the voxel-wise response and further applied it to thousands of new words. Our results suggest that both semantic categories and relations are represented by spatially overlapping cortical patterns, instead of anatomically segregated regions. Semantic relations that reflect conceptual progression from concreteness to abstractness are represented by cortical patterns of activation in the default mode network and deactivation in the frontoparietal attention network. We conclude that the human brain uses distributed networks to encode not only concepts but also relationships between concepts. In particular, the default mode network plays a central role in semantic processing for abstraction of concepts.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Engineering,Neuroscience},
  file = {/Users/ron/Zotero/storage/3WKPX2X3/Zhang et al. - 2020 - Connecting concepts in the brain by mapping cortic.pdf}
}

@misc{zhangGenerativeFlowNetworks2022,
  title = {Generative {{Flow Networks}} for {{Discrete Probabilistic Modeling}}},
  author = {Zhang, Dinghuai and Malkin, Nikolay and Liu, Zhen and Volokhova, Alexandra and Courville, Aaron and Bengio, Yoshua},
  year = {2022},
  month = jun,
  number = {arXiv:2202.01361},
  eprint = {2202.01361},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-11-21},
  abstract = {We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN's effectiveness on various probabilistic modeling tasks. Code is publicly available at https://github.com/zdhNarsil/EB\_GFN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/GN9QUBE5/Zhang et al. - 2022 - Generative Flow Networks for Discrete Probabilisti.pdf;/Users/ron/Zotero/storage/59QWYK38/2202.html}
}

@article{zhangHippocampalSpatialRepresentations2023,
  title = {Hippocampal Spatial Representations Exhibit a Hyperbolic Geometry That Expands with Experience},
  author = {Zhang, Huanqiu and Rich, P. Dylan and Lee, Albert K. and Sharpee, Tatyana O.},
  year = {2023},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {26},
  number = {1},
  pages = {131--139},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  urldate = {2023-07-25},
  abstract = {Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the CA1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of CA1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Learning and memory,Neural encoding},
  file = {/Users/ron/Zotero/storage/IYQFNVWL/Zhang et al. - 2023 - Hippocampal spatial representations exhibit a hype.pdf}
}

@inproceedings{zhangNeuralGuidedConstraint2018,
  title = {Neural {{Guided Constraint Logic Programming}} for {{Program Synthesis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Lisa and Rosenblatt, Gregory and Fetaya, Ethan and Liao, Renjie and Byrd, William and Might, Matthew and Urtasun, Raquel and Zemel, Richard},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-05-08},
  abstract = {Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren's internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.},
  file = {/Users/ron/Zotero/storage/ZL54PXSA/Zhang et al. - 2018 - Neural Guided Constraint Logic Programming for Pro.pdf}
}

@inproceedings{zhangNovelNeuralSource2019,
  title = {A {{Novel Neural Source Code Representation Based}} on {{Abstract Syntax Tree}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
  year = {2019},
  month = may,
  pages = {783--794},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  urldate = {2023-05-08},
  abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
  isbn = {978-1-72810-869-8},
  langid = {english},
  file = {/Users/ron/Zotero/storage/39Q8L7T8/Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf}
}

@misc{zhangUnifyingGenerativeModels2022,
  title = {Unifying {{Generative Models}} with {{GFlowNets}}},
  author = {Zhang, Dinghuai and Chen, Ricky T. Q. and Malkin, Nikolay and Bengio, Yoshua},
  year = {2022},
  month = sep,
  number = {arXiv:2209.02606},
  eprint = {2209.02606},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-11-21},
  abstract = {There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. We present a short note on the connections between existing deep generative models and the GFlowNet framework, shedding light on their overlapping traits and providing a unifying viewpoint through the lens of learning with Markovian trajectories. This provides a means for unifying training and inference algorithms, and provides a route to construct an agglomeration of generative models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/BYSAQEUS/Zhang et al. - 2022 - Unifying Generative Models with GFlowNets.pdf;/Users/ron/Zotero/storage/KVY25EUG/2209.html}
}

@misc{zhangUnifyingGenerativeModels2023,
  title = {Unifying {{Generative Models}} with {{GFlowNets}} and {{Beyond}}},
  author = {Zhang, Dinghuai and Chen, Ricky T. Q. and Malkin, Nikolay and Bengio, Yoshua},
  year = {2023},
  month = jan,
  number = {arXiv:2209.02606},
  eprint = {2209.02606},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-28},
  abstract = {There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. Here, we demonstrate the connections between existing deep generative models and the recently introduced GFlowNet framework, a probabilistic inference machine which treats sampling as a decision-making process. This analysis sheds light on their overlapping traits and provides a unifying viewpoint through the lens of learning with Markovian trajectories. Our framework provides a means for unifying training and inference algorithms, and provides a route to shine a unifying light over many generative models. Beyond this, we provide a practical and experimentally verified recipe for improving generative modeling with insights from the GFlowNet perspective.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ron/Zotero/storage/4VK49IUB/Zhang et al. - 2023 - Unifying Generative Models with GFlowNets and Beyo.pdf;/Users/ron/Zotero/storage/AUDLIE5X/2209.html}
}

@inproceedings{zhaoConsciousnessInspiredPlanningAgent2021,
  title = {A {{Consciousness-Inspired Planning Agent}} for {{Model-Based Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhao, Mingde and Liu, Zhen and Luan, Sitao and Zhang, Shuyuan and Precup, Doina and Bengio, Yoshua},
  year = {2021},
  volume = {34},
  pages = {1569--1581},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-11},
  abstract = {We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance.},
  file = {/Users/ron/Zotero/storage/CZ2Y6BRT/Zhao et al. - 2021 - A Consciousness-Inspired Planning Agent for Model-.pdf}
}

@misc{madan2023learning,
  title={Learning GFlowNets from partial episodes for improved convergence and stability}, 
  author={Kanika Madan and Jarrid Rector-Brooks and Maksym Korablyov and Emmanuel Bengio and Moksh Jain and Andrei Nica and Tom Bosc and Yoshua Bengio and Nikolay Malkin},
  year={2023},
  eprint={2209.12782},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{Pouget_Beck_Ma_Latham_2013, 
  title={Probabilistic brains: knowns and unknowns}, 
  volume={16}, 
  ISSN={1546-1726}, 
  DOI={10.1038/nn.3495}, 
  abstractNote={There is strong behavioral and physiological evidence that the brain both represents probability distributions and performs probabilistic inference. Computational neuroscientists have started to shed light on how these probabilistic representations and computations might be implemented in neural circuits. One particularly appealing aspect of these theories is their generality: they can be used to model a wide range of tasks, from sensory processing to high-level cognition. To date, however, these theories have only been applied to very simple tasks. Here we discuss the challenges that will emerge as researchers start focusing their efforts on real-life computations, with a focus on probabilistic learning, structural learning and approximate inference.}, 
  number={9}, journal={Nature Neuroscience}, 
  author={Pouget, Alexandre and Beck, Jeffrey M. and Ma, Wei Ji and Latham, Peter E.}, 
  year={2013}, 
  month=sep, 
  pages={1170–1178}, 
  language={eng} 
}

@article{Strick_Dum_Fiez_2009, 
  title={Cerebellum and nonmotor function}, 
  volume={32}, 
  ISSN={1545-4126}, 
  DOI={10.1146/annurev.neuro.31.060407.125606}, 
  abstractNote={Does the cerebellum influence nonmotor behavior? Recent anatomical studies demonstrate that the output of the cerebellum targets multiple nonmotor areas in the prefrontal and posterior parietal cortex, as well as the cortical motor areas. The projections to different cortical areas originate from distinct output channels within the cerebellar nuclei. The cerebral cortical area that is the main target of each output channel is a major source of input to the channel. Thus, a closed-loop circuit represents the major architectural unit of cerebro-cerebellar interactions. The outputs of these loops provide the cerebellum with the anatomical substrate to influence the control of movement and cognition. Neuroimaging and neuropsychological data supply compelling support for this view. The range of tasks associated with cerebellar activation is remarkable and includes tasks designed to assess attention, executive control, language, working memory, learning, pain, emotion, and addiction. These data, along with the revelations about cerebro-cerebellar circuitry, provide a new framework for exploring the contribution of the cerebellum to diverse aspects of behavior.}, 
  journal={Annual Review of Neuroscience}, 
  author={Strick, Peter L. and Dum, Richard P. and Fiez, Julie A.}, 
  year={2009}, 
  pages={413–434}, 
  language={eng} 
}

@article{Tee_Taylor_2019, 
  title={A Quantized Representation of Probability in the Brain}, 
  volume={5}, 
  ISSN={2332-7804}, 
  DOI={10.1109/tmbmc.2019.2950182}, 
  abstractNote={Conventional and current wisdom assumes that the brain represents probability as a continuous number to many decimal places. This assumption seems implausible given finite and scarce resources in the brain. Quantization is an information encoding process whereby a continuous quantity is systematically divided into a finite number of possible categories. Rounding is a simple example of quantization. We apply this information theoretic concept to develop a novel quantized (i.e., discrete) probability distortion function. We develop three conjunction probability gambling tasks to look for evidence of quantized probability representations in the brain. We hypothesize that certain ranges of probability will be lumped together in the same indifferent category if a quantized representation exists. For example, two distinct probabilities such as 0.57 and 0.585 may be treated indifferently. Our extensive data analysis has found strong evidence to support such a quantized representation: 59/76 participants (i.e., 78%) demonstrated a best fit to 4-bit quantized models instead of continuous models. This observation is the major development and novelty of the present work. The brain is very likely to be employing a quantized representation of probability. This discovery demonstrates a major precision limitation of the brain’s representational and decision-making ability.}, 
  number={1}, 
  journal={IEEE transactions on molecular, biological, and multi-scale communications}, 
  author={Tee, James and Taylor, Desmond P.}, 
  year={2019}, 
  month=oct, 
  pages={19–29} 
}

@article{Griffiths_Zhu_Grant_McCoy_2023, 
  title={Bayes in the age of intelligent machines}, 
  url={http://arxiv.org/abs/2311.10206}, 
  abstractNote={The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data.}, 
  note={arXiv:2311.10206 [cs]}, 
  number={arXiv:2311.10206}, 
  publisher={arXiv}, 
  author={Griffiths, Thomas L. and Zhu, Jian-Qiao and Grant, Erin and McCoy, R. Thomas}, 
  year={2023}, 
  month=nov 
}

@article{marr1976understanding,
  title={From understanding computation to understanding neural circuitry},
  author={Marr, David and Poggio, Tomaso},
  year={1976}
}






