

% \section{Examples of holarchies:}

% Language: The construction of language from letters to words, sentences, paragraphs, etc. forms a hierarchy where the basic building blocks are combined to create increasingly complex structures.

% Music: Music has self-similar patterns, with notes forming motifs, motifs forming phrases, phrases forming sections, and so on, creating symphonies. Rhythmic and melodic patterns are often repeated at different scales to create coherence in composition.

% Social Structures: Individuals form families, families form communities, communities form societies, and societies form civilizations. Governance and social norms often have similar patterns repeated at each level of this hierarchy.

% The idea of holarchy is reflected in:
% Schema Theory: Cognitive structures known as schemas are used to organize knowledge. These are mental frameworks that help individuals understand and interpret information. Schemas build upon each other; for example, we have object schemas (like 'dog'), which are part of larger event schemas ('walking a dog'), which then integrate into even broader narrative schemas (a day in the life).

% Conceptual Metaphors: George Lakoff and Mark Johnson's work on metaphors suggests that our abstract thinking is fundamentally metaphorical, built upon simpler, bodily experiences. For example, we understand time as a resource ("saving time") or as a path ("looking forward to the weekend"), extending basic physical experiences into the realm of complex thought.

% Piaget's Theory of Cognitive Development: Jean Piaget proposed that children construct an understanding of the world around them, experience discrepancies between what they know and what they discover in their environment, and then adjust their ideas accordingly. This constructivist approach posits that learning is a self-organizing process where the complexity of understanding increases over time.

% Thoughts: These are the most basic units, akin to neurons or individual cells in biology. They are the raw mental events that occur in response to stimuli or internal processes.

% Ideas: Ideas are more complex and structured, formed by the association and integration of multiple thoughts, similar to how cells form tissues. They can be viewed as the connections and patterns that arise from the neural network of thoughts.

% Beliefs: Beliefs arise from reinforced ideas. Just as tissues organize into organs with specific functions, beliefs represent a more organized and functional assembly of ideas. They are harder to change because they are reinforced by recurring thought patterns and are integral to the structure of our mental 'organism.'

% Ideologies: At the highest level, ideologies are systems of beliefs, akin to entire organisms or ecosystems in biology. They are complex, integrated structures of beliefs that guide behavior and interpret the world, much like how an organism interacts with its environment.

% Each of these should again be nested. E.g. the concept of a bird is a nested holarchy of separate birds


\section{Cognition, Nested Multi-scale Hierarchies, and Self-Organisation}

Biological systems demonstrate remarkable complexity through self-organization, a process where spontaneous pattern formation results from the interactions of individual components within an environment. 

Biological organization is composed of nested goal-seeking agents, maintaining operational and organizational closure through homeostasis and allostasis \cite{ciaunica_nested_2023, vernon_embodied_2015} [explain terms in more detail, or is glossary enough?].

Atoms form molecules, which form cells, tissues, organs, organisms, groups, societies, civilizations, and so on. Levin describes this as a multiscale competency architecture, which is not only structural but also functional \cite{Levin_2023}. Each level of this organization has some competency and solves problems in its own action space. Moreover, each level interacts with the levels above and below which is reflected in the idea of circular causality \cite{ciaunica_nested_2023, vernon_embodied_2015}. 
% Douglas Hofstadter calls this a heterarchy and gives the example






\subsection{LLMs/ Self-attention / transformers}
% generative models. 
% explain the encoder decoder architecture.
% grounding, etc. 
% embedding spaces.
% show that this allows for analogical thought and reasoning
% perhaps include bit from AAPS proposal

\begin{itemize}
    \item could LLMs be regarded as symbolic like models? since each token is a vector and then the model builds aggregate representations given those vectors?
\end{itemize}

In his recent book "What is ChatGPT doing?", Stephen Wolfram analyses the internal mechanisms of ChatGPT \cite{wolfram2023chatgpt}.

It would be remiss to not describe and analyze large language models (LLMs) such as ChatGPT [source], Bard [source], and Llama [source], given their recent undeniable and phenomenal successes. These models are descendents of the Transformer architecture, originally introduced in 2017 by Vaswani et al. \cite{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017} and are trained on massive datasets of text and code. This allows them to learn the statistical relationships between words and phrases, and to \source[inline]{generate human-quality text, translate languages, write different kinds of creative content, and answer questions in an informative way}.

[in order to try to be self-contained,] I will briefly outline the inner workings of these neural network architectures. 

Embedding is the process of converting tokens into dense vectors of numbers. These vectors represent the semantic and syntactic information about the tokens. The embedding layer is important for LLMs because it allows them to learn the relationships between tokens in a high-dimensional space.
Akin to Gördenford's conceptual spaces \ref{subsec:gordenford}, tokens are essentially laid out in a high-dimensional space with the intention of tokens being organized according to their semantic meaning. E.g. "dog" should be closer to "canine" than to "panda". 
Various levels of embeddings are encoded, which gives aggregate words, sentences, paragraphs, etc. positions in this latent space.

At the heart of the Transformer lies the self-attention mechanism, which enables the model to weigh the significance of different tokens in a sentence relative to a given token.

Formally, 
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}
where:
\begin{conditions*}
    Q & A matrix of query vectors, where each row represents a query for a single word in the input sequence. \\
    K & A matrix of key vectors, where each row represents a key for a single word in the input sequence. \\
    V & A matrix of value vectors, where each row represents a value for a single word in the input sequence. \\
    d_k & The dimension of the key and query vectors. \\
    \text{softmax} & A function that normalizes the attention scores such that they sum to 1.
\end{conditions*}

The significance of each part of the formula is as follows:

Q, K, and V: These matrices are obtained by projecting the input sequence into three different spaces using three separate weight matrices. This projection allows the model to learn different representations of the input sequence, which can be useful for different tasks.
$\frac{QK^T}{\sqrt{dk}}$: This term calculates the similarity between each query vector and each key vector. The dot product of two vectors is a measure of their similarity, but it is also sensitive to the scale of the vectors. Dividing by the square root of the dimension of the key vectors helps to normalize the attention scores such that they are comparable across different words in the input sequence.
softmax: The softmax function normalizes the attention scores such that they sum to 1. This ensures that the attention weights represent a probability distribution over the words in the input sequence.
V: The value matrix contains the representations of the words in the input sequence that will be attended to. The attention weights are multiplied by the value matrix to produce a weighted sum of the value vectors. This weighted sum represents the context-aware representation of the current word in the input sequence.
Self-attention is a powerful mechanism that allows models to learn long-range dependencies in input sequences. This is because the attention weights can be used to focus on different parts of the input sequence, depending on the context of the current word.

Multi-Head Attention: Instead of a single set of attention weights, the model uses multiple sets, enabling it to focus on different parts of the input for different tasks simultaneously.
Feed-forward Networks: Each layer of the transformer contains a feed-forward neural network, which is applied independently to each position. [This allows the transformer to make statistics over compounded sentences (does wolfram explain it?)]

The \emph{encoder} is a block that takes an input sequence of tokens and produces a sequence of hidden states. The hidden states represent the information that the encoder has extracted from the input sequence.

The \emph{decoder} is a neural network that takes the hidden states from the encoder and produces an output sequence of tokens. The decoder is trained to predict the next word in the sequence, given the previous words and the hidden states.

Next token prediction.

In his book, Wolfram shows that GPT doesn't have any explicit knowledge of grammar and the parse trees that govern natural language, yet they learn these nested syntax trees implicitly. 
Nonsense sentences such as Chomsky's famous example "Colorless green ideas sleep furiously", or jabberwocky sentences, show that there is more to language than syntax. There seems to be a semantic grammar, which LLM models learn implicitly.

Because LLMs have seen vast amount of sentences of the structure if $X$ then $Y$
Logic is a way of saying that sentences that follow certain patterns are reasonable, while others are not. For example, it's reasonable to say "All X are Y. This is not Y, so it's not an X" (as in "All giraffes are tall. This is not tall, so it's not a giraffe.").

It's possible that LLMs have "discovered" syllogistic logic by looking at huge amounts of text. However, when it comes to more sophisticated formal logic, they fail [source], since they learned this implicit logic by correlation and they don't "know" the actual rules of logic.

- we don't know yet whether implicit logic has actually been discovered or whether the model is just that good by correlation, since it has seen so much data.


This is the learnt semantic space. We can look e.g. at analogies such as man is to king as woman to queen.

When LLMs create a sentence, they are essentially following a trajectory in semantic space. This can be seen as a POMDP?


\subsubsection{Limitations}
\rephrase[inline]{from JB: LLMs learn how to complete sequences, they do statistics over patterns}
Stephen shows that when LLMs create statistics over text, they first discover patterns, then syntax, then style, and only lastly semantics. Humans however start out by discovering semantics and then find patterns, syntax and lastly style. 
















\subsection{Compositionality}
\cite{Lake_Ullman_Tenenbaum_Gershman_2017}.

Humans possess a native capacity for meta-cognitive evaluation, instinctively gauging the reliability of their own knowledge. Such self-assessment serves as a compass for learning, steering the acquisition of new information in a manner that refines and optimizes our cognitive architectures.

Compositionality is central to cognitive productivity and learning. It allows for the creation of infinitely varied representations from a finite set of basic elements, similar to the mind's capacity to think an endless array of thoughts or generate countless sentences. This is particularly useful in forming hierarchical structures that simplify complex relationships, thus making inductive reasoning more efficient.

Humans are endowed with an intuitive grasp of causality, which functions as the underpinning for both predictive and counterfactual reasoning.  This enables humans to extrapolate beyond the confines of empirical data, to conjure hypothetical worlds e.g. in imagination, play, and dreaming.


- Can LLMs extract composable entities to recombine?
\subsection{Correlation Does Not Imply Causation}

Generative models assign probability distributions, representing observations in such a way that they can generate new data points similar to the observations. The mapping between the probability distributions and the data is correlational. 

Causal models in contrast, represent hypotheses of how observations were generated. They do not just represent the observations by correlation, but aim to accurately reflect the mechanisms by which the data originated.[source]



\subsection{Causal Models}
A causal model typically consists of a set of variables and a set of directed edges between these variables, often represented as a Directed Acyclic Graph (DAG) [ref from GFN]. The vertices in this graph denote variables, which, in this context, could be seen as cognitive states. The directed edges, meanwhile, signify causal relationships, pointing from cause to effect.

these directed edges often correspond to conditional probability distributions. For instance, if we have a directed edge from 
$X \to Y$, it implies that the distribution of 
$Y$ is conditional on $X$, i.e. $P(Y \vert X)$. The model encompasses these conditional distributions for all variables given their parents in the DAG, encapsulating the joint distribution over all variables.

Causal models facilitate interventions, counterfactuals, and causal inference. Intervention is the ability to model the outcome of purposeful changes, represented mathematically as "do" operations [pearl]. For example, if one were to intervene to set 
$X = x$, the model would enable us to compute the distribution over $Y$ post-intervention.

Counterfactual reasoning allows us to traverse back in time, in a sense, and assess alternative realities—what would have happened to $Y$ if $X$ had been different? This provides a structured framework for hindsight, and one could argue, even a modicum of wisdom.

[refer to pearls paper of three levels of causality \cite{Pearl_2018}, go more in detail?]

Using GFlowNet we can make inferences about intermediate variables. This is because we approximate probability distributions and not point estimates.















\subsection{System 1 \& System 2}

Kahneman describes two distinct systems that characterize the dual aspects of human cognition [source]:
\emph{System 1} operates automatically and quickly, with little or no effort and no sense of voluntary control. It encompasses intuitive judgments and perceptual associations — it allows for rapid, heuristic-based processing that is often subconscious.
This system is akin to the generative model in Bayesian inference. Just as System 1 can produce instantaneous responses and intuitions, the generative model provides immediate perceptual hypotheses and predictions that guide behavior in a fluid and dynamic manner without the need for conscious deliberation.

\emph{System 2} allocates attention to the effortful mental activities that demand it, including complex computations. It is associated with the conscious, rational mind and is deliberate, effortful, and orderly. It can be equated to the recognition density in Bayesian inference. The process of updating the recognition density is more reflective and can be related to the slow, controlled inference that characterizes conscious thought. Adjusting the recognition density is akin to the effortful correction or modulation of System 1's rapid predictions when errors are detected or when more complex reasoning is required.

- example: when learning to play piano, we need conscious effort on certain phrases. Once chunked, we can use them to learn more complex phrases.


\cite{Lake_Ullman_Tenenbaum_Gershman_2017}


[how do we get those primitives? I think we can get them innately, through extraction, or perhaps else, so there may be multiple sources. this means that we dont only improve the dsl internally, but perhaps also through communication (memes) or further extraction; however, don't mistake the explanation of something as the actual thought. you can never see how to actually construct a thought. but concepts are symbols and people can show you the symbolic trajectory you need to take to get to the result. ]








\subsection{Language of Thought}

Jerry Fodor suggests that thought takes place within a mental language - sometimes referred to as "Mentalese." Under this hypothesis, our cognitive processes can be viewed as computations involving a system of mental representations that can be composed into complex thoughts, akin to how sentences are formed from words according to the rules of grammar. Fodor's theory implies an innate structure underlying human cognition, with systematic, rule-governed operations that manipulate symbols.


Keep this here or in discussion?
\begin{itemize}
    \item frame-problem
    \item IBE + abduction
    \item Markpoel 7 criteria
\end{itemize}







Under the Free-Energy Principle [necessary?, maybe just Bayesian principles in general], organisms are seen to construct internal models of the internal and external world to predict and hence reduce the surprise of sensory inputs, which requires a similarly nested, hierarchical organization of cognitive processes \cite{friston_free-energy_2010, friston_world_2021}. 

Instead of viewing the brain as a passive data collector, the brain is a query mechanism, primarily engaged in top-down prediction. The primary function becomes generating predictions. What ascends is the prediction error, the mismatch between predictions and actual [sensorimotor, proprioceptive, interoceptive, etc. ] input. In this framework, perception becomes an query-response mechanism. The brain tries to infer probable causal factors from its received data.

\section{Bayesian Inference}

The fundamental tenet of Bayesian computational cognition is that the brain interprets the world by forming probabilistic models and updates them according to Bayesian inference principles. This means the brain weighs prior beliefs (previous experiences and knowledge) and the likelihood of new sensory evidence to arrive at posterior beliefs (updated model of the world). The brain uses these posterior beliefs to make predictions about future events, thus enabling adaptive behavior.

Bayesian inference can be formalized by:

\[ P(H \vert E) = \frac{P(E \vert H) \cdot P(H)}{P(E)} \]
Where:
\begin{itemize}
    \item \( P(H \vert E) \) is the posterior probability of hypothesis \( H \) given evidence \( E \).
    \item \( P(E \vert H) \) is the likelihood of evidence \( E \) given that hypothesis \( H \) is true.
    \item \( P(H) \) is the prior probability of hypothesis \( H \).
    \item \( P(E) \) is the marginal likelihood, probability of evidence \( E \).
\end{itemize}

In this Bayesian framework, the joint probability \( P(E, H) = P(E \vert H) \cdot P(H) \) is referred to as the generative model that hypothesizes how sensory data are generated by the hidden states of the world. In other words, it specifies a joint probability distribution over sensory inputs and potential causes of those inputs. These causes can be anything from the presence of objects in the environment to more abstract concepts like social cues.

The recognition model is an approximation of the posterior \(Q(H \vert E) \approx P(H \vert E) \) the brain's inference about the state of the world given the data.
[explain marginalisation problem?]





% \subsection{Concepts}
% what is necessary?


% \subsection{LLMs}
% - But LLMs already solve a lot of that.


% PROS:
% - Semantics vs Syntax, semantic grammar
% - vector representations of concepts, vector algebra, distance, etc. 
% - Compositionality


% CONS:
% - They conflate an implicit world model with inference. (model in wolframs terms)
% - they lack causality.
% - pattern to syntax etc. vs other way around


% - LOT









\subsection{Game Engine in the Head}
% \begin{quote}
%     "All I ever wanted was to pick apart the day, put the pieces back together my way." - Aesop Rock
% \end{quote}

\begin{itemize}
    \item game engine in the head
    \item Reverse engineering the world
    \item Intuitive physics etc. 
    \item Extension of the LOT, circumventing the downfall of GOFAI, frame-problem, etc. 
\end{itemize}


\section{Probabilistic Programming}

Dehaene et al. posit that human cognition is uniquely characterized by its ability to form symbolic representations and recursive mental structures akin to a language of thought, enabling the creation of domain-specific conceptual systems \cite{dehaene_symbols_2022}. This cognitive ability allows for the generation of new concepts through the compositional arrangement of existing elements, a process exemplified by the derivation of geometric concepts. Cognition simplifies complex patterns into mental representations via mental compression, where the complexity of a concept is measured by the length of its mental representation as per the Minimum Description Length (MDL) principle.

Neuroscientific research indicates the presence of specialized brain circuits responsible for processing different domains of these languages of thought, with certain brain regions involved in linguistic processing and others in non-linguistic domains like mathematics and spatial reasoning. The recognition of mathematical patterns is linked to the ability to detect repetition with variation, a process underpinned by specific neural areas that vary with cognitive domain.

Distinct from other primates, humans have developed the capability to use symbols in complex, rule-based systems, highlighting a unique aspect of human cognitive development. Non-human primates may associate signs with concepts; however, they do not appear to use these in the recursive, rule-based manner that humans do.

[Tenenbaum et al] posit that the brain implements mechanisms analogous to those found in probabilistic programming languages, enabling it to represent and infer the probabilistic structure of the world. Probabilistic programming provides a framework for defining complex probabilistic models and for performing inference in these models, and the hypothesis is that the brain engages in similar computational processes. [multiple sources]

Experiments show that humans do not seem to start from blank-slate but rather from rich domain knowledge [argument for primitives, more sources] \cite{lake_building_2016}. Lake et al. propose that concepts can be represented as simple stochastic programs [elaborate]. A program here can be thought of a procedure that generates more examples of the same concept. If a program would represent the concept "animal", if would generate examples such as "giraffe", "zebra", "fish", and so on. Of course, higher-level programs could produce lower-level programs, in other words, in this paradigm, the essential aspect of compositionality gives rise to a part-whole hierarchical structure, i.e. a holarchy [reference].

[the assumption we make here is that features are somehow aggregated or extracted into symbols [see \cite{garcez_neurosymbolic_2020}], primitives, along with possibly innate symbols \cite{Lake_Ullman_Tenenbaum_Gershman_2017}] 


