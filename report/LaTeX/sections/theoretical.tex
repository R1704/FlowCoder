% \chapter{Theoretical Framework}




% Examples of holarchies:

% Language: The construction of language from letters to words, sentences, paragraphs, etc. forms a hierarchy where the basic building blocks are combined to create increasingly complex structures.

% Music: Music has self-similar patterns, with notes forming motifs, motifs forming phrases, phrases forming sections, and so on, creating symphonies. Rhythmic and melodic patterns are often repeated at different scales to create coherence in composition.

% Social Structures: Individuals form families, families form communities, communities form societies, and societies form civilizations. Governance and social norms often have similar patterns repeated at each level of this hierarchy.

% The idea of holarchy is reflected in:
% Schema Theory: Cognitive structures known as schemas are used to organize knowledge. These are mental frameworks that help individuals understand and interpret information. Schemas build upon each other; for example, we have object schemas (like 'dog'), which are part of larger event schemas ('walking a dog'), which then integrate into even broader narrative schemas (a day in the life).

% Conceptual Metaphors: George Lakoff and Mark Johnson's work on metaphors suggests that our abstract thinking is fundamentally metaphorical, built upon simpler, bodily experiences. For example, we understand time as a resource ("saving time") or as a path ("looking forward to the weekend"), extending basic physical experiences into the realm of complex thought.

% Piaget's Theory of Cognitive Development: Jean Piaget proposed that children construct an understanding of the world around them, experience discrepancies between what they know and what they discover in their environment, and then adjust their ideas accordingly. This constructivist approach posits that learning is a self-organizing process where the complexity of understanding increases over time.

% Thoughts: These are the most basic units, akin to neurons or individual cells in biology. They are the raw mental events that occur in response to stimuli or internal processes.

% Ideas: Ideas are more complex and structured, formed by the association and integration of multiple thoughts, similar to how cells form tissues. They can be viewed as the connections and patterns that arise from the neural network of thoughts.

% Beliefs: Beliefs arise from reinforced ideas. Just as tissues organize into organs with specific functions, beliefs represent a more organized and functional assembly of ideas. They are harder to change because they are reinforced by recurring thought patterns and are integral to the structure of our mental 'organism.'

% Ideologies: At the highest level, ideologies are systems of beliefs, akin to entire organisms or ecosystems in biology. They are complex, integrated structures of beliefs that guide behavior and interpret the world, much like how an organism interacts with its environment.

% Each of these should again be nested. E.g. the concept of a bird is a nested holarchy of separate birds


\section{Cognition, Nested Multi-scale Hierarchies, and Self-Organisation}

Biological systems demonstrate remarkable complexity through self-organization, a process where spontaneous pattern formation results from the interactions of individual components within an environment. 

Biological organization is composed of nested goal-seeking agents, maintaining operational and organizational closure through homeostasis and allostasis \cite{ciaunica_nested_2023, vernon_embodied_2015} [explain terms in more detail, or is glossary enough?].

Atoms form molecules, which form cells, tissues, organs, organisms, groups, societies, civilizations, and so on. Levin describes this as a multiscale competency architecture, which is not only structural but also functional \cite{Levin_2023}. Each level of this organization has some competency and solves problems in its own action space. Moreover, each level interacts with the levels above and below which is reflected in the idea of circular causality \cite{ciaunica_nested_2023, vernon_embodied_2015}. 
% Douglas Hofstadter calls this a heterarchy and gives the examp







\subsection{Compositionality}
\cite{Lake_Ullman_Tenenbaum_Gershman_2017}.

Humans possess a native capacity for meta-cognitive evaluation, instinctively gauging the reliability of their own knowledge. Such self-assessment serves as a compass for learning, steering the acquisition of new information in a manner that refines and optimizes our cognitive architectures.

Compositionality is central to cognitive productivity and learning. It allows for the creation of infinitely varied representations from a finite set of basic elements, similar to the mind's capacity to think an endless array of thoughts or generate countless sentences. This is particularly useful in forming hierarchical structures that simplify complex relationships, thus making inductive reasoning more efficient.

Humans are endowed with an intuitive grasp of causality, which functions as the underpinning for both predictive and counterfactual reasoning.  This enables humans to extrapolate beyond the confines of empirical data, to conjure hypothetical worlds e.g. in imagination, play, and dreaming.



% \subsection{Correlation Does Not Imply Causation}

% Generative models assign probability distributions, representing observations in such a way that they can generate new data points similar to the observations. The mapping between the probability distributions and the data is correlational. 

% Causal models in contrast, represent hypotheses of how observations were generated. They do not just represent the observations by correlation, but aim to accurately reflect the mechanisms by which the data originated.[source]



% \subsection{Causal Models}
% A causal model typically consists of a set of variables and a set of directed edges between these variables, often represented as a Directed Acyclic Graph (DAG) [ref from GFN]. The vertices in this graph denote variables, which, in this context, could be seen as cognitive states. The directed edges, meanwhile, signify causal relationships, pointing from cause to effect.

% these directed edges often correspond to conditional probability distributions. For instance, if we have a directed edge from 
% $X \to Y$, it implies that the distribution of 
% $Y$ is conditional on $X$, i.e. $P(Y \vert X)$. The model encompasses these conditional distributions for all variables given their parents in the DAG, encapsulating the joint distribution over all variables.

% Causal models facilitate interventions, counterfactuals, and causal inference. Intervention is the ability to model the outcome of purposeful changes, represented mathematically as "do" operations [pearl]. For example, if one were to intervene to set 
% $X = x$, the model would enable us to compute the distribution over $Y$ post-intervention.

% Counterfactual reasoning allows us to traverse back in time, in a sense, and assess alternative realitiesâ€”what would have happened to $Y$ if $X$ had been different? This provides a structured framework for hindsight, and one could argue, even a modicum of wisdom.

% [refer to pearls paper of three levels of causality \cite{Pearl_2018}, go more in detail?]

Using GFlowNet we can make inferences about intermediate variables. This is because we approximate probability distributions and not point estimates.



\subsection{Language of Thought}

Jerry Fodor suggests that thought takes place within a mental language - sometimes referred to as "Mentalese." Under this hypothesis, our cognitive processes can be viewed as computations involving a system of mental representations that can be composed into complex thoughts, akin to how sentences are formed from words according to the rules of grammar. Fodor's theory implies an innate structure underlying human cognition, with systematic, rule-governed operations that manipulate symbols.


Keep this here or in discussion?
\begin{itemize}
    \item frame-problem
    \item IBE + abduction
    \item Markpoel 7 criteria
\end{itemize}
















Under the Free-Energy Principle [necessary?, maybe just Bayesian principles in general], organisms are seen to construct internal models of the internal and external world to predict and hence reduce the surprise of sensory inputs, which requires a similarly nested, hierarchical organization of cognitive processes \cite{friston_free-energy_2010, friston_world_2021}. 

Instead of viewing the brain as a passive data collector, the brain is a query mechanism, primarily engaged in top-down prediction. The primary function becomes generating predictions. What ascends is the prediction error, the mismatch between predictions and actual [sensorimotor, proprioceptive, interoceptive, etc. ] input. In this framework, perception becomes an query-response mechanism. The brain tries to infer probable causal factors from its received data.

\section{Bayesian Inference}

The fundamental tenet of Bayesian computational cognition is that the brain interprets the world by forming probabilistic models and updates them according to Bayesian inference principles. This means the brain weighs prior beliefs (previous experiences and knowledge) and the likelihood of new sensory evidence to arrive at posterior beliefs (updated model of the world). The brain uses these posterior beliefs to make predictions about future events, thus enabling adaptive behavior.

Bayesian inference can be formalized by:

\[ P(H \vert E) = \frac{P(E \vert H) \cdot P(H)}{P(E)} \]
Where:
\begin{itemize}
    \item \( P(H \vert E) \) is the posterior probability of hypothesis \( H \) given evidence \( E \).
    \item \( P(E \vert H) \) is the likelihood of evidence \( E \) given that hypothesis \( H \) is true.
    \item \( P(H) \) is the prior probability of hypothesis \( H \).
    \item \( P(E) \) is the marginal likelihood, probability of evidence \( E \).
\end{itemize}

In this Bayesian framework, the joint probability \( P(E, H) = P(E \vert H) \cdot P(H) \) is referred to as the generative model that hypothesizes how sensory data are generated by the hidden states of the world. In other words, it specifies a joint probability distribution over sensory inputs and potential causes of those inputs. These causes can be anything from the presence of objects in the environment to more abstract concepts like social cues.

The recognition model is an approximation of the posterior \(Q(H \vert E) \approx P(H \vert E) \) the brain's inference about the state of the world given the data.
[explain marginalisation problem?]





% \subsection{Concepts}
% what is necessary?


% \subsection{LLMs}
% - But LLMs already solve a lot of that.


% PROS:
% - Semantics vs Syntax, semantic grammar
% - vector representations of concepts, vector algebra, distance, etc. 
% - Compositionality


% CONS:
% - They conflate an implicit world model with inference. (model in wolframs terms)
% - they lack causality.
% - pattern to syntax etc. vs other way around


% - LOT









\subsection{Game Engine in the Head}
% \begin{quote}
%     "All I ever wanted was to pick apart the day, put the pieces back together my way." - Aesop Rock
% \end{quote}

\begin{itemize}
    \item game engine in the head
    \item Reverse engineering the world
    \item Intuitive physics etc. 
    \item Extension of the LOT, circumventing the downfall of GOFAI, frame-problem, etc. 
\end{itemize}


Dehaene et al. posit that human cognition is uniquely characterized by its ability to form symbolic representations and recursive mental structures akin to a language of thought, enabling the creation of domain-specific conceptual systems \cite{dehaene_symbols_2022}. This cognitive ability allows for the generation of new concepts through the compositional arrangement of existing elements, a process exemplified by the derivation of geometric concepts. Cognition simplifies complex patterns into mental representations via mental compression, where the complexity of a concept is measured by the length of its mental representation as per the Minimum Description Length (MDL) principle.

Neuroscientific research indicates the presence of specialized brain circuits responsible for processing different domains of these languages of thought, with certain brain regions involved in linguistic processing and others in non-linguistic domains like mathematics and spatial reasoning. The recognition of mathematical patterns is linked to the ability to detect repetition with variation, a process underpinned by specific neural areas that vary with cognitive domain.

Distinct from other primates, humans have developed the capability to use symbols in complex, rule-based systems, highlighting a unique aspect of human cognitive development. Non-human primates may associate signs with concepts; however, they do not appear to use these in the recursive, rule-based manner that humans do.

[Tenenbaum et al] posit that the brain implements mechanisms analogous to those found in probabilistic programming languages, enabling it to represent and infer the probabilistic structure of the world. Probabilistic programming provides a framework for defining complex probabilistic models and for performing inference in these models, and the hypothesis is that the brain engages in similar computational processes. [multiple sources]

Experiments show that humans do not seem to start from blank-slate but rather from rich domain knowledge [argument for primitives, more sources] \cite{lake_building_2016}. Lake et al. propose that concepts can be represented as simple stochastic programs [elaborate]. A program here can be thought of a procedure that generates more examples of the same concept. If a program would represent the concept "animal", if would generate examples such as "giraffe", "zebra", "fish", and so on. Of course, higher-level programs could produce lower-level programs, in other words, in this paradigm, the essential aspect of compositionality gives rise to a part-whole hierarchical structure, i.e. a holarchy [reference].

[the assumption we make here is that features are somehow aggregated or extracted into symbols [see \cite{garcez_neurosymbolic_2020}], primitives, along with possibly innate symbols \cite{Lake_Ullman_Tenenbaum_Gershman_2017}] 


