 Considerations on how to compare outputs.
 - Edit distance
 - Soft edit distance to make it differentiable.

 - Reward doesn't have to be differentiable because its only part of the trajectory balance which is used as loss. Together, it is differentiable. 



Policy tempering
The strategy you've mentioned, often called "temperature scaling" or "softmax temperature", is a common method used in reinforcement learning and other domains. By raising the probabilities (or applying the scaling in the logit space, which is more numerically stable), you're adjusting the sharpness of the policy distribution.

Effect of Temperature on Distribution:
If 
β
β (or the temperature) is set to 1, then the original distribution is unchanged.
If 
β
β is set to a value greater than 1, the distribution becomes more uniform (actions with non-zero probabilities become more likely).
If 
β
β is set to a value between 0 and 1 (but not including 0), the distribution becomes more peaked (the most likely actions become even more likely).
Effect on Logits:
Scaling logits by a factor is equivalent to raising the probabilities to that power, after normalization. This is due to the properties of the softmax function.
If you just exponentiate the logits, you'd indeed scale the logits, but the relative probabilities after the softmax would be affected by the temperature, which is the desired effect.
Choosing 
β
β:
The value of 
β
β (temperature) often requires experimentation. The right setting can depend on the task, the nature of the policy, and the desired behavior.
A common approach is to start with 
β
=
1
β=1 (no scaling) and then adjust up or down based on observed behavior.
If you want your policy to explore more, you can increase 
β
β to make the policy more uniform.
If you want your policy to be more deterministic and exploit its current knowledge, you can decrease 
β
β to make the distribution more peaked.


\section{Assumptions}